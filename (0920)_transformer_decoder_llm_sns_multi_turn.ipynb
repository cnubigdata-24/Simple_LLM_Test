{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0p4bCIOSEeF",
        "outputId": "031a6252-d49a-4326-bca2-cab4448b278a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiJpqZ45SPW-",
        "outputId": "02e4c849-cfa4-4627-a9a6-64fb7669d8d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 719592\n",
            "drwxr-xr-x 1 root root      4096 Sep 19 17:36 .\n",
            "drwxr-xr-x 1 root root      4096 Sep 19 12:07 ..\n",
            "drwxr-xr-x 4 root root      4096 Sep 16 13:40 .config\n",
            "drwx------ 5 root root      4096 Sep 19 12:13 drive\n",
            "-rw-r--r-- 1 root root 736830265 Sep 19 17:36 final_sns_model_a100.pt\n",
            "drwxr-xr-x 2 root root      4096 Sep 19 17:17 .ipynb_checkpoints\n",
            "drwxr-xr-x 1 root root      4096 Sep 16 13:40 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al \"/content/drive/MyDrive/Colab Notebooks/data_cache\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdTensVjSS_T",
        "outputId": "884d8986-520b-4be6-dcd9-c4c1aff5c51b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 13367\n",
            "-rw------- 1 root root 13686793 Sep 19 16:11 conversations_default.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Korean SNS Multi-turn Conversation Dataset Decoder-only Transformer LLM Tutorial\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import gc"
      ],
      "metadata": {
        "id": "XQMXMW4NSamP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU and Hyperparameter Configuration\n",
        "\n",
        "# GPU setup\n",
        "def setup_device_and_mode():\n",
        "    print(f\"\\n{'='*10} GPU Environment {'='*10}\")\n",
        "    print(f\"‧ PyTorch version: {torch.__version__}\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"GPU is not available. Setting CPU mode.\")\n",
        "        return device, \"CPU\"\n",
        "    else:\n",
        "        print(f\"‧ CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "    gpu_name = torch.cuda.get_device_name()\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "    print(f\"‧ GPU: {gpu_name}\")\n",
        "    print(f\"‧ GPU Memory: {gpu_memory:.1f}GB\")\n",
        "\n",
        "    if \"A100\" in gpu_name:\n",
        "        print(\"‧ A100 GPU detected - Setting A100 mode\")\n",
        "        return device, \"A100\"\n",
        "    elif any(gpu in gpu_name for gpu in [\"L4\", \"T4\", \"V100\", \"K80\", \"P100\"]):\n",
        "        print(\"‧ L4/T4/V100-class GPU detected - Setting medium performance mode\")\n",
        "        return device, \"T4\"\n",
        "    else:\n",
        "        print(\"‧ Other GPU detected - Setting general GPU mode\")\n",
        "        return device, \"T4\"\n",
        "\n",
        "# Configuration dictionary by GPU mode\n",
        "GPU_CONFIGS = {\n",
        "    \"A100\": {\n",
        "        \"SEQ_LEN\": 256, \"BATCH_SIZE\": 16, \"EMBED_DIM\": 768, \"N_LAYERS\": 8,\n",
        "        \"N_HEADS\": 8, \"FFN_DIM\": 2048, \"MAX_CONVERSATIONS\": 3000,\n",
        "        \"EPOCHS\": 1, \"NUM_WORKERS\": 4, \"ENABLE_TRANSFORMER_DEMO\": True\n",
        "    },\n",
        "    \"T4\": {\n",
        "        \"SEQ_LEN\": 128, \"BATCH_SIZE\": 8, \"EMBED_DIM\": 512, \"N_LAYERS\": 6,\n",
        "        \"N_HEADS\": 6, \"FFN_DIM\": 1024, \"MAX_CONVERSATIONS\": 2000,\n",
        "        \"EPOCHS\": 1, \"NUM_WORKERS\": 2, \"ENABLE_TRANSFORMER_DEMO\": True\n",
        "    },\n",
        "    \"CPU\": {\n",
        "        \"SEQ_LEN\": 64, \"BATCH_SIZE\": 4, \"EMBED_DIM\": 256, \"N_LAYERS\": 4,\n",
        "        \"N_HEADS\": 4, \"FFN_DIM\": 512, \"MAX_CONVERSATIONS\": 1000,\n",
        "        \"EPOCHS\": 1, \"NUM_WORKERS\": 0, \"ENABLE_TRANSFORMER_DEMO\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# Global configuration\n",
        "GLOBAL_CONFIG = {\n",
        "    \"USE_CACHE\": True,\n",
        "\n",
        "    \"USE_CUSTOM_TOKENIZER\": False,\n",
        "    \"PRETRAINED_TOKENIZER\": \"klue/bert-base\",\n",
        "\n",
        "    \"MAX_FILES_FOR_FULL_DATASET\": 2000,\n",
        "\n",
        "    \"DEFAULT_CACHE_FILE\": \"conversations_default.pkl\",\n",
        "    \"DATA_CACHE_PATH\": \"/content/drive/MyDrive/Colab Notebooks/data_cache\",\n",
        "    \"LABELED_DATA_PATH\": \"/content/drive/MyDrive/Colab Notebooks/sns_multi_turn_dataset\",\n",
        "\n",
        "    \"VOCAB_SIZE\": 20000,\n",
        "    \"SAVE_EVERY\": 6000,  # Model checkpoint save interval\n",
        "\n",
        "    \"LR\": 2e-4,\n",
        "    \"WARMUP_RATIO\": 0.1,  # Learning rate warmup ratio\n",
        "    \"SEED\": 42\n",
        "}\n",
        "\n",
        "# Directory creation function\n",
        "def create_directories():\n",
        "    print(f\"\\n{'='*10} Directory Setup {'='*10}\")\n",
        "\n",
        "    paths_to_create = [\n",
        "        GLOBAL_CONFIG[\"DATA_CACHE_PATH\"],\n",
        "        GLOBAL_CONFIG[\"LABELED_DATA_PATH\"]\n",
        "    ]\n",
        "\n",
        "    for path in paths_to_create:\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "            print(f\"‧ Created directory: {path}\")\n",
        "        else:\n",
        "            print(f\"‧ Directory already exists: {path}\")\n",
        "\n",
        "# Execute setup and apply configuration\n",
        "DEVICE, GPU_MODE = setup_device_and_mode()\n",
        "config = GPU_CONFIGS[GPU_MODE]\n",
        "\n",
        "# Set global variables\n",
        "for key, value in {**config, **GLOBAL_CONFIG}.items():\n",
        "    globals()[key] = value\n",
        "\n",
        "# Create necessary directories\n",
        "create_directories()\n",
        "\n",
        "# Configuration output\n",
        "print(f\"\\n{'='*10} Configuration {'='*10}\")\n",
        "print(f\"‧ Batch size: {BATCH_SIZE}, Sequence length: {SEQ_LEN}\")\n",
        "print(f\"‧ Model size: {EMBED_DIM}D, {N_LAYERS} layers, {N_HEADS} heads\")\n",
        "print(f\"‧ Transformer Demo Log: {'Enabled' if ENABLE_TRANSFORMER_DEMO else 'Disabled'}\")\n",
        "print(f\"‧ Max conversations: {MAX_CONVERSATIONS:,}\")\n",
        "\n",
        "# GPU optimization and seed setup\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK4JlkGASuEG",
        "outputId": "2cd9d7c2-8919-48b4-de37-dbe870410183"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== GPU Environment ==========\n",
            "‧ PyTorch version: 2.8.0+cu126\n",
            "‧ CUDA Version: 12.6\n",
            "‧ GPU: NVIDIA A100-SXM4-40GB\n",
            "‧ GPU Memory: 42.5GB\n",
            "‧ A100 GPU detected - Setting A100 mode\n",
            "\n",
            "========== Directory Setup ==========\n",
            "‧ Directory already exists: /content/drive/MyDrive/Colab Notebooks/data_cache\n",
            "‧ Directory already exists: /content/drive/MyDrive/Colab Notebooks/sns_multi_turn_dataset\n",
            "\n",
            "========== Configuration ==========\n",
            "‧ Batch size: 16, Sequence length: 256\n",
            "‧ Model size: 768D, 8 layers, 8 heads\n",
            "‧ Transformer Demo Log: Enabled\n",
            "‧ Max conversations: 3,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Korean SNS Multi-turn Conversation Dataset Decoder-only Transformer LLM Tutorial\n",
        "\n",
        "# Time measurement utility function\n",
        "def log_time(start_time, step_name):\n",
        "    elapsed = time.time() - start_time\n",
        "    hours, minutes = int(elapsed // 3600), int((elapsed % 3600) // 60)\n",
        "    seconds = elapsed % 60\n",
        "\n",
        "    if hours > 0:\n",
        "        time_str = f\"{hours}h {minutes}m {seconds:.1f}s\"\n",
        "    elif minutes > 0:\n",
        "        time_str = f\"{minutes}m {seconds:.1f}s\"\n",
        "    else:\n",
        "        time_str = f\"{seconds:.1f}s\"\n",
        "\n",
        "    print(f\"==> [Elapsed Time] {step_name}: {time_str}\")\n",
        "    return time.time()\n",
        "\n",
        "# JSON file filtering function for training data\n",
        "def quick_filter_json_files(json_files, max_files):\n",
        "    if len(json_files) <= max_files:\n",
        "        return json_files\n",
        "\n",
        "    print(f\"File filtering: {len(json_files):,} → {max_files:,} files\")\n",
        "\n",
        "    file_info = []\n",
        "    for file_path in json_files:\n",
        "        try:\n",
        "            size = os.path.getsize(file_path)\n",
        "            if 500 < size < 50000:\n",
        "                file_info.append((file_path, size))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    file_info.sort(key=lambda x: x[1])\n",
        "    step = max(1, len(file_info) // max_files)\n",
        "    selected_files = [info[0] for info in file_info[::step][:max_files]]\n",
        "\n",
        "    print(f\"Selection completed: {len(selected_files):,} files\")\n",
        "    return selected_files\n",
        "\n",
        "# Tokenizer management class for KLUE/BERT-base or custom tokenizer\n",
        "class TokenizerManager:\n",
        "    def __init__(self, use_custom=False, pretrained_model=\"klue/bert-base\"):\n",
        "        self.use_custom = use_custom\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.tokenizer = None\n",
        "        self.vocab_size = None\n",
        "\n",
        "    def setup_tokenizer(self, conversations=None):\n",
        "        start_time = time.time()\n",
        "\n",
        "        if self.use_custom:\n",
        "            print(\"> Using BERT tokenizer instead of custom tokenizer\")\n",
        "            self.use_custom = False\n",
        "\n",
        "        print(f\"> Loading tokenizer: {self.pretrained_model}\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model)\n",
        "            if not hasattr(self.tokenizer, 'pad_token') or self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token if hasattr(self.tokenizer, 'eos_token') else '[PAD]'\n",
        "            special_tokens = [\"[TURN]\", \"[SPKA]\", \"[SPKB]\"]\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "            self.vocab_size = len(self.tokenizer)\n",
        "            tokenizer_type = \"KLUE/BERT\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"> Tokenizer loading failed: {e}\")\n",
        "            print(\"> Using BERT multilingual tokenizer\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "            special_tokens = [\"[TURN]\", \"[SPKA]\", \"[SPKB]\"]\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "            self.vocab_size = len(self.tokenizer)\n",
        "            tokenizer_type = \"BERT Multilingual\"\n",
        "\n",
        "        log_time(start_time, f\"> {tokenizer_type} tokenizer setup complete\")\n",
        "        print(f\"> Vocabulary size: {self.vocab_size:,}\")\n",
        "        return self.tokenizer\n",
        "\n",
        "    def encode(self, text):\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        return self.tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "\n",
        "    def get_special_tokens(self):\n",
        "        special_tokens = {}\n",
        "        special_tokens['PAD'] = self.tokenizer.pad_token_id or 0\n",
        "        special_tokens['UNK'] = self.tokenizer.unk_token_id or 1\n",
        "        special_tokens['BOS'] = self.tokenizer.cls_token_id or 2\n",
        "        special_tokens['EOS'] = self.tokenizer.sep_token_id or 3\n",
        "\n",
        "        try:\n",
        "            special_tokens['TURN_SEP'] = self.tokenizer.convert_tokens_to_ids('[TURN]')\n",
        "            special_tokens['SPEAKER_A'] = self.tokenizer.convert_tokens_to_ids('[SPKA]')\n",
        "            special_tokens['SPEAKER_B'] = self.tokenizer.convert_tokens_to_ids('[SPKB]')\n",
        "        except:\n",
        "            special_tokens['TURN_SEP'] = 4\n",
        "            special_tokens['SPEAKER_A'] = 5\n",
        "            special_tokens['SPEAKER_B'] = 6\n",
        "\n",
        "        return special_tokens\n",
        "\n",
        "def process_json_file(json_file_path):\n",
        "    try:\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if 'utterances' in data and len(data['utterances']) >= 2:\n",
        "            conversation = []\n",
        "            for utterance in data['utterances']:\n",
        "                speaker = utterance.get('speaker', 'unknown')\n",
        "                text = utterance.get('text', '').strip()\n",
        "                if text and len(text) < 2000:\n",
        "                    conversation.append({'speaker': speaker, 'text': text})\n",
        "\n",
        "            if 2 <= len(conversation) <= 100:\n",
        "                return conversation\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# LLM training data cache file loading/saving class\n",
        "class DataCache:\n",
        "    def __init__(self):\n",
        "        self.cache_dir = Path(DATA_CACHE_PATH)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def load_any_existing_cache(self):\n",
        "        try:\n",
        "            cache_path = self.cache_dir / DEFAULT_CACHE_FILE\n",
        "\n",
        "            if not cache_path.exists():\n",
        "                print(f\"> [Cache] Default cache file not found: {DEFAULT_CACHE_FILE}\")\n",
        "                return None\n",
        "\n",
        "            print(f\"> [Cache] Loading default cache file: {DEFAULT_CACHE_FILE}\")\n",
        "\n",
        "            with open(cache_path, 'rb') as f:\n",
        "                cache_data = pickle.load(f)\n",
        "\n",
        "            conversations = cache_data.get('conversations', [])\n",
        "            print(f\"> [Cache] Cache loaded successfully ({len(conversations):,} conversations)\")\n",
        "\n",
        "            if len(conversations) == 0:\n",
        "                print(\"> [Cache] Cache is empty - returning None\")\n",
        "                return None\n",
        "\n",
        "            return conversations\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Cache] Cache loading failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def save_default_cache(self, conversations):\n",
        "        cache_path = self.cache_dir / DEFAULT_CACHE_FILE\n",
        "\n",
        "        try:\n",
        "            cache_data = {\n",
        "                'conversations': conversations,\n",
        "                'timestamp': time.time(),\n",
        "                'total_conversations': len(conversations)\n",
        "            }\n",
        "\n",
        "            with open(cache_path, 'wb') as f:\n",
        "                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            file_size = cache_path.stat().st_size / (1024 * 1024)\n",
        "            print(f\"> [Cache] New cache saved successfully: {DEFAULT_CACHE_FILE} ({file_size:.1f}MB)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"> [Cache] Cache saving failed: {e}\")\n",
        "\n",
        "# LLM training JSON dataset loader class (uses cache file if available instead of JSON)\n",
        "class FastSNSDataLoader:\n",
        "    def __init__(self, labeled_data_path):\n",
        "        self.labeled_data_path = labeled_data_path\n",
        "        self.cache = DataCache() if USE_CACHE else None\n",
        "\n",
        "    def load_json_conversations(self):\n",
        "        start_time = time.time()\n",
        "\n",
        "        if self.cache:\n",
        "            cached_conversations = self.cache.load_any_existing_cache()\n",
        "            if cached_conversations and len(cached_conversations) > 0:\n",
        "                log_time(start_time, f\"> Loaded from cache ({len(cached_conversations):,} conversations)\")\n",
        "                return cached_conversations\n",
        "\n",
        "        print(\"> [Processing] Starting new data processing...\")\n",
        "\n",
        "        json_pattern = os.path.join(self.labeled_data_path, \"**\", \"*.json\")\n",
        "        json_files = glob(json_pattern, recursive=True)\n",
        "        print(f\"Found JSON files: {len(json_files):,}\")\n",
        "\n",
        "        if MAX_FILES_FOR_FULL_DATASET and len(json_files) > MAX_FILES_FOR_FULL_DATASET:\n",
        "            json_files = quick_filter_json_files(json_files, MAX_FILES_FOR_FULL_DATASET)\n",
        "\n",
        "        file_search_time = log_time(start_time, \"JSON file search and filtering\")\n",
        "\n",
        "        print(\"Sequential JSON parsing...\")\n",
        "        conversations = []\n",
        "\n",
        "        batch_size = 2000\n",
        "        for i in range(0, len(json_files), batch_size):\n",
        "            batch_files = json_files[i:i+batch_size]\n",
        "            batch_desc = f\"JSON processing batch {i//batch_size + 1}/{(len(json_files)-1)//batch_size + 1}\"\n",
        "\n",
        "            for json_file in tqdm(batch_files, desc=batch_desc, leave=False):\n",
        "                conv = process_json_file(json_file)\n",
        "                if conv:\n",
        "                    conversations.append(conv)\n",
        "\n",
        "            if i % (batch_size * 2) == 0:\n",
        "                gc.collect()\n",
        "\n",
        "        print(f\"> Extracted conversations: {len(conversations):,}\")\n",
        "        log_time(file_search_time, \"JSON file processing complete\")\n",
        "\n",
        "        if self.cache and conversations:\n",
        "            self.cache.save_default_cache(conversations)\n",
        "\n",
        "        return conversations\n",
        "\n",
        "# Transformer process sample output function\n",
        "def detailed_transformer_demo(model, tokenizer_manager, sample_text=\"안녕하세요! 오늘 어떤 하루 보내셨나요?\"):\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"■ TRANSFORMER INTERNAL PROCESS ANALYSIS\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\\n1. Input Sentence Analysis\")\n",
        "    print(f\"   Original sentence: '{sample_text}'\")\n",
        "    print(f\"   Sentence length: {len(sample_text)} characters\")\n",
        "\n",
        "    # Tokenization process\n",
        "    print(f\"\\n2. Tokenization Process\")\n",
        "    tokens = tokenizer_manager.encode(sample_text)\n",
        "    print(f\"   Token ID array: {tokens[:10]}...\")\n",
        "    print(f\"   Token count: {len(tokens)}\")\n",
        "\n",
        "    try:\n",
        "        token_texts = tokenizer_manager.tokenizer.convert_ids_to_tokens(tokens[:10])\n",
        "        print(f\"   Token texts (first 10): {token_texts}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Add special tokens\n",
        "    special_tokens = tokenizer_manager.get_special_tokens()\n",
        "    full_tokens = [special_tokens['BOS'], special_tokens['SPEAKER_A']] + tokens[:10] + [special_tokens['EOS']]\n",
        "\n",
        "    print(f\"\\n3. Special Token Addition\")\n",
        "    print(f\"   BOS (start): {special_tokens['BOS']}\")\n",
        "    print(f\"   SPEAKER_A: {special_tokens['SPEAKER_A']}\")\n",
        "    print(f\"   EOS (end): {special_tokens['EOS']}\")\n",
        "    print(f\"   Full sequence: {full_tokens}\")\n",
        "\n",
        "    # Padding\n",
        "    seq_len = min(16, model.seq_len)  # Smaller size for visualization\n",
        "    if len(full_tokens) < seq_len:\n",
        "        full_tokens.extend([special_tokens['PAD']] * (seq_len - len(full_tokens)))\n",
        "    else:\n",
        "        full_tokens = full_tokens[:seq_len]\n",
        "\n",
        "    print(f\"\\n4. Padding (sequence length adjustment)\")\n",
        "    print(f\"   Target length: {seq_len}\")\n",
        "    print(f\"   Padded sequence: {full_tokens}\")\n",
        "\n",
        "    # Tensor conversion and embedding\n",
        "    input_ids = torch.tensor([full_tokens], dtype=torch.long, device=DEVICE)\n",
        "    print(f\"\\n5. Tensor Conversion\")\n",
        "    print(f\"   Input tensor shape: {input_ids.shape} (batch=1, sequence={seq_len})\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Token embedding\n",
        "        token_emb = model.token_emb(input_ids)\n",
        "        print(f\"\\n6. Token Embedding (word → vector conversion)\")\n",
        "        print(f\"   Embedding shape: {token_emb.shape}\")\n",
        "        print(f\"   First token vector (first 5 dims): {token_emb[0, 0, :5].cpu().numpy().round(4).tolist()}\")\n",
        "\n",
        "        # Position embedding/encoding\n",
        "        if model.use_learnable_pos_emb:\n",
        "            positions = torch.arange(0, seq_len, device=DEVICE).unsqueeze(0)\n",
        "            pos_emb = model.pos_emb(positions)\n",
        "            print(f\"\\n7. Position Embedding (learnable position vectors)\")\n",
        "            print(f\"   Position embedding shape: {pos_emb.shape}\")\n",
        "            print(f\"   Position[0] vector (first 5 dims): {pos_emb[0, 0, :5].cpu().numpy().round(4).tolist()}\")\n",
        "            print(f\"   Position[1] vector (first 5 dims): {pos_emb[0, 1, :5].cpu().numpy().round(4).tolist()}\")\n",
        "            print(f\"   Type: Learnable parameters (GPT-style)\")\n",
        "        else:\n",
        "            pos_emb = model.pos_encoding[:, :seq_len, :]\n",
        "            print(f\"\\n7. Positional Encoding (fixed sinusoidal patterns)\")\n",
        "            print(f\"   Positional encoding shape: {pos_emb.shape}\")\n",
        "            print(f\"   Position[0] vector (first 5 dims): {pos_emb[0, 0, :5].cpu().numpy().round(4).tolist()}\")\n",
        "            print(f\"   Position[1] vector (first 5 dims): {pos_emb[0, 1, :5].cpu().numpy().round(4).tolist()}\")\n",
        "            print(f\"   Type: Fixed sinusoidal (original Transformer)\")\n",
        "\n",
        "        # Show the mathematical difference\n",
        "        if not model.use_learnable_pos_emb:\n",
        "            print(f\"   Sin/Cos pattern: pos_0 = sin(0/10000^(0/d)), pos_1 = cos(0/10000^(0/d)), ...\")\n",
        "\n",
        "        # Embedding combination\n",
        "        x = token_emb + pos_emb\n",
        "        print(f\"\\n8. Embedding Combination (token + position)\")\n",
        "        print(f\"   Combined embedding shape: {x.shape}\")\n",
        "        print(f\"   First token final vector (first 5 dims): {x[0, 0, :5].cpu().numpy().round(4).tolist()}\")\n",
        "\n",
        "        # First Transformer layer analysis\n",
        "        if len(model.layers) > 0:\n",
        "            first_layer = model.layers[0]\n",
        "\n",
        "            print(f\"\\n9. Transformer Layer (Layer 1/{len(model.layers)})\")\n",
        "            print(f\"   Config: embed_dim={first_layer.embed_dim}, n_heads={first_layer.n_heads}, head_dim={first_layer.head_dim}\")\n",
        "\n",
        "            # Q, K, V calculation\n",
        "            q = first_layer.q_proj(x)\n",
        "            k = first_layer.k_proj(x)\n",
        "            v = first_layer.v_proj(x)\n",
        "\n",
        "            print(f\"\\n10. Q, K, V Projection (weight matrix multiplication)\")\n",
        "            print(f\"   Input X shape: {x.shape}\")\n",
        "            print(f\"   Weight W_Q shape: {list(first_layer.q_proj.weight.shape)}\")\n",
        "            print(f\"   Query (Q = X @ W_Q) shape: {q.shape}\")\n",
        "            print(f\"   Key   (K = X @ W_K) shape: {k.shape}\")\n",
        "            print(f\"   Value (V = X @ W_V) shape: {v.shape}\")\n",
        "            print(f\"   Q[0,0] sample (first 5 dims): {q[0, 0, :5].cpu().numpy().round(4).tolist()}\")\n",
        "\n",
        "            # Multi-head transformation\n",
        "            batch_size = 1\n",
        "            n_heads = first_layer.n_heads\n",
        "            head_dim = first_layer.head_dim\n",
        "\n",
        "            q_heads = q.view(batch_size, seq_len, n_heads, head_dim).transpose(1, 2)\n",
        "            k_heads = k.view(batch_size, seq_len, n_heads, head_dim).transpose(1, 2)\n",
        "            v_heads = v.view(batch_size, seq_len, n_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "            print(f\"\\n11. Multi-head Transformation\")\n",
        "            print(f\"   Q multi-head shape: {q_heads.shape} (batch, heads, sequence, head_dim)\")\n",
        "            print(f\"   Each head performs independent {head_dim}-dimensional attention\")\n",
        "\n",
        "            # Attention Score calculation\n",
        "            scores = torch.matmul(q_heads, k_heads.transpose(-2, -1)) / math.sqrt(head_dim)\n",
        "            print(f\"\\n12. Attention Score Calculation (Q @ K^T / √d_k)\")\n",
        "            print(f\"   Score shape: {scores.shape} (batch, heads, sequence, sequence)\")\n",
        "            print(f\"   Normalization: √{head_dim} = {math.sqrt(head_dim):.2f}\")\n",
        "\n",
        "            # First head's score matrix sample\n",
        "            print(f\"\\n   [Head 1 Attention Score Matrix (4x4 sample)]\")\n",
        "            sample_scores = scores[0, 0, :4, :4].cpu().numpy()\n",
        "            for i in range(4):\n",
        "                print(f\"   {[f'{val:6.3f}' for val in sample_scores[i]]}\")\n",
        "\n",
        "            # causal_mask application\n",
        "            causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=DEVICE) * float('-inf'), diagonal=1)\n",
        "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n",
        "            masked_scores = scores + causal_mask\n",
        "\n",
        "            print(f\"\\n13. causal_mask Application (future token blocking)\")\n",
        "            print(f\"   Masked scores (4x4 sample, -inf indicates future tokens):\")\n",
        "            sample_masked = masked_scores[0, 0, :4, :4].cpu().numpy()\n",
        "            for i in range(4):\n",
        "                row = []\n",
        "                for val in sample_masked[i]:\n",
        "                    if val == float('-inf'):\n",
        "                        row.append('  -∞  ')\n",
        "                    else:\n",
        "                        row.append(f'{val:6.3f}')\n",
        "                print(f\"   {row}\")\n",
        "\n",
        "            # Softmax\n",
        "            attn_weights = F.softmax(masked_scores, dim=-1)\n",
        "            print(f\"\\n14. Softmax Normalization (probability distribution)\")\n",
        "            print(f\"   Attention weight shape: {attn_weights.shape}\")\n",
        "            print(f\"   [Head 1 Attention Weights (4x4, each row sum=1.0)]\")\n",
        "            sample_weights = attn_weights[0, 0, :4, :4].cpu().numpy()\n",
        "            for i in range(4):\n",
        "                print(f\"   {[f'{val:.4f}' for val in sample_weights[i]]} → sum: {sample_weights[i].sum():.4f}\")\n",
        "\n",
        "            # Attention output\n",
        "            attn_output = torch.matmul(attn_weights, v_heads)\n",
        "            print(f\"\\n15. Attention Output (Attention @ V)\")\n",
        "            print(f\"   Attention output shape: {attn_output.shape}\")\n",
        "\n",
        "            # Head combination\n",
        "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, first_layer.embed_dim)\n",
        "            attn_output = first_layer.out_proj(attn_output)\n",
        "            print(f\"   Head combination shape: {attn_output.shape}\")\n",
        "\n",
        "            # Add & Norm\n",
        "            x_norm1 = first_layer.ln1(x + first_layer.dropout(attn_output))\n",
        "            print(f\"\\n16. Add & LayerNorm (residual connection)\")\n",
        "            print(f\"   Residual connection: X + Attention(X)\")\n",
        "            print(f\"   Input range: [{x.min().item():.3f}, {x.max().item():.3f}] → Normalized: [{x_norm1.min().item():.3f}, {x_norm1.max().item():.3f}]\")\n",
        "\n",
        "            # FFN\n",
        "            ff_output = first_layer.ff(x_norm1)\n",
        "            print(f\"\\n17. Feed-Forward Network (FFN)\")\n",
        "            print(f\"   FFN structure: Linear({first_layer.embed_dim} → {first_layer.ff[0].out_features}) → GELU → Linear({first_layer.ff[3].in_features} → {first_layer.embed_dim})\")\n",
        "            print(f\"   FFN output shape: {ff_output.shape}\")\n",
        "\n",
        "            # Final Add & Norm\n",
        "            x_final = first_layer.ln2(x_norm1 + first_layer.dropout(ff_output))\n",
        "            print(f\"\\n18. Final Add & LayerNorm\")\n",
        "            print(f\"   Layer 1 final output shape: {x_final.shape}\")\n",
        "            print(f\"   Information flow: Input → Attention → FFN → Output\")\n",
        "\n",
        "        # Full model output\n",
        "        print(f\"\\n19. Final Model Output (through all layers)\")\n",
        "        output = model(input_ids)\n",
        "        print(f\"   Logit shape: {output.shape} (batch, sequence, vocab_size)\")\n",
        "\n",
        "        # Last position prediction\n",
        "        last_logits = output[0, -1, :]\n",
        "        print(f\"\\n20. Next Token Prediction (last position)\")\n",
        "        print(f\"   Vocabulary size: {len(last_logits)}, Token distribution generated\")\n",
        "\n",
        "        # Softmax probabilities\n",
        "        probs = F.softmax(last_logits, dim=-1)\n",
        "        top_probs, top_indices = torch.topk(probs, 5)\n",
        "        top_logits, _ = torch.topk(last_logits, 5)\n",
        "\n",
        "        print(f\"\\n   Top 5 Token Predictions (Softmax probabilities):\")\n",
        "        for i, (prob, idx, logit) in enumerate(zip(top_probs, top_indices, top_logits)):\n",
        "            try:\n",
        "                token_text = tokenizer_manager.tokenizer.convert_ids_to_tokens([idx.item()])[0]\n",
        "                print(f\"   {i+1}. Token {idx.item():5d} ('{token_text:10s}'): logit={logit.item():7.4f}, prob={prob.item()*100:6.3f}%\")\n",
        "            except:\n",
        "                print(f\"   {i+1}. Token {idx.item():5d}: logit={logit.item():7.4f}, prob={prob.item()*100:6.3f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100  + \"\\n\")\n",
        "\n",
        "# Multi-turn dataset tokenization class\n",
        "class MultiTurnDataset(Dataset):\n",
        "    def __init__(self, tokenizer_manager, conversations, seq_len, max_conversations=None):\n",
        "        start_time = time.time()\n",
        "        self.tokenizer_manager = tokenizer_manager\n",
        "        self.seq_len = seq_len\n",
        "        self.conversations = conversations[:max_conversations] if max_conversations else conversations\n",
        "        self.special_tokens = tokenizer_manager.get_special_tokens()\n",
        "\n",
        "        print(f\"> Conversation count: {len(self.conversations):,}\")\n",
        "        print(f\"> Vocabulary size: {tokenizer_manager.vocab_size:,}\")\n",
        "\n",
        "        self.token_sequences = []\n",
        "        tokenization_start = log_time(start_time, \"Dataset initialization\")\n",
        "\n",
        "        # GPU mode-specific processing\n",
        "        if GPU_MODE == \"A100\":\n",
        "            print(\"> A100 mode: Tokenization in progress...\")\n",
        "\n",
        "            all_tokens = []\n",
        "            for conversation in tqdm(self.conversations, desc=\"Tokenization\"):\n",
        "                conv_tokens = [self.special_tokens['BOS']]\n",
        "\n",
        "                for i, turn in enumerate(conversation):\n",
        "                    speaker_token = self.special_tokens['SPEAKER_A'] if i % 2 == 0 else self.special_tokens['SPEAKER_B']\n",
        "                    conv_tokens.append(speaker_token)\n",
        "                    text_tokens = self.tokenizer_manager.encode(turn['text'])\n",
        "                    conv_tokens.extend(text_tokens)\n",
        "\n",
        "                    if i < len(conversation) - 1:\n",
        "                        conv_tokens.append(self.special_tokens['TURN_SEP'])\n",
        "\n",
        "                conv_tokens.append(self.special_tokens['EOS'])\n",
        "                all_tokens.extend(conv_tokens)\n",
        "\n",
        "            # Generate overlapping sequences\n",
        "            step_size = seq_len // 4\n",
        "            for i in range(0, len(all_tokens) - seq_len + 1, step_size):\n",
        "                chunk = all_tokens[i:i + seq_len]\n",
        "                if len(chunk) == seq_len:\n",
        "                    self.token_sequences.append(chunk)\n",
        "        else:\n",
        "            # T4/CPU: Tokenization\n",
        "            print(\"> T4/CPU tokenization in progress...\")\n",
        "\n",
        "            for conversation in tqdm(self.conversations, desc=\"Tokenization progress\"):\n",
        "                conv_tokens = [self.special_tokens['BOS']]\n",
        "\n",
        "                for i, turn in enumerate(conversation):\n",
        "                    speaker_token = self.special_tokens['SPEAKER_A'] if i % 2 == 0 else self.special_tokens['SPEAKER_B']\n",
        "                    conv_tokens.append(speaker_token)\n",
        "                    text = turn['text'][:200] if GPU_MODE == \"T4\" else turn['text'][:100]\n",
        "                    text_tokens = self.tokenizer_manager.encode(text)\n",
        "                    if GPU_MODE == \"T4\":\n",
        "                        text_tokens = text_tokens[:50]\n",
        "                    else:\n",
        "                        text_tokens = text_tokens[:30]\n",
        "                    conv_tokens.extend(text_tokens)\n",
        "\n",
        "                    if i < len(conversation) - 1:\n",
        "                        conv_tokens.append(self.special_tokens['TURN_SEP'])\n",
        "\n",
        "                conv_tokens.append(self.special_tokens['EOS'])\n",
        "\n",
        "                # Padding or truncation\n",
        "                if len(conv_tokens) < seq_len:\n",
        "                    conv_tokens.extend([self.special_tokens['PAD']] * (seq_len - len(conv_tokens)))\n",
        "                else:\n",
        "                    conv_tokens = conv_tokens[:seq_len]\n",
        "\n",
        "                self.token_sequences.append(conv_tokens)\n",
        "\n",
        "                if GPU_MODE == \"CPU\" and len(self.token_sequences) >= 500:\n",
        "                    break\n",
        "\n",
        "        print(f\"> Generated sequences: {len(self.token_sequences):,}\")\n",
        "        log_time(tokenization_start, \"Tokenization complete\")\n",
        "\n",
        "        # Sample analysis\n",
        "        if self.conversations:\n",
        "            conversation = self.conversations[0]\n",
        "            print(f\"\\n[Sample Conversation] {min(len(conversation), 3)} turns\")\n",
        "            for i, turn in enumerate(conversation[:3]):  # Show all 3 turns\n",
        "                text = turn['text'][:40] + ('...' if len(turn['text']) > 40 else '')\n",
        "                print(f\"  Turn{i+1} ({turn['speaker']}): {text}\")\n",
        "            print()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.token_sequences[idx]\n",
        "        return torch.tensor(sequence, dtype=torch.long)\n",
        "\n",
        "# Transformer decoder implementation\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, ffn_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ffn_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_dim, embed_dim)\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, causal_mask=None):\n",
        "        # Self-attention\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        batch_size, seq_len = q.size(0), q.size(1)\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if causal_mask is not None:\n",
        "            scores = scores + causal_mask\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, v)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        x = self.ln1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed forward\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.ln2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "# Fixed positional encoding function (original Transformer)\n",
        "def create_sinusoidal_positional_encoding(seq_len, embed_dim):\n",
        "    \"\"\"Create fixed sinusoidal positional encoding as in original Transformer paper\"\"\"\n",
        "    pe = torch.zeros(seq_len, embed_dim)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    return pe.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Transformer decoder class with position encoding option\n",
        "# use_learnable_pos_emb = True: GPT-style (learnable Position Embedding), False: Original Transformer style (fixed Positional Encoding)\n",
        "class SNSDecoderLM(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, embed_dim, n_layers, n_heads, ffn_dim,\n",
        "                 dropout=0.1, use_learnable_pos_emb=True):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_dim = embed_dim\n",
        "        self.use_learnable_pos_emb = use_learnable_pos_emb\n",
        "\n",
        "        # Position encoding/embedding selection\n",
        "        if use_learnable_pos_emb:\n",
        "            # Learnable position embedding (GPT-style)\n",
        "            self.pos_emb = nn.Embedding(seq_len, embed_dim)\n",
        "            print(f\"> Using learnable Position Embedding (GPT-style)\")\n",
        "        else:\n",
        "            # Fixed sinusoidal positional encoding (original Transformer)\n",
        "            self.register_buffer('pos_encoding', create_sinusoidal_positional_encoding(seq_len, embed_dim))\n",
        "            print(f\"> Using fixed Positional Encoding (original Transformer)\")\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(embed_dim, n_heads, ffn_dim, dropout) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.head.weight = self.token_emb.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.seq_len, f\"Sequence length {t} exceeds maximum {self.seq_len}\"\n",
        "\n",
        "        tok_emb = self.token_emb(idx)\n",
        "\n",
        "        if self.use_learnable_pos_emb:\n",
        "            # Learnable position embedding\n",
        "            positions = torch.arange(0, t, device=idx.device).unsqueeze(0).expand(b, t)\n",
        "            pos_emb = self.pos_emb(positions)\n",
        "        else:\n",
        "            # Fixed positional encoding\n",
        "            pos_emb = self.pos_encoding[:, :t, :].expand(b, t, self.embed_dim)\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        causal_mask = torch.triu(torch.ones((t, t), device=idx.device) * float('-inf'), diagonal=1)\n",
        "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, _ = layer(x, causal_mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# CrossEntropyLoss calculation\n",
        "def multiturn_loss(logits, targets, special_tokens):\n",
        "    b, t, v = logits.size()\n",
        "    logits = logits[:, :-1, :].contiguous()\n",
        "    targets = targets[:, 1:].contiguous()\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=special_tokens['PAD'])\n",
        "    loss = loss_fn(logits.view(-1, v), targets.view(-1))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "b9E4eqgiT4XU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-turn conversation generation\n",
        "# returns top-K candidates with probabilities\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_multiturn_top_k(model, tokenizer_manager, prompt, max_new_tokens=40, temperature=0.8, top_k=30, num_candidates=3, device=DEVICE):\n",
        "    model.eval()\n",
        "    special_tokens = tokenizer_manager.get_special_tokens()\n",
        "\n",
        "    responses_with_probs = []\n",
        "\n",
        "    for candidate in range(num_candidates):\n",
        "        # Temperature variation for diversity\n",
        "        temp = temperature + (candidate * 0.15)\n",
        "\n",
        "        prompt_tokens = tokenizer_manager.encode(prompt)\n",
        "        tokens = [special_tokens['BOS'], special_tokens['SPEAKER_A']] + prompt_tokens + [special_tokens['TURN_SEP'], special_tokens['SPEAKER_B']]\n",
        "\n",
        "        # Accumulate probability for each token\n",
        "        token_probs = []\n",
        "\n",
        "        for step in range(max_new_tokens):\n",
        "            # Context window management\n",
        "            context = tokens[-SEQ_LEN:]\n",
        "            input_ids = torch.tensor([context], dtype=torch.long, device=device)\n",
        "\n",
        "            # Model forward pass\n",
        "            logits = model(input_ids)\n",
        "            last_logits = logits[0, -1, :] / temp\n",
        "\n",
        "            # Top-K filtering\n",
        "            if top_k > 0:\n",
        "                top_k_logits, top_k_indices = torch.topk(last_logits, min(top_k, last_logits.size(-1)))\n",
        "                last_logits = torch.full_like(last_logits, float('-inf'))\n",
        "                last_logits.scatter_(0, top_k_indices, top_k_logits)\n",
        "\n",
        "            # Convert to probabilities\n",
        "            probs = F.softmax(last_logits, dim=-1)\n",
        "            probs[special_tokens['UNK']] = 0.0\n",
        "            probs[special_tokens['PAD']] = 0.0\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "            # Sample next token\n",
        "            next_token = torch.multinomial(probs, 1).item()\n",
        "            token_prob = probs[next_token].item()\n",
        "            token_probs.append(token_prob)\n",
        "            tokens.append(next_token)\n",
        "\n",
        "            # Stop if EOS token generated\n",
        "            if next_token == special_tokens['EOS']:\n",
        "                break\n",
        "\n",
        "        # Decode response tokens\n",
        "        response_tokens = []\n",
        "        speaker_b_started = False\n",
        "\n",
        "        for token in tokens:\n",
        "            if token == special_tokens['SPEAKER_B']:\n",
        "                speaker_b_started = True\n",
        "                continue\n",
        "            elif token in [special_tokens['TURN_SEP'], special_tokens['EOS'], special_tokens['BOS']]:\n",
        "                continue\n",
        "            elif speaker_b_started and token not in [special_tokens['PAD'], special_tokens['UNK']]:\n",
        "                response_tokens.append(token)\n",
        "\n",
        "        response = tokenizer_manager.decode(response_tokens).strip()\n",
        "        if response and len(response) > 0:\n",
        "            # Calculate average probability\n",
        "            avg_prob = sum(token_probs) / len(token_probs) if token_probs else 0\n",
        "            responses_with_probs.append((response, avg_prob))\n",
        "\n",
        "    # Sort by probability\n",
        "    responses_with_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_responses = []\n",
        "    seen_texts = set()\n",
        "    for resp, prob in responses_with_probs:\n",
        "        if resp not in seen_texts:\n",
        "            unique_responses.append((resp, prob))\n",
        "            seen_texts.add(resp)\n",
        "\n",
        "    # Fallback responses\n",
        "    if not unique_responses:\n",
        "        unique_responses = [\n",
        "            (\"안녕하세요!\", 0.33),\n",
        "            (\"네, 반가워요!\", 0.33),\n",
        "            (\"좋은 하루네요!\", 0.34)\n",
        "        ]\n",
        "\n",
        "    return unique_responses[:3]\n",
        "\n",
        "# Enhanced model generation test (with probability values)\n",
        "def test_model_generation_improved(model, tokenizer_manager):\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"■ Enhanced Model Generation Test (3 topics, 3 candidates each + probabilities)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    test_prompts = [\n",
        "        \"안녕하세요! 오늘 어떤 하루 보내셨나요?\",\n",
        "        \"요즘 재미있게 본 영화가 있나요?\",\n",
        "        \"취미가 뭐예요?\"\n",
        "    ]\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n[Topic {i}/3]\")\n",
        "        print(f\"> Prompt: {prompt}\")\n",
        "\n",
        "        try:\n",
        "            responses_with_probs = generate_multiturn_top_k(\n",
        "                model, tokenizer_manager, prompt,\n",
        "                max_new_tokens=35, temperature=0.7, top_k=40, num_candidates=3\n",
        "            )\n",
        "\n",
        "            print(\"> LLM Responses (sorted by probability):\")\n",
        "            for j, (response, prob) in enumerate(responses_with_probs[:3], 1):\n",
        "                print(f\"‧ {j}. [{prob*100:.2f}%] {response}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"X Generation error: {e}\")\n",
        "\n",
        "# Interactive multi-turn test (with probabilities)\n",
        "def interactive_multiturn_test(model, tokenizer_manager, max_turns=10):\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"■ Interactive Multi-turn Test (max {max_turns} turns)\")\n",
        "    print(\"‧ To exit, type 'quit', 'exit', or '종료'\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model.eval()\n",
        "    turn_count = 0\n",
        "    conversation_history = []\n",
        "\n",
        "    while turn_count < max_turns:\n",
        "        turn_count += 1\n",
        "        print(f\"\\n> [Turn {turn_count}/{max_turns}]\")\n",
        "\n",
        "        try:\n",
        "            user_input = input(\"> User: \").strip()\n",
        "\n",
        "            if user_input.lower() in ['quit', 'exit', '종료', 'q']:\n",
        "                print(\"> Ending conversation.\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                print(\"> Empty input. Please try again.\")\n",
        "                turn_count -= 1\n",
        "                continue\n",
        "\n",
        "            conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "            responses_with_probs = generate_multiturn_top_k(\n",
        "                model, tokenizer_manager, user_input,\n",
        "                max_new_tokens=40, temperature=0.8, top_k=35, num_candidates=3\n",
        "            )\n",
        "\n",
        "            print(\"> LLM Responses (sorted by probability):\")\n",
        "            for i, (response, prob) in enumerate(responses_with_probs[:3], 1):\n",
        "                print(f\"‧ {i}. [{prob*100:.2f}%] {response}\")\n",
        "\n",
        "            if responses_with_probs:\n",
        "                conversation_history.append(f\"AI: {responses_with_probs[0][0]}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n> Conversation interrupted.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"X Error occurred: {e}\")\n",
        "\n",
        "    print(f\"\\n> Total {turn_count} turns of conversation completed.\")\n",
        "\n",
        "    if conversation_history:\n",
        "        print(f\"\\n■ Conversation Summary:\")\n",
        "        for line in conversation_history[-6:]:\n",
        "            print(f\"‧ {line}\")\n",
        "\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "JM84HAiQVaFE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"■ SNS Multi-turn Conversation LLM Training\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    tokenizer_type = \"KLUE/BERT Tokenizer\"\n",
        "    print(f\"‧ Tokenizer: {tokenizer_type}\")\n",
        "    print(f\"‧ Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}, Sequence length: {SEQ_LEN}\")\n",
        "    print(f\"‧ Max # of data files: {MAX_FILES_FOR_FULL_DATASET:,}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # Data loading\n",
        "    step_start = time.time()\n",
        "    sns_loader = FastSNSDataLoader(LABELED_DATA_PATH)\n",
        "    log_time(step_start, \"Data loader initialization\")\n",
        "\n",
        "    step_start = time.time()\n",
        "\n",
        "    # Attempt to load real data or generate dummy data\n",
        "    try:\n",
        "        conversations = sns_loader.load_json_conversations()\n",
        "        if not conversations or len(conversations) < 10:\n",
        "            raise ValueError(\"Insufficient conversation data available.\")\n",
        "    except Exception as e:\n",
        "        print(f\"> SNS data loading failed: {e}\")\n",
        "        print(\"> Generating dummy data...\")\n",
        "\n",
        "        # Generate dummy data\n",
        "        conversations = []\n",
        "        sample_dialogues = [\n",
        "            [\n",
        "                {\"speaker\": \"A\", \"text\": \"안녕하세요! 오늘 날씨가 참 좋네요.\"},\n",
        "                {\"speaker\": \"B\", \"text\": \"네, 정말 좋아요. 산책하기 딱 좋은 날씨예요.\"},\n",
        "                {\"speaker\": \"A\", \"text\": \"맞아요. 공원에 가려고 하는데 같이 가실래요?\"},\n",
        "                {\"speaker\": \"B\", \"text\": \"좋아요! 언제 출발할까요?\"}\n",
        "            ],\n",
        "            [\n",
        "                {\"speaker\": \"A\", \"text\": \"요즘 뭐하고 지내세요?\"},\n",
        "                {\"speaker\": \"B\", \"text\": \"새로운 프로젝트 준비 중이에요. 많이 바쁘네요.\"},\n",
        "                {\"speaker\": \"A\", \"text\": \"어떤 프로젝트인가요?\"},\n",
        "                {\"speaker\": \"B\", \"text\": \"AI 관련 프로젝트예요. 정말 흥미로워요!\"}\n",
        "            ]\n",
        "        ]\n",
        "\n",
        "        # Generate dummy conversations (repeated)\n",
        "        for _ in range(500):\n",
        "            conversations.append(random.choice(sample_dialogues))\n",
        "\n",
        "    load_time = log_time(step_start, f\"Data loading complete ({len(conversations):,} conversations)\")\n",
        "\n",
        "    # Tokenizer setup\n",
        "    print(\"\\n‧ Tokenizer setup started...\")\n",
        "    step_start = time.time()\n",
        "    tokenizer_manager = TokenizerManager(use_custom=USE_CUSTOM_TOKENIZER, pretrained_model=PRETRAINED_TOKENIZER)\n",
        "    tokenizer = tokenizer_manager.setup_tokenizer()\n",
        "    tokenizer_time = log_time(step_start, \"Tokenizer setup complete\")\n",
        "\n",
        "    # Dataset creation\n",
        "    print(\"\\n‧ Dataset creation started...\")\n",
        "    step_start = time.time()\n",
        "\n",
        "    max_conversations = min(MAX_CONVERSATIONS, len(conversations))\n",
        "    print(f\"{GPU_MODE} mode: Using {max_conversations:,} out of {len(conversations):,} conversations\")\n",
        "\n",
        "    dataset = MultiTurnDataset(tokenizer_manager, conversations, SEQ_LEN, max_conversations)\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(\"X Dataset is empty.\")\n",
        "        return\n",
        "\n",
        "    dataset_time = log_time(step_start, f\"Dataset creation complete ({len(dataset):,} sequences)\")\n",
        "\n",
        "    actual_batch_size = min(BATCH_SIZE, len(dataset))\n",
        "    if actual_batch_size < BATCH_SIZE:\n",
        "        print(f\"> Adjusting batch size from {BATCH_SIZE} to {actual_batch_size}\")\n",
        "\n",
        "    # DataLoader setup - resolve num_workers issue\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=actual_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=0,  # Disable multiprocessing to prevent deadlock\n",
        "        pin_memory=False  # Also disable pin_memory\n",
        "    )\n",
        "\n",
        "    total_steps = max(EPOCHS * len(dataloader), 10)\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"> DataLoader is empty.\")\n",
        "        return\n",
        "\n",
        "    # Model initialization\n",
        "    print(\"\\n‧ Model initialization started...\")\n",
        "    step_start = time.time()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    model = SNSDecoderLM(\n",
        "        vocab_size=tokenizer_manager.vocab_size,\n",
        "        seq_len=SEQ_LEN,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        n_layers=N_LAYERS,\n",
        "        n_heads=N_HEADS,\n",
        "        ffn_dim=FFN_DIM,\n",
        "        dropout=0.1,\n",
        "        use_learnable_pos_emb=True  # Add position embedding option\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Optimizer setup (AMP disabled)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LR,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=0.1,\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # AMP scaler disabled\n",
        "    scaler = None  # Completely disable AMP\n",
        "\n",
        "    warmup_steps = max(int(total_steps * WARMUP_RATIO), 1)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=LR,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=WARMUP_RATIO,\n",
        "        div_factor=25,\n",
        "        final_div_factor=10000,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "    print(f\"Parameter count: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Batch count: {len(dataloader)}\")\n",
        "    print(f\"Total steps: {total_steps}\")\n",
        "\n",
        "    model_init_time = log_time(step_start, \"Model initialization complete\")\n",
        "\n",
        "    # Transformer process analysis\n",
        "    if ENABLE_TRANSFORMER_DEMO:\n",
        "        print(\"\\n‧ Running Transformer analysis demo...\")\n",
        "        try:\n",
        "            detailed_transformer_demo(model, tokenizer_manager, \"안녕하세요! 오늘 어떤 하루 보내셨나요?\")\n",
        "        except Exception as e:\n",
        "            print(f\"X Transformer demo error: {e}\")\n",
        "\n",
        "    # Model training\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"■ Model Training Started\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "    special_tokens = tokenizer_manager.get_special_tokens()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} ===\")\n",
        "\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        # Debugging information\n",
        "        print(f\"‧ DataLoader batch count: {len(dataloader)}\")\n",
        "        print(f\"‧ Batch size: {actual_batch_size}\")\n",
        "        print(f\"‧ Dataset size: {len(dataset)}\")\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=f\"‧ Epoch {epoch+1}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            try:\n",
        "                batch = batch.to(DEVICE, non_blocking=False)  # Changed to non_blocking=False\n",
        "\n",
        "                # Simplified without AMP\n",
        "                logits = model(batch)\n",
        "                loss = multiturn_loss(logits, batch, special_tokens)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                scheduler.step()\n",
        "                epoch_loss += loss.item()\n",
        "                batch_count += 1\n",
        "                global_step += 1\n",
        "\n",
        "                # Increased memory cleanup interval\n",
        "                if global_step % 50 == 0 and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                # Progress update\n",
        "                gpu_memory = f'{torch.cuda.memory_allocated()/1e9:.1f}GB' if torch.cuda.is_available() else 'CPU'\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n",
        "                    'mem': gpu_memory,\n",
        "                    'batch': f'{batch_idx+1}/{len(dataloader)}'\n",
        "                })\n",
        "\n",
        "                # Debug first few batches\n",
        "                if batch_idx < 3:\n",
        "                    print(f\"\\n‧ Batch {batch_idx+1} processing complete - Loss: {loss.item():.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‧ X Error processing batch {batch_idx}: {e}\")\n",
        "                print(f\"Batch shape: {batch.shape if 'batch' in locals() else 'N/A'}\")\n",
        "                continue\n",
        "\n",
        "        if batch_count > 0:\n",
        "            avg_epoch_loss = epoch_loss / batch_count\n",
        "            log_time(epoch_start_time, f\"Epoch {epoch+1} complete (avg loss: {avg_epoch_loss:.4f})\")\n",
        "        else:\n",
        "            print(f\"> No batches processed in Epoch {epoch+1}.\")\n",
        "\n",
        "    training_time = log_time(training_start_time, \"Complete training finished\")\n",
        "\n",
        "    # Final model saving\n",
        "    final_checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'tokenizer_config': {\n",
        "            'use_custom': USE_CUSTOM_TOKENIZER,\n",
        "            'pretrained_model': PRETRAINED_TOKENIZER,\n",
        "            'vocab_size': tokenizer_manager.vocab_size\n",
        "        },\n",
        "        'config': {\n",
        "            'vocab_size': tokenizer_manager.vocab_size,\n",
        "            'seq_len': SEQ_LEN,\n",
        "            'embed_dim': EMBED_DIM,\n",
        "            'n_layers': N_LAYERS,\n",
        "            'n_heads': N_HEADS,\n",
        "            'ffn_dim': FFN_DIM,\n",
        "            'use_learnable_pos_emb': True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    model_path = f\"final_sns_model_{GPU_MODE.lower()}.pt\"\n",
        "    torch.save(final_checkpoint, model_path)\n",
        "    print(f\"> Final model saved: {model_path}\")\n",
        "\n",
        "    # Model testing\n",
        "    test_model_generation_improved(model, tokenizer_manager)\n",
        "\n",
        "    # Interactive testing\n",
        "    try:\n",
        "        interactive_multiturn_test(model, tokenizer_manager, 10)\n",
        "    except Exception as e:\n",
        "        print(f\"X Interactive test error: {e}\")\n",
        "\n",
        "    # Execution time summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"■ Execution Time Summary\")\n",
        "    print(\"=\"*80)\n",
        "    total_time = time.time() - total_start_time\n",
        "    print(f\"Total execution time: {total_time/60:.1f} minutes\")\n",
        "\n",
        "    print(\"\\n> Training completed!!\")"
      ],
      "metadata": {
        "id": "xTPOFdlIVyPs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    if hasattr(torch.multiprocessing, 'set_start_method'):\n",
        "        try:\n",
        "            torch.multiprocessing.set_start_method('spawn', force=True)\n",
        "        except RuntimeError:\n",
        "            pass\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFe6TQUcV1yW",
        "outputId": "4d37ac4b-91fc-4df4-9726-6f654e546846"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "■ SNS Multi-turn Conversation LLM Training\n",
            "================================================================================\n",
            "‧ Tokenizer: KLUE/BERT Tokenizer\n",
            "‧ Epochs: 1, Batch size: 16, Sequence length: 256\n",
            "‧ Max # of data files: 2,000\n",
            "================================================================================\n",
            "==> [Elapsed Time] Data loader initialization: 0.0s\n",
            "> [Cache] Loading default cache file: conversations_default.pkl\n",
            "> [Cache] Cache loaded successfully (10,000 conversations)\n",
            "==> [Elapsed Time] > Loaded from cache (10,000 conversations): 0.1s\n",
            "==> [Elapsed Time] Data loading complete (10,000 conversations): 0.1s\n",
            "\n",
            "‧ Tokenizer setup started...\n",
            "> Loading tokenizer: klue/bert-base\n",
            "==> [Elapsed Time] > KLUE/BERT tokenizer setup complete: 0.6s\n",
            "> Vocabulary size: 32,003\n",
            "==> [Elapsed Time] Tokenizer setup complete: 0.6s\n",
            "\n",
            "‧ Dataset creation started...\n",
            "A100 mode: Using 3,000 out of 10,000 conversations\n",
            "> Conversation count: 3,000\n",
            "> Vocabulary size: 32,003\n",
            "==> [Elapsed Time] Dataset initialization: 0.0s\n",
            "> A100 mode: Tokenization in progress...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 3000/3000 [00:02<00:00, 1023.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Generated sequences: 10,563\n",
            "==> [Elapsed Time] Tokenization complete: 3.0s\n",
            "\n",
            "[Sample Conversation] 3 turns\n",
            "  Turn1 (speakerA): 선수촌 식사 맛없다 ㅠㅠ\n",
            "  Turn2 (speakerB): 레알? 왜 맛없어?\n",
            "  Turn3 (speakerA): 메뉴가 다 똑같아서 지루해\n",
            "\n",
            "==> [Elapsed Time] Dataset creation complete (10,563 sequences): 3.0s\n",
            "\n",
            "‧ Model initialization started...\n",
            "> Using learnable Position Embedding (GPT-style)\n",
            "Parameter count: 68,888,320\n",
            "Batch count: 660\n",
            "Total steps: 660\n",
            "==> [Elapsed Time] Model initialization complete: 1.4s\n",
            "\n",
            "‧ Running Transformer analysis demo...\n",
            "\n",
            "====================================================================================================\n",
            "■ TRANSFORMER INTERNAL PROCESS ANALYSIS\n",
            "====================================================================================================\n",
            "\n",
            "1. Input Sentence Analysis\n",
            "   Original sentence: '안녕하세요! 오늘 어떤 하루 보내셨나요?'\n",
            "   Sentence length: 22 characters\n",
            "\n",
            "2. Tokenization Process\n",
            "   Token ID array: [5891, 2205, 5971, 5, 3822, 3711, 4051, 4209, 3011, 2075]...\n",
            "   Token count: 12\n",
            "   Token texts (first 10): ['안녕', '##하', '##세요', '!', '오늘', '어떤', '하루', '보내', '##셨', '##나']\n",
            "\n",
            "3. Special Token Addition\n",
            "   BOS (start): 2\n",
            "   SPEAKER_A: 32001\n",
            "   EOS (end): 3\n",
            "   Full sequence: [2, 32001, 5891, 2205, 5971, 5, 3822, 3711, 4051, 4209, 3011, 2075, 3]\n",
            "\n",
            "4. Padding (sequence length adjustment)\n",
            "   Target length: 16\n",
            "   Padded sequence: [2, 32001, 5891, 2205, 5971, 5, 3822, 3711, 4051, 4209, 3011, 2075, 3, 0, 0, 0]\n",
            "\n",
            "5. Tensor Conversion\n",
            "   Input tensor shape: torch.Size([1, 16]) (batch=1, sequence=16)\n",
            "\n",
            "6. Token Embedding (word → vector conversion)\n",
            "   Embedding shape: torch.Size([1, 16, 768])\n",
            "   First token vector (first 5 dims): [0.01640000008046627, 0.01769999973475933, 0.002899999963119626, -0.00860000029206276, 0.009600000455975533]\n",
            "\n",
            "7. Position Embedding (learnable position vectors)\n",
            "   Position embedding shape: torch.Size([1, 16, 768])\n",
            "   Position[0] vector (first 5 dims): [0.00279999990016222, 0.003100000089034438, 0.01600000075995922, -0.012500000186264515, -0.012000000104308128]\n",
            "   Position[1] vector (first 5 dims): [-0.014399999752640724, -0.01549999974668026, -0.018699999898672104, -0.012400000356137753, -0.04600000008940697]\n",
            "   Type: Learnable parameters (GPT-style)\n",
            "\n",
            "8. Embedding Combination (token + position)\n",
            "   Combined embedding shape: torch.Size([1, 16, 768])\n",
            "   First token final vector (first 5 dims): [0.019200000911951065, 0.020800000056624413, 0.01889999955892563, -0.020999999716877937, -0.002400000113993883]\n",
            "\n",
            "9. Transformer Layer (Layer 1/8)\n",
            "   Config: embed_dim=768, n_heads=8, head_dim=96\n",
            "\n",
            "10. Q, K, V Projection (weight matrix multiplication)\n",
            "   Input X shape: torch.Size([1, 16, 768])\n",
            "   Weight W_Q shape: [768, 768]\n",
            "   Query (Q = X @ W_Q) shape: torch.Size([1, 16, 768])\n",
            "   Key   (K = X @ W_K) shape: torch.Size([1, 16, 768])\n",
            "   Value (V = X @ W_V) shape: torch.Size([1, 16, 768])\n",
            "   Q[0,0] sample (first 5 dims): [-0.026399999856948853, -0.013500000350177288, -0.00559999980032444, 0.018200000748038292, 0.0038999998942017555]\n",
            "\n",
            "11. Multi-head Transformation\n",
            "   Q multi-head shape: torch.Size([1, 8, 16, 96]) (batch, heads, sequence, head_dim)\n",
            "   Each head performs independent 96-dimensional attention\n",
            "\n",
            "12. Attention Score Calculation (Q @ K^T / √d_k)\n",
            "   Score shape: torch.Size([1, 8, 16, 16]) (batch, heads, sequence, sequence)\n",
            "   Normalization: √96 = 9.80\n",
            "\n",
            "   [Head 1 Attention Score Matrix (4x4 sample)]\n",
            "   [' 0.000', '-0.000', '-0.000', '-0.001']\n",
            "   ['-0.000', ' 0.001', ' 0.000', ' 0.000']\n",
            "   [' 0.000', '-0.000', ' 0.001', ' 0.000']\n",
            "   ['-0.000', '-0.001', '-0.000', ' 0.000']\n",
            "\n",
            "13. causal_mask Application (future token blocking)\n",
            "   Masked scores (4x4 sample, -inf indicates future tokens):\n",
            "   [' 0.000', '  -∞  ', '  -∞  ', '  -∞  ']\n",
            "   ['-0.000', ' 0.001', '  -∞  ', '  -∞  ']\n",
            "   [' 0.000', '-0.000', ' 0.001', '  -∞  ']\n",
            "   ['-0.000', '-0.001', '-0.000', ' 0.000']\n",
            "\n",
            "14. Softmax Normalization (probability distribution)\n",
            "   Attention weight shape: torch.Size([1, 8, 16, 16])\n",
            "   [Head 1 Attention Weights (4x4, each row sum=1.0)]\n",
            "   ['1.0000', '0.0000', '0.0000', '0.0000'] → sum: 1.0000\n",
            "   ['0.4998', '0.5002', '0.0000', '0.0000'] → sum: 1.0000\n",
            "   ['0.3333', '0.3332', '0.3335', '0.0000'] → sum: 1.0000\n",
            "   ['0.2500', '0.2499', '0.2500', '0.2501'] → sum: 1.0000\n",
            "\n",
            "15. Attention Output (Attention @ V)\n",
            "   Attention output shape: torch.Size([1, 8, 16, 96])\n",
            "   Head combination shape: torch.Size([1, 16, 768])\n",
            "\n",
            "16. Add & LayerNorm (residual connection)\n",
            "   Residual connection: X + Attention(X)\n",
            "   Input range: [-0.119, 0.098] → Normalized: [-3.975, 3.557]\n",
            "\n",
            "17. Feed-Forward Network (FFN)\n",
            "   FFN structure: Linear(768 → 2048) → GELU → Linear(2048 → 768)\n",
            "   FFN output shape: torch.Size([1, 16, 768])\n",
            "\n",
            "18. Final Add & LayerNorm\n",
            "   Layer 1 final output shape: torch.Size([1, 16, 768])\n",
            "   Information flow: Input → Attention → FFN → Output\n",
            "\n",
            "19. Final Model Output (through all layers)\n",
            "   Logit shape: torch.Size([1, 16, 32003]) (batch, sequence, vocab_size)\n",
            "\n",
            "20. Next Token Prediction (last position)\n",
            "   Vocabulary size: 32003, Token distribution generated\n",
            "\n",
            "   Top 5 Token Predictions (Softmax probabilities):\n",
            "   1. Token     0 ('[PAD]     '): logit= 7.6054, prob= 5.107%\n",
            "   2. Token  5135 ('학부        '): logit= 2.3654, prob= 0.027%\n",
            "   3. Token  2749 ('##햇       '): logit= 2.0391, prob= 0.020%\n",
            "   4. Token   629 ('궤         '): logit= 1.9622, prob= 0.018%\n",
            "   5. Token  5918 ('경선        '): logit= 1.9602, prob= 0.018%\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "■ Model Training Started\n",
            "================================================================================\n",
            "\n",
            "=== Epoch 1/1 ===\n",
            "‧ DataLoader batch count: 660\n",
            "‧ Batch size: 16\n",
            "‧ Dataset size: 10563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‧ Epoch 1:   0%|          | 1/660 [00:00<01:30,  7.32it/s, loss=10.5414, lr=8.11e-06, mem=1.7GB, batch=1/660]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‧ Batch 1 processing complete - Loss: 10.5414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‧ Epoch 1:   0%|          | 2/660 [00:00<01:28,  7.42it/s, loss=10.5166, lr=8.45e-06, mem=1.7GB, batch=2/660]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‧ Batch 2 processing complete - Loss: 10.5166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‧ Epoch 1:   0%|          | 3/660 [00:00<01:28,  7.44it/s, loss=10.3999, lr=9.01e-06, mem=1.7GB, batch=3/660]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‧ Batch 3 processing complete - Loss: 10.3999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‧ Epoch 1: 100%|██████████| 660/660 [01:28<00:00,  7.50it/s, loss=3.7193, lr=2.20e-09, mem=1.7GB, batch=660/660]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> [Elapsed Time] Epoch 1 complete (avg loss: 4.3879): 1m 28.0s\n",
            "==> [Elapsed Time] Complete training finished: 1m 28.0s\n",
            "> Final model saved: final_sns_model_a100.pt\n",
            "\n",
            "================================================================================\n",
            "■ Enhanced Model Generation Test (3 topics, 3 candidates each + probabilities)\n",
            "================================================================================\n",
            "\n",
            "[Topic 1/3]\n",
            "> Prompt: 안녕하세요! 오늘 어떤 하루 보내셨나요?\n",
            "> LLM Responses (sorted by probability):\n",
            "‧ 1. [52.72%] 응! 짜짜? 어떤 기능이야! 그러면 나도 가서? 응! 나도 같이 플레이 중이야! 나도 그래\n",
            "‧ 2. [41.46%] 응 그래! 그러면 왜 왜? 나도 재밌겠다! 그럼 나도 그래? 나도 좋아! 고마워! 우리도 ㅋㅋ 굿\n",
            "‧ 3. [39.42%] 나도! 나도 가보자! 나도 꼭 가보자! 어떤 방법으로으로 시작해? 응! 같이 가자! 나\n",
            "\n",
            "[Topic 2/3]\n",
            "> Prompt: 요즘 재미있게 본 영화가 있나요?\n",
            "> LLM Responses (sorted by probability):\n",
            "‧ 1. [52.87%] 응, 어떤 곳이 뭐임? 근데 왜 그래? 글쿠나. 그래서 어떻게 해야 해? 그래도 어떻게 해야 해 근데 그\n",
            "‧ 2. [39.96%] 마좌 그 책이 너무 작은 어떤 거야? ㅇㅋㅇ! 최근에 대해 들었던 거야? 응! 그래서 이번에도 어떤\n",
            "‧ 3. [38.31%] 레알? 어떤 게임은 렬루다가? 게임이야 그래서 이번에 대해 들음? 그래? 어떤 결과는 어떤 게임을 하더라\n",
            "\n",
            "[Topic 3/3]\n",
            "> Prompt: 취미가 뭐예요?\n",
            "> LLM Responses (sorted by probability):\n",
            "‧ 1. [54.47%] 응, 어떤 기능이 있어? 오, 그리고 이번에는 어떤 게임이 있었어? 응, 어떻게 되면 안 돼?\n",
            "‧ 2. [47.75%] 어떤 팀이 안되기 위한 거야? 마좌 그래서 선수들은 렬루다가? 그러면 너는 거야? 그래?\n",
            "‧ 3. [33.77%] 그건 잘 해야 해. 어떻게 될까? 아직 뭐임? 뭐 할 거래. ㅋㅋ 맞아! 이 책은 사람들에 관심 있는 거\n",
            "\n",
            "================================================================================\n",
            "■ Interactive Multi-turn Test (max 10 turns)\n",
            "‧ To exit, type 'quit', 'exit', or '종료'\n",
            "================================================================================\n",
            "\n",
            "> [Turn 1/10]\n",
            "> User: 축구 봤어?\n",
            "> LLM Responses (sorted by probability):\n",
            "‧ 1. [36.41%] 나도 가보고 말했대 이번에 대한 책에서 우승하고 있어? 와 레알? 어떤 경기는 어떤 팀이야? 그렇지! 그리고 새로운\n",
            "‧ 2. [32.07%] 그래도 이번에 대해 들어봤고 하지? 레알? 응! 이제 우리와 이번에는 뭐가 없었어 응, 그러면 나도 좋은 없으면\n",
            "‧ 3. [27.38%] 응, 한국 축구 선수, 그러면 다른 결과 결과라고 들었어. 레알? 어떤 기능? 뭔가요. 맞아. 그리고 게임이야. ㅎㅎ 그래? 그럼\n",
            "\n",
            "> [Turn 2/10]\n",
            "> User: quit\n",
            "> Ending conversation.\n",
            "\n",
            "> Total 2 turns of conversation completed.\n",
            "\n",
            "■ Conversation Summary:\n",
            "‧ User: 축구 봤어?\n",
            "‧ AI: 나도 가보고 말했대 이번에 대한 책에서 우승하고 있어? 와 레알? 어떤 경기는 어떤 팀이야? 그렇지! 그리고 새로운\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "■ Execution Time Summary\n",
            "================================================================================\n",
            "Total execution time: 12.5 minutes\n",
            "\n",
            "> Training completed!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQasMHjTV3ko"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}