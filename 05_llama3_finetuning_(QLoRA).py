# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l3e0DMVFfCRZ5QOlfia_eHJi96pjja0r
"""



!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers==0.0.27" trl peft accelerate bitsandbytes

import torch
from unsloth import FastLanguageModel, is_bfloat16_supported
from transformers import TrainingArguments, TextStreamer
from trl import SFTTrainer
from datasets import Dataset
import pandas as pd

# 시스템 정보 출력
print("="*60)
print("시스템 정보")
print("="*60)
print(f"PyTorch 버전: {torch.__version__}")
print(f"CUDA 사용 가능: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB")
    print("\nGPU 모드 - 빠른 학습 가능")
else:
    print("\nCPU 모드 - 학습이 다소 느릴 수 있습니다")
    print("Google Colab에서는: 런타임 > 런타임 유형 변경 > GPU 선택")

# 하이퍼파라미터 설정
max_seq_length = 512  # 문의 텍스트는 짧으므로 512면 충분
load_in_4bit = True   # QLoRA: 4-bit quantization 사용 (메모리 1/4로 절감)
dtype = None          # 자동으로 최적 dtype 선택 (bfloat16 or float16)

# Llama 3.2 1B Instruct 모델 로드: Unsloth에서 최적화한 버전

print("\n모델 로딩 중...")

# FastLanguageModel: Unsloth의 핵심 클래스, 일반 로드보다 2-3배 빠름
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-1B-Instruct",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_your_token_here",  # Hugging Face 토큰 (필요시)
)

print(f"\n모델 로드 완료!")
print(f"모델: Llama 3.2 1B Instruct")
print(f"Quantization: 4-bit (QLoRA)")

# LoRA 적용: 원본 모델에 LoRA 어댑터 추가
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,  # LoRA rank: 16이면 적당한 성능과 속도 보장
    lora_alpha = 16,  # Scaling factor: r과 같은 값 사용
    lora_dropout = 0,  # 비활성화 (작은 데이터셋: 0)
    bias = "none",  # Bias 파라미터는 학습하지 않음

    # target_modules: Llama의 주요 attention 레이어에 LoRA 적용
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],

    use_gradient_checkpointing = "unsloth",  # Unsloth 최적화된 gradient checkpointing

    random_state = 42,  # 재현성을 위한 시드 고정
)

print("\nLoRA 설정 완료!")
print(f"LoRA rank (r): 16")
print(f"학습 파라미터: 전체의 약 1% 미만")

# 고객 문의 분류 데이터셋: 카테고리 (배송/반품/결제/제품/계정)
training_data = [
    # 배송 관련 (3개)
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "주문한 상품이 언제 도착하나요?",
        "output": "배송"
    },
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "배송 조회는 어떻게 하나요?",
        "output": "배송"
    },
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "배송지 변경이 가능한가요?",
        "output": "배송"
    },

    # 반품/교환 관련 (3개)
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "반품하고 싶은데 어떻게 하나요?",
        "output": "반품"
    },
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "사이즈가 안 맞아서 교환하고 싶어요.",
        "output": "반품"
    },
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "상품이 파손되어 왔어요. 교환 가능한가요?",
        "output": "반품"
    },

    # 결제 관련 (3개)
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "결제가 두 번 처리된 것 같아요.",
        "output": "결제"
    },
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "환불은 언제 되나요?",
        "output": "결제"
    },
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "카드 할부가 가능한가요?",
        "output": "결제"
    },

    # 제품 관련 (2개)
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "이 제품 재입고는 언제 되나요?",
        "output": "제품"
    },
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "이 상품 사이즈표를 알고 싶어요.",
        "output": "제품"
    },

    # 계정 관련 (2개)
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "비밀번호를 잊어버렸어요.",
        "output": "계정"
    },
    {
        "instruction": "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다.",
        "input": "회원 탈퇴하고 싶어요.",
        "output": "계정"
    },
]

# 데이터프레임으로 변환
df = pd.DataFrame(training_data)

print("\n학습 데이터셋:")
print(f"총 {len(df)}개 샘플")
print("\n카테고리별 분포:")
print(df['output'].value_counts())
print("\n샘플 데이터:")

print(df.head(3))

# Alpaca 프롬프트 템플릿
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

# EOS 토큰: 문장의 끝을 나타내는 특수 토큰, 모델이 언제 생성을 멈춰야 하는지 알려줌
EOS_TOKEN = tokenizer.eos_token

# 데이터를 Alpaca 포맷으로 변환
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]

    texts = []
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        # Alpaca 템플릿에 데이터 삽입 + EOS 토큰 추가
        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN
        texts.append(text)

    return {"text": texts}

# Hugging Face Dataset으로 변환
dataset = Dataset.from_pandas(df)

# Alpaca 포맷 적용
dataset = dataset.map(
    formatting_prompts_func,
    batched=True,
)

print("\nAlpaca 포맷 적용 완료!")
print("\n[포맷팅된 샘플 예시]")
print(dataset[0]['text'][:300] + "...")

# 학습 설정
training_args = TrainingArguments(
    output_dir = "./outputs_classification", # 결과 저장 디렉토리

    # 배치 크기: 2 (작은 데이터셋이므로 작게 설정)
    per_device_train_batch_size = 2,

    # 그래디언트 누적: 4번 누적 후 업데이트 (실제 배치 크기 = 2 x 4 = 8)
    gradient_accumulation_steps = 4,

    # Warmup: 처음 10% 동안 학습률을 점진적으로 증가
    warmup_ratio = 0.1,

    num_train_epochs = 3,
    learning_rate = 2e-4,

    # 16비트 연산 사용: bfloat16이 지원되면 사용, 아니면 float16
    fp16 = not is_bfloat16_supported(),
    bf16 = is_bfloat16_supported(),

    logging_steps = 1,

    # Optimizer: 8bit AdamW (메모리 효율적)
    optim = "adamw_8bit",

    # Weight decay: 가중치 감소 (과적합 방지)
    weight_decay = 0.01,

    # Learning rate scheduler: cosine (학습률을 코사인 곡선으로 감소)
    lr_scheduler_type = "cosine",

    seed = 42,
    report_to = "none", # 외부 로깅 비활성화
)

print("학습 설정 완료!")
print(f"\n배치 크기: {training_args.per_device_train_batch_size}")
print(f"그래디언트 누적: {training_args.gradient_accumulation_steps}")
print(f"실제 배치 크기: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"Epochs: {training_args.num_train_epochs}")
print(f"학습률: {training_args.learning_rate}")
print(f"총 학습 스텝: 약 {len(dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}")

# SFTTrainer: trl 라이브러리의 instruction-following 모델 학습용 Trainer
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",  # 텍스트가 들어있는 필드명
    max_seq_length = max_seq_length,
    args = training_args,
    packing = False,  # 여러 샘플을 하나로 묶지 않음
)

print("="*60)
print("학습 시작")
print("="*60)

print(f"학습 데이터: {len(dataset)}개")
print(f"Epochs: {training_args.num_train_epochs}")
print(f"예상 시간: GPU 3-5분, CPU 15-20분")
print("\n학습 중...\n")

# 실제 학습 실행
trainer.train()

print("\n" + "="*60)
print("학습 완료!")
print("="*60)

# 추론 모드로 전환: 학습 최적화 해제, 일반 모드로 사용
FastLanguageModel.for_inference(model)

# 고객 문의 분류 함수
def classify_inquiry(inquiry_text):
    instruction = "다음 고객 문의를 카테고리로 분류하세요. 카테고리는 배송, 반품, 결제, 제품, 계정 중 하나입니다."
    prompt = alpaca_prompt.format(instruction, inquiry_text, "")

    # 토큰화
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")

    # 텍스트 생성
    outputs = model.generate(
        **inputs,
        max_new_tokens=10,  # 카테고리 이름만 생성
        temperature=0.1,    # 낮은 temperature = 더 결정적인 출력
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    # 디코딩
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Response 만 추출
    if "### Response:" in result:
        prediction = result.split("### Response:")[1].strip()
    else:
        prediction = result

    return prediction

print("분류 함수 준비 완료!")

print("="*60)
print("테스트 1: 학습 데이터로 테스트")
print("="*60)

# 학습 데이터로 테스트
test_cases_trained = [
    "주문한 상품이 언제 도착하나요?",
    "반품하고 싶은데 어떻게 하나요?",
    "결제가 두 번 처리된 것 같아요.",
    "이 제품 재입고는 언제 되나요?",
    "비밀번호를 잊어버렸어요.",
]

expected_categories = ["배송", "반품", "결제", "제품", "계정"]

for i, (inquiry, expected) in enumerate(zip(test_cases_trained, expected_categories), 1):
    prediction = classify_inquiry(inquiry)

    print(f"\n[테스트 {i}]")
    print(f"문의: {inquiry}")
    print(f"정답: {expected}")
    print(f"예측: {prediction}")
    print(f"결과: {'O 정확' if expected in prediction else 'X 오답'}")
    print("-"*60)

print("\n" + "="*60)
print("테스트 2: 새로운 문의로 테스트 (학습에 없던 문의)")
print("="*60)

# 학습에 없던 새로운 데이터
test_cases_new = [
    {"inquiry": "택배가 아직 안 왔어요.", "expected": "배송"},
    {"inquiry": "색상을 바꾸고 싶은데 교환 되나요?", "expected": "반품"},
    {"inquiry": "신용카드로 결제했는데 취소하려면?", "expected": "결제"},
    {"inquiry": "이 제품 블랙 색상 있나요?", "expected": "제품"},
    {"inquiry": "로그인이 안 돼요.", "expected": "계정"},
    {"inquiry": "배송 날짜를 변경하고 싶어요.", "expected": "배송"},
    {"inquiry": "포인트 환불 받을 수 있나요?", "expected": "결제"},
]

correct = 0
total = len(test_cases_new)

for i, test_case in enumerate(test_cases_new, 1):
    inquiry = test_case["inquiry"]
    expected = test_case["expected"]

    prediction = classify_inquiry(inquiry)
    is_correct = expected in prediction

    if is_correct:
        correct += 1

    print(f"\n[테스트 {i}]")
    print(f"문의: {inquiry}")
    print(f"정답: {expected}")
    print(f"예측: {prediction}")
    print(f"결과: {'O 정확' if is_correct else 'X 오답'}")
    print("-"*60)

# 정확도 계산
accuracy = (correct / total) * 100
print(f"\n" + "="*60)
print(f"새로운 문의 정확도: {correct}/{total} = {accuracy:.1f}%")
print("="*60)

# LoRA 어댑터만 저장 (용량 작음, 약 10-50MB)
model.save_pretrained("classification_lora_model")
tokenizer.save_pretrained("classification_lora_model")

print("\n모델 저장 완료!")
print("저장 위치: ./classification_lora_model")
print("\n저장된 파일:")
print("- LoRA 어댑터 (adapter_model.safetensors)")
print("- Tokenizer 설정")
print("\n용량: 약 10-50MB (전체 모델의 1% 미만)")