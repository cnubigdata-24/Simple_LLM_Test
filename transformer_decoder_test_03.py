!apt-get -qq install fonts-nanum
!fc-cache -fv
!rm -rf ~/.cache/matplotlib

import matplotlib.pyplot as plt
plt.rc('font', family='NanumGothic')

!{sys.executable} -m pip install seaborn

# Transformer ì–¸ì–´ ìƒì„± ëª¨ë¸ - GPT ìŠ¤íƒ€ì¼ Decoder-Only

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import math
import re
import warnings
warnings.filterwarnings('ignore')

# CUDA í…ì„œë¥¼ numpyë¡œ ë³€í™˜
def safe_numpy(tensor):
    if tensor.is_cuda:
        return tensor.cpu().detach().numpy()
    else:
        return tensor.detach().numpy()

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
def set_seed(seed=42):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

set_seed(42)

print("=" * 100)
print("TRANSFORMER ì–¸ì–´ ìƒì„± ëª¨ë¸")
print("=" * 100)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nğŸ“± STEP 1: í™˜ê²½ ì„¤ì •")
print("-" * 80)

# GPU í™˜ê²½ í™•ì¸
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"   ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: True")
    print(f"   GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    print(f"   í˜„ì¬ GPU: {torch.cuda.get_device_name(0)}")
    print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"   CUDA ë²„ì „: {torch.version.cuda}")
else:
    device = torch.device('cpu')
    print(f"   ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: False")
    print(f"   CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤")
    print(f"   ì£¼ì˜: CPUì—ì„œëŠ” í•™ìŠµì´ ìƒë‹¹íˆ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")

print(f"   PyTorch ë²„ì „: {torch.__version__}")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
config = {
    'vocab_size': 5000,
    'd_model': 128,
    'n_heads': 8,
    'n_layers': 4,
    'd_ff': 512,
    'max_seq_len': 64,
    'dropout': 0.1,
    'learning_rate': 0.0001,
    'batch_size': 4,
    'epochs': 30
}

# CPU í™˜ê²½ì—ì„œëŠ” ë” ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ì„¤ì • ì¡°ì •
if device.type == 'cpu':
    print(f"\nâš ï¸  CPU í™˜ê²½ ê°ì§€ - í•™ìŠµ ì†ë„ ìµœì í™”")
    print("-" * 80)
    config['epochs'] = 10  # ì—í­ ìˆ˜ ê°ì†Œ
    config['d_model'] = 64  # ëª¨ë¸ í¬ê¸° ì¶•ì†Œ
    config['d_ff'] = 256   # í”¼ë“œí¬ì›Œë“œ í¬ê¸° ì¶•ì†Œ
    print(f"   CPU ìµœì í™”: ì—í­ {config['epochs']}, ëª¨ë¸ì°¨ì› {config['d_model']}")
    print(f"   ì˜ˆìƒ í•™ìŠµ ì‹œê°„: ì•½ 3-5ë¶„")
else:
    print(f"   ì˜ˆìƒ í•™ìŠµ ì‹œê°„: ì•½ 1-2ë¶„")

print(f"\nâš™ï¸ ìµœì¢… í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
for key, value in config.items():
    print(f"   {key}: {value}")

# ìƒ˜í”Œ í•œêµ­ì–´ ë°ì´í„°
sample_texts = [
    "ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”",
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”",
    "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤",
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤",
    "ì»´í“¨í„° ê³¼í•™ì€ ë§¤ìš° í¥ë¯¸ë¡œìš´ ë¶„ì•¼ì…ë‹ˆë‹¤",
    "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ê³µë¶€í•˜ê³  ìˆì–´ìš”",
    "ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤",
    "ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì´ ë†€ëìŠµë‹ˆë‹¤",
    "ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ëŠ” ë¯¸ë˜ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤",
    "ì½”ë”©ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì´ ì¬ë¯¸ìˆì–´ìš”"
]

print(f"\nğŸ“š STEP 2: í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
print("-" * 80)
print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(sample_texts)}")
print(f"   ìƒ˜í”Œ ë°ì´í„° ì˜ˆì‹œ:")
for i, text in enumerate(sample_texts[:3]):
    print(f"     {i+1}. {text}")
print(f"     ... (ì´ {len(sample_texts)}ê°œ)")

# ê³ ê¸‰ í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤
class AdvancedTokenizer:
    def __init__(self, texts, min_freq=1):
        print(f"\nğŸ”¤ í† í¬ë‚˜ì´ì € ìƒì„± ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
        all_words = []
        for text in texts:
            # í•œêµ­ì–´ì™€ ì˜ì–´, ìˆ«ìë¥¼ ëª¨ë‘ í¬í•¨í•˜ë„ë¡ ì •ê·œì‹ ê°œì„ 
            words = re.findall(r'\w+', text)
            all_words.extend(words)
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(all_words)
        
        # ìµœì†Œ ë¹ˆë„ ì´ìƒì˜ ë‹¨ì–´ë§Œ ì–´íœ˜ì— í¬í•¨
        filtered_words = [word for word, count in word_counts.items() if count >= min_freq]
        
        # íŠ¹ìˆ˜ í† í° ì •ì˜
        special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
        
        # ìµœì¢… ì–´íœ˜ êµ¬ì„±
        self.vocab = special_tokens + sorted(filtered_words)
        self.word_to_id = {word: i for i, word in enumerate(self.vocab)}
        self.id_to_word = {i: word for i, word in enumerate(self.vocab)}
        self.vocab_size = len(self.vocab)
        
        # íŠ¹ìˆ˜ í† í° ID
        self.pad_id = self.word_to_id['<PAD>']
        self.unk_id = self.word_to_id['<UNK>']
        self.bos_id = self.word_to_id['<BOS>']
        self.eos_id = self.word_to_id['<EOS>']
        
        print(f"   ì „ì²´ ë‹¨ì–´ ìˆ˜: {len(all_words)}")
        print(f"   ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(word_counts)}")
        print(f"   í•„í„°ë§ í›„ ì–´íœ˜ í¬ê¸°: {self.vocab_size}")
        print(f"   ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ 5ê°œ: {dict(word_counts.most_common(5))}")
        print(f"   íŠ¹ìˆ˜ í† í°: {special_tokens}")
        print(f"   ì–´íœ˜ ì˜ˆì‹œ: {self.vocab[:10]}")
    
    def encode(self, text, add_special_tokens=True, show_process=False):
        words = re.findall(r'\w+', text)
        token_ids = [self.word_to_id.get(word, self.unk_id) for word in words]
        
        if add_special_tokens:
            token_ids = [self.bos_id] + token_ids + [self.eos_id]
        
        if show_process:
            print(f"     ì¸ì½”ë”©: '{text}'")
            print(f"     ë‹¨ì–´ ë¶„í• : {words}")
            print(f"     í† í° ID: {token_ids}")
            decoded_words = [self.id_to_word[id] for id in token_ids]
            print(f"     ê²€ì¦: {decoded_words}")
        
        return token_ids
    
    def decode(self, token_ids, remove_special_tokens=True):
        if remove_special_tokens:
            # íŠ¹ìˆ˜ í† í° ì œê±°
            token_ids = [id for id in token_ids if id not in [self.pad_id, self.bos_id, self.eos_id]]
        
        words = [self.id_to_word.get(id, '<UNK>') for id in token_ids]
        return ' '.join(words)
    
    def batch_encode(self, texts, max_length=None, padding=True):
        # ë°°ì¹˜ ì¸ì½”ë”©
        encoded_texts = [self.encode(text) for text in texts]
        
        if max_length is None:
            max_length = max(len(seq) for seq in encoded_texts)
        
        if padding:
            padded_texts = []
            for seq in encoded_texts:
                if len(seq) > max_length:
                    seq = seq[:max_length]
                else:
                    seq = seq + [self.pad_id] * (max_length - len(seq))
                padded_texts.append(seq)
            return padded_texts
        
        return encoded_texts

# ìœ„ì¹˜ ì¸ì½”ë”© í´ë˜ìŠ¤ (sin/cos ê¸°ë°˜)
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        super().__init__()
        self.d_model = d_model
        self.max_len = max_len
        
        # ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ ìƒì„±
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # div_term ê³„ì‚°: 1 / (10000^(2i/d_model))
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        # sinê³¼ cosë¥¼ ë²ˆê°ˆì•„ ì ìš©
        pe[:, 0::2] = torch.sin(position * div_term)  # ì§ìˆ˜ ì¸ë±ìŠ¤
        pe[:, 1::2] = torch.cos(position * div_term)  # í™€ìˆ˜ ì¸ë±ìŠ¤
        
        pe = pe.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        self.register_buffer('pe', pe)
        
        print(f"\nğŸ“ ìœ„ì¹˜ ì¸ì½”ë”© ì´ˆê¸°í™”:")
        print(f"   ìµœëŒ€ ê¸¸ì´: {max_len}")
        print(f"   ëª¨ë¸ ì°¨ì›: {d_model}")
        print(f"   ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ ëª¨ì–‘: {pe.shape}")
        print(f"   div_term ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): {div_term[:5].numpy()}")
    
    def forward(self, x, show_process=False):
        seq_len = x.size(1)
        pos_encoding = self.pe[:, :seq_len]
        
        if show_process:
            print(f"\n   ğŸ“ ìœ„ì¹˜ ì¸ì½”ë”© ì ìš©:")
            print(f"     ì…ë ¥ ëª¨ì–‘: {x.shape}")
            print(f"     ìœ„ì¹˜ ì¸ì½”ë”© ëª¨ì–‘: {pos_encoding.shape}")
            print(f"     ìœ„ì¹˜ 0ì˜ ì¸ì½”ë”© (ì²˜ìŒ 8ì°¨ì›): {safe_numpy(pos_encoding[0, 0, :8])}")
            print(f"     ìœ„ì¹˜ 1ì˜ ì¸ì½”ë”© (ì²˜ìŒ 8ì°¨ì›): {safe_numpy(pos_encoding[0, 1, :8])}")
        
        return x + pos_encoding

# ë§ˆìŠ¤í‚¹ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_padding_mask(seq, pad_id=0):
    # íŒ¨ë”© í† í° ìœ„ì¹˜ë¥¼ Falseë¡œ ë§ˆí‚¹
    return (seq != pad_id).unsqueeze(1).unsqueeze(2)

def create_causal_mask(seq_len):
    # í•˜ì‚¼ê° ë§ˆìŠ¤í¬ ìƒì„± (ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•¨)
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask == 0  # TrueëŠ” í—ˆìš©, FalseëŠ” ë§ˆìŠ¤í‚¹

def demonstrate_masking(tokenizer):
    print(f"\nğŸ­ STEP 3: ë§ˆìŠ¤í‚¹ ë©”ì»¤ë‹ˆì¦˜ ì´í•´")
    print("-" * 80)
    
    # ìƒ˜í”Œ ì‹œí€€ìŠ¤ ìƒì„±
    sample_text = "ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€ í•˜ë£¨"
    tokens = tokenizer.encode(sample_text)
    
    # íŒ¨ë”© ì¶”ê°€
    max_len = 8
    if len(tokens) < max_len:
        tokens.extend([tokenizer.pad_id] * (max_len - len(tokens)))
    else:
        tokens = tokens[:max_len]
    
    token_tensor = torch.tensor([tokens])
    
    print(f"   ìƒ˜í”Œ í…ìŠ¤íŠ¸: '{sample_text}'")
    print(f"   í† í° ID: {tokens}")
    token_words = [tokenizer.id_to_word[id] for id in tokens]
    print(f"   í† í° ë‹¨ì–´: {token_words}")
    
    # 1. íŒ¨ë”© ë§ˆìŠ¤í¬
    padding_mask = create_padding_mask(token_tensor, tokenizer.pad_id)
    print(f"\n   1ï¸âƒ£ íŒ¨ë”© ë§ˆìŠ¤í¬ (True=ìœ íš¨í•œ í† í°, False=íŒ¨ë”©):")
    print(f"   ë§ˆìŠ¤í¬ ëª¨ì–‘: {padding_mask.shape}")
    mask_1d = padding_mask[0, 0, 0].numpy()
    for i, (word, mask_val) in enumerate(zip(token_words, mask_1d)):
        status = "âœ“ìœ íš¨" if mask_val else "âœ—íŒ¨ë”©"
        print(f"     ìœ„ì¹˜ {i}: '{word}' -> {status}")
    
    # 2. ì¸ê³¼ì  ë§ˆìŠ¤í¬ (Causal Mask)
    causal_mask = create_causal_mask(len(tokens))
    print(f"\n   2ï¸âƒ£ ì¸ê³¼ì  ë§ˆìŠ¤í¬ (True=ì°¸ì¡°ê°€ëŠ¥, False=ë¯¸ë˜í† í°):")
    print(f"   ë§ˆìŠ¤í¬ ëª¨ì–‘: {causal_mask.shape}")
    print(f"   ê° í† í°ì´ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” í† í°ë“¤:")
    
    for i, word in enumerate(token_words):
        allowed_positions = [j for j, allowed in enumerate(causal_mask[i]) if allowed]
        allowed_words = [token_words[j] for j in allowed_positions if j < len(token_words)]
        print(f"     '{word}' -> {allowed_words}")
    
    # 3. ê²°í•©ëœ ë§ˆìŠ¤í¬
    combined_mask = padding_mask & causal_mask.unsqueeze(0).unsqueeze(0)
    print(f"\n   3ï¸âƒ£ ê²°í•©ëœ ë§ˆìŠ¤í¬ (íŒ¨ë”© + ì¸ê³¼ì ):")
    print(f"   ìµœì¢… ì–´í…ì…˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë§ˆìŠ¤í¬")
    print(f"   ë§ˆìŠ¤í¬ í–‰ë ¬ ì‹œê°í™”:")
    
    final_mask = combined_mask[0, 0].numpy()
    print(f"{'':>12}", end="")
    for word in token_words:
        print(f"{word[:6]:>8}", end="")
    print()
    
    for i, query_word in enumerate(token_words):
        print(f"{query_word[:10]:>12}", end="")
        for j in range(len(token_words)):
            symbol = "âœ“" if final_mask[i, j] else "âœ—"
            print(f"{symbol:>8}", end="")
        print()
    
    return token_tensor, combined_mask

# ìŠ¤ì¼€ì¼ë“œ ë‹·-í”„ë¡œë•íŠ¸ ì–´í…ì…˜
class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k, dropout=0.1):
        super().__init__()
        self.d_k = d_k
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, Q, K, V, mask=None, show_process=False):
        batch_size, n_heads, seq_len, d_k = Q.shape
        
        if show_process:
            print(f"\n   ğŸ¯ ìŠ¤ì¼€ì¼ë“œ ë‹·-í”„ë¡œë•íŠ¸ ì–´í…ì…˜ ê³„ì‚°:")
            print(f"     Q ëª¨ì–‘: {Q.shape}")
            print(f"     K ëª¨ì–‘: {K.shape}")  
            print(f"     V ëª¨ì–‘: {V.shape}")
            print(f"     ìŠ¤ì¼€ì¼ë§ íŒ©í„°: 1/âˆš{d_k} = {1/math.sqrt(d_k):.4f}")
        
        # 1. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°: Q @ K^T / âˆšd_k
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        if show_process:
            print(f"     ì–´í…ì…˜ ìŠ¤ì½”ì–´ ëª¨ì–‘: {scores.shape}")
            # ì²« ë²ˆì§¸ í—¤ë“œì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œ ìŠ¤ì½”ì–´ ì¶œë ¥
            sample_scores = safe_numpy(scores[0, 0, :3, :3])
            print(f"     ìŠ¤ì½”ì–´ ì˜ˆì‹œ (ì²« í—¤ë“œ, 3x3):")
            for i in range(3):
                print(f"       {sample_scores[i]}")
        
        # 2. ë§ˆìŠ¤í¬ ì ìš©
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            if show_process:
                print(f"     ë§ˆìŠ¤í¬ ì ìš© ì™„ë£Œ (False ìœ„ì¹˜ë¥¼ -1e9ë¡œ ì„¤ì •)")
        
        # 3. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        if show_process:
            print(f"     ì†Œí”„íŠ¸ë§¥ìŠ¤ í›„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜:")
            sample_weights = safe_numpy(attention_weights[0, 0, :3, :3])
            print(f"     ê°€ì¤‘ì¹˜ ì˜ˆì‹œ (ì²« í—¤ë“œ, 3x3):")
            for i in range(3):
                row_sum = sample_weights[i].sum()
                print(f"       {sample_weights[i]} (í•©ê³„: {row_sum:.3f})")
        
        # 4. Valueì™€ ê°€ì¤‘í•©
        context = torch.matmul(attention_weights, V)
        
        if show_process:
            print(f"     ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ë²¡í„° ëª¨ì–‘: {context.shape}")
        
        return context, attention_weights

# ë©€í‹°í—¤ë“œ ì–´í…ì…˜
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # ì„ í˜• ë³€í™˜ ë ˆì´ì–´ë“¤
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, d_model, bias=False)
        self.W_v = nn.Linear(d_model, d_model, bias=False)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k, dropout)
        
        print(f"\nğŸ­ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ì´ˆê¸°í™”:")
        print(f"   ëª¨ë¸ ì°¨ì›: {d_model}")
        print(f"   í—¤ë“œ ìˆ˜: {n_heads}")
        print(f"   í—¤ë“œë‹¹ ì°¨ì›: {self.d_k}")
        print(f"   ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, query, key, value, mask=None, show_process=False):
        batch_size, seq_len, d_model = query.shape
        
        if show_process:
            print(f"\n   ğŸ¯ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ forward:")
            print(f"     ì…ë ¥ ëª¨ì–‘: {query.shape}")
        
        # 1. Q, K, V ë³€í™˜
        Q = self.W_q(query)
        K = self.W_k(key)  
        V = self.W_v(value)
        
        if show_process:
            print(f"     Q ë³€í™˜ í›„: {Q.shape}")
            print(f"     K ë³€í™˜ í›„: {K.shape}")
            print(f"     V ë³€í™˜ í›„: {V.shape}")
        
        # 2. ë©€í‹°í—¤ë“œë¡œ ë¶„í•  ë° reshape
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        if show_process:
            print(f"     ë©€í‹°í—¤ë“œ ë¶„í•  í›„ Q: {Q.shape}")
            print(f"     [ë°°ì¹˜, í—¤ë“œ, ì‹œí€€ìŠ¤, í—¤ë“œì°¨ì›]")
        
        # 3. ì–´í…ì…˜ ê³„ì‚°
        context, attention_weights = self.attention(Q, K, V, mask, show_process)
        
        # 4. í—¤ë“œë“¤ì„ ë‹¤ì‹œ ì—°ê²°
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        # 5. ìµœì¢… ì„ í˜• ë³€í™˜
        output = self.W_o(context)
        
        if show_process:
            print(f"     í—¤ë“œ ì—°ê²° í›„: {context.shape}")
            print(f"     ìµœì¢… ì¶œë ¥: {output.shape}")
        
        return output, attention_weights

# í¬ì§€ì…˜-ì™€ì´ì¦ˆ í”¼ë“œí¬ì›Œë“œ
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.d_ff = d_ff
        
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()
        
        print(f"\nğŸ”„ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”:")
        print(f"   ì…ë ¥ ì°¨ì›: {d_model}")
        print(f"   ì€ë‹‰ ì°¨ì›: {d_ff}")
        print(f"   ì¶œë ¥ ì°¨ì›: {d_model}")
        print(f"   í™œì„±í™” í•¨ìˆ˜: ReLU")
        print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, x, show_process=False):
        if show_process:
            print(f"\n   ğŸ”„ í”¼ë“œí¬ì›Œë“œ ì²˜ë¦¬:")
            print(f"     ì…ë ¥ ëª¨ì–‘: {x.shape}")
            print(f"     ì…ë ¥ ê°’ ë²”ìœ„: [{x.min().item():.3f}, {x.max().item():.3f}]")
        
        # ì²« ë²ˆì§¸ ì„ í˜• ë³€í™˜ + ReLU
        hidden = self.activation(self.linear1(x))
        
        if show_process:
            print(f"     ì€ë‹‰ì¸µ ëª¨ì–‘: {hidden.shape}")
            print(f"     ì€ë‹‰ì¸µ ê°’ ë²”ìœ„: [{hidden.min().item():.3f}, {hidden.max().item():.3f}]")
            print(f"     ReLU í›„ 0 ê°’ ë¹„ìœ¨: {(hidden == 0).float().mean().item():.3f}")
        
        # ë“œë¡­ì•„ì›ƒ
        hidden = self.dropout(hidden)
        
        # ë‘ ë²ˆì§¸ ì„ í˜• ë³€í™˜
        output = self.linear2(hidden)
        
        if show_process:
            print(f"     ìµœì¢… ì¶œë ¥ ëª¨ì–‘: {output.shape}")
            print(f"     ì¶œë ¥ ê°’ ë²”ìœ„: [{output.min().item():.3f}, {output.max().item():.3f}]")
        
        return output

# íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ë ˆì´ì–´
class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
        print(f"\nğŸ—ï¸ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ë ˆì´ì–´ ì´ˆê¸°í™”:")
        print(f"   ë ˆì´ì–´ êµ¬ì„±: Self-Attention + FFN")
        print(f"   ì”ì°¨ ì—°ê²° + ë ˆì´ì–´ ì •ê·œí™” ì ìš©")
        print(f"   ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, x, mask=None, show_process=False):
        if show_process:
            print(f"\n   ğŸ—ï¸ ë””ì½”ë” ë ˆì´ì–´ ì²˜ë¦¬:")
            print(f"     ì…ë ¥ ëª¨ì–‘: {x.shape}")
            print(f"     ì…ë ¥ í‰ê· /í‘œì¤€í¸ì°¨: {x.mean().item():.4f} / {x.std().item():.4f}")
        
        # 1. ì…€í”„ ì–´í…ì…˜ + ì”ì°¨ ì—°ê²° + ì •ê·œí™”
        residual = x
        attn_output, attention_weights = self.self_attention(x, x, x, mask, show_process)
        x = self.norm1(residual + self.dropout(attn_output))
        
        if show_process:
            print(f"     ì–´í…ì…˜ í›„ í‰ê· /í‘œì¤€í¸ì°¨: {x.mean().item():.4f} / {x.std().item():.4f}")
        
        # 2. í”¼ë“œí¬ì›Œë“œ + ì”ì°¨ ì—°ê²° + ì •ê·œí™”  
        residual = x
        ff_output = self.feed_forward(x, show_process)
        x = self.norm2(residual + self.dropout(ff_output))
        
        if show_process:
            print(f"     FFN í›„ í‰ê· /í‘œì¤€í¸ì°¨: {x.mean().item():.4f} / {x.std().item():.4f}")
        
        return x, attention_weights

# ë©”ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ ì–¸ì–´ ëª¨ë¸
class TransformerLanguageModel(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_len, dropout=0.1):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # ì„ë² ë”© ë ˆì´ì–´ë“¤
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ìŠ¤íƒ
        self.decoder_layers = nn.ModuleList([
            TransformerDecoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.layer_norm = nn.LayerNorm(d_model)
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
        # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        self.init_weights()
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"\nğŸ›ï¸ íŠ¸ëœìŠ¤í¬ë¨¸ ì–¸ì–´ ëª¨ë¸ ì™„ì„±:")
        print(f"   ì–´íœ˜ í¬ê¸°: {vocab_size:,}")
        print(f"   ëª¨ë¸ ì°¨ì›: {d_model}")
        print(f"   í—¤ë“œ ìˆ˜: {n_heads}")
        print(f"   ë ˆì´ì–´ ìˆ˜: {n_layers}")
        print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        print(f"   ëª¨ë¸ í¬ê¸°: {total_params * 4 / 1024 / 1024:.2f} MB")
    
    def init_weights(self):
        # Xavier ì´ˆê¸°í™”
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=self.d_model ** -0.5)
    
    def forward(self, x, show_process=False, return_attention=False):
        batch_size, seq_len = x.shape
        
        if show_process:
            print(f"\nğŸš€ ëª¨ë¸ FORWARD ì‹œì‘:")
            print(f"   ì…ë ¥ í…ì„œ ëª¨ì–‘: {x.shape}")
            print(f"   í† í° ID ë²”ìœ„: [{x.min().item()}, {x.max().item()}]")
        
        # 1. ë§ˆìŠ¤í¬ ìƒì„±
        padding_mask = create_padding_mask(x)
        causal_mask = create_causal_mask(seq_len)
        
        # ë§ˆìŠ¤í¬ ê²°í•©: íŒ¨ë”© ë§ˆìŠ¤í¬ì™€ ì¸ê³¼ ë§ˆìŠ¤í¬
        if causal_mask.device != x.device:
            causal_mask = causal_mask.to(x.device)
        
        combined_mask = padding_mask & causal_mask.unsqueeze(0).unsqueeze(0)
        
        if show_process:
            print(f"   íŒ¨ë”© ë§ˆìŠ¤í¬ ëª¨ì–‘: {padding_mask.shape}")
            print(f"   ì¸ê³¼ ë§ˆìŠ¤í¬ ëª¨ì–‘: {causal_mask.shape}")
            print(f"   ê²°í•© ë§ˆìŠ¤í¬ ëª¨ì–‘: {combined_mask.shape}")
        
        # 2. í† í° ì„ë² ë”©
        token_embeddings = self.token_embedding(x)
        
        if show_process:
            print(f"\n   ğŸ“ í† í° ì„ë² ë”©:")
            print(f"     ì„ë² ë”© í›„ ëª¨ì–‘: {token_embeddings.shape}")
            print(f"     ì„ë² ë”© ê°’ ë²”ìœ„: [{token_embeddings.min().item():.3f}, {token_embeddings.max().item():.3f}]")
            print(f"     ì²« ë²ˆì§¸ í† í° ì„ë² ë”© (ì²˜ìŒ 8ì°¨ì›): {safe_numpy(token_embeddings[0, 0, :8])}")
        
        # 3. ì„ë² ë”© ìŠ¤ì¼€ì¼ë§ (ë…¼ë¬¸ì—ì„œ ì œì•ˆ)
        token_embeddings = token_embeddings * math.sqrt(self.d_model)
        
        if show_process:
            print(f"     ìŠ¤ì¼€ì¼ë§ í›„ ë²”ìœ„: [{token_embeddings.min().item():.3f}, {token_embeddings.max().item():.3f}]")
        
        # 4. ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        x = self.position_encoding(token_embeddings, show_process=show_process)
        x = self.dropout(x)
        
        if show_process:
            print(f"     ë“œë¡­ì•„ì›ƒ í›„ ëª¨ì–‘: {x.shape}")
        
        # 5. íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ë ˆì´ì–´ë“¤ì„ í†µê³¼
        attention_weights_list = []
        
        for i, decoder_layer in enumerate(self.decoder_layers):
            if show_process:
                print(f"\n   ğŸ—ï¸ ë””ì½”ë” ë ˆì´ì–´ {i+1}/{len(self.decoder_layers)}:")
            
            x, attention_weights = decoder_layer(x, combined_mask, show_process=(show_process and i == 0))
            
            if return_attention:
                attention_weights_list.append(attention_weights)
            
            if show_process:
                print(f"     ë ˆì´ì–´ {i+1} ì¶œë ¥ ë²”ìœ„: [{x.min().item():.3f}, {x.max().item():.3f}]")
        
        # 6. ìµœì¢… ë ˆì´ì–´ ì •ê·œí™”
        x = self.layer_norm(x)
        
        if show_process:
            print(f"\n   ğŸ“Š ìµœì¢… ì •ê·œí™”:")
            print(f"     ì •ê·œí™” í›„ í‰ê· : {x.mean().item():.4f}")
            print(f"     ì •ê·œí™” í›„ í‘œì¤€í¸ì°¨: {x.std().item():.4f}")
        
        # 7. ì¶œë ¥ í”„ë¡œì ì…˜ (ì–´íœ˜ í¬ê¸°ë¡œ ë³€í™˜)
        logits = self.output_projection(x)
        
        if show_process:
            print(f"\n   ğŸ¯ ì¶œë ¥ í”„ë¡œì ì…˜:")
            print(f"     ë¡œì§“ ëª¨ì–‘: {logits.shape}")
            print(f"     ë¡œì§“ ë²”ìœ„: [{logits.min().item():.3f}, {logits.max().item():.3f}]")
            
            # ì²« ë²ˆì§¸ í† í°ì— ëŒ€í•œ ìƒìœ„ 5ê°œ ì˜ˆì¸¡ ì¶œë ¥
            first_token_logits = logits[0, 0]
            top5_values, top5_indices = torch.topk(first_token_logits, 5)
            print(f"     ì²« ë²ˆì§¸ ìœ„ì¹˜ ìƒìœ„ 5ê°œ ì˜ˆì¸¡:")
            for j, (val, idx) in enumerate(zip(top5_values, top5_indices)):
                print(f"       {j+1}. ID {idx.item()}: {val.item():.3f}")
        
        if return_attention:
            return logits, attention_weights_list
        
        return logits

# í•™ìŠµ ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜
def prepare_training_data(tokenizer, texts, max_length=32):
    print(f"\nğŸ“Š STEP 4: í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
    print("-" * 80)
    
    # í…ìŠ¤íŠ¸ë“¤ì„ í† í°í™”
    encoded_texts = []
    for text in texts:
        tokens = tokenizer.encode(text, add_special_tokens=True)
        encoded_texts.append(tokens)
    
    print(f"   ì›ë³¸ í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}")
    print(f"   í† í°í™” ì˜ˆì‹œ:")
    for i, (text, tokens) in enumerate(zip(texts[:3], encoded_texts[:3])):
        print(f"     {i+1}. '{text}'")
        print(f"        í† í°: {tokens}")
        words = [tokenizer.id_to_word[id] for id in tokens]
        print(f"        ë‹¨ì–´: {words}")
    
    # ë°°ì¹˜ íŒ¨ë”©
    padded_sequences = []
    original_lengths = []
    
    for tokens in encoded_texts:
        original_lengths.append(len(tokens))
        if len(tokens) > max_length:
            tokens = tokens[:max_length]
        else:
            tokens = tokens + [tokenizer.pad_id] * (max_length - len(tokens))
        padded_sequences.append(tokens)
    
    # ì…ë ¥ê³¼ íƒ€ê²Ÿ ì¤€ë¹„ (ë‹¤ìŒ í† í° ì˜ˆì¸¡)
    input_sequences = [seq[:-1] for seq in padded_sequences]  # ë§ˆì§€ë§‰ í† í° ì œì™¸
    target_sequences = [seq[1:] for seq in padded_sequences]  # ì²« í† í° ì œì™¸
    
    print(f"\n   íŒ¨ë”© í›„ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_length}")
    print(f"   í‰ê·  ì›ë³¸ ê¸¸ì´: {np.mean(original_lengths):.1f}")
    print(f"   ìµœëŒ€ ì›ë³¸ ê¸¸ì´: {max(original_lengths)}")
    print(f"   ì…ë ¥ ì‹œí€€ìŠ¤ ëª¨ì–‘: ({len(input_sequences)}, {len(input_sequences[0])})")
    print(f"   íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ëª¨ì–‘: ({len(target_sequences)}, {len(target_sequences[0])})")
    
    # ì…ë ¥/íƒ€ê²Ÿ ì˜ˆì‹œ ì¶œë ¥
    print(f"\n   ì…ë ¥/íƒ€ê²Ÿ ì˜ˆì‹œ (ì²« ë²ˆì§¸ ìƒ˜í”Œ):")
    print(f"     ì…ë ¥:  {input_sequences[0][:10]}...")
    print(f"     íƒ€ê²Ÿ:  {target_sequences[0][:10]}...")
    
    input_words = [tokenizer.id_to_word[id] for id in input_sequences[0][:10]]
    target_words = [tokenizer.id_to_word[id] for id in target_sequences[0][:10]]
    print(f"     ì…ë ¥ ë‹¨ì–´: {input_words}")
    print(f"     íƒ€ê²Ÿ ë‹¨ì–´: {target_words}")
    
    return torch.tensor(input_sequences), torch.tensor(target_sequences)

# í•™ìŠµ í•¨ìˆ˜
def train_model(model, train_inputs, train_targets, tokenizer, config):
    print(f"\nğŸ“ STEP 5: ëª¨ë¸ í•™ìŠµ")
    print("-" * 80)
    
    model.train()
    
    # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜
    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)
    
    print(f"   ì˜µí‹°ë§ˆì´ì €: Adam (lr={config['learning_rate']})")
    print(f"   ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss (íŒ¨ë”© í† í° ë¬´ì‹œ)")
    print(f"   ì—í­ ìˆ˜: {config['epochs']}")
    
    losses = []
    perplexities = []
    
    # ì²« ë²ˆì§¸ ì—í­ì€ ìƒì„¸íˆ ì¶œë ¥
    for epoch in range(config['epochs']):
        optimizer.zero_grad()
        
        # ìˆœì „íŒŒ
        show_detail = (epoch == 0)  # ì²« ë²ˆì§¸ ì—í­ë§Œ ìƒì„¸ ì¶œë ¥
        outputs = model(train_inputs, show_process=show_detail)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = criterion(outputs.reshape(-1, model.vocab_size), train_targets.reshape(-1))
        
        # ì—­ì „íŒŒ
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (í­ë°œ ë°©ì§€)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # í†µê³„ ê¸°ë¡
        losses.append(loss.item())
        perplexity = torch.exp(loss).item()
        perplexities.append(perplexity)
        
        # ì£¼ê¸°ì  ì¶œë ¥
        if epoch % 5 == 0 or epoch == config['epochs'] - 1:
            print(f"   ì—í­ {epoch+1:3d}: ì†ì‹¤={loss.item():.4f}, í„í”Œë ‰ì‹œí‹°={perplexity:.2f}")
        
        # ì²« ë²ˆì§¸ ì—í­ ìƒì„¸ ë¶„ì„
        if epoch == 0:
            print(f"\n   ğŸ“ˆ ì²« ë²ˆì§¸ ì—í­ ìƒì„¸ ë¶„ì„:")
            with torch.no_grad():
                sample_logits = outputs[0, 0]  # ì²« ë²ˆì§¸ ìƒ˜í”Œ, ì²« ë²ˆì§¸ í† í°
                probs = F.softmax(sample_logits, dim=-1)
                
                print(f"     ìƒ˜í”Œ ë¡œì§“ ë²”ìœ„: [{sample_logits.min().item():.3f}, {sample_logits.max().item():.3f}]")
                print(f"     ìµœëŒ€ í™•ë¥ : {probs.max().item():.4f}")
                print(f"     ì—”íŠ¸ë¡œí”¼: {(-probs * torch.log(probs + 1e-8)).sum().item():.3f}")
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´
                total_grad_norm = 0
                for param in model.parameters():
                    if param.grad is not None:
                        total_grad_norm += param.grad.data.norm(2).item() ** 2
                total_grad_norm = total_grad_norm ** 0.5
                print(f"     ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„: {total_grad_norm:.6f}")
    
    print(f"\n   ìµœì¢… ì†ì‹¤: {losses[-1]:.4f}")
    print(f"   ìµœì¢… í„í”Œë ‰ì‹œí‹°: {perplexities[-1]:.2f}")
    
    return losses, perplexities

# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
def plot_training_curves(losses, perplexities):
    print(f"\nğŸ“Š STEP 6: í•™ìŠµ ê³¡ì„  ì‹œê°í™”")
    print("-" * 80)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ì†ì‹¤ ê³¡ì„ 
    ax1.plot(losses, 'b-', linewidth=2, label='Training Loss')
    ax1.set_title('í•™ìŠµ ì†ì‹¤ (Training Loss)', fontsize=14)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # í„í”Œë ‰ì‹œí‹° ê³¡ì„ 
    ax2.plot(perplexities, 'r-', linewidth=2, label='Perplexity')
    ax2.set_title('í„í”Œë ‰ì‹œí‹° (Perplexity)', fontsize=14)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Perplexity')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    plt.show()
    
    print(f"   ì†ì‹¤ ê°ì†Œìœ¨: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%")
    print(f"   í„í”Œë ‰ì‹œí‹° ê°œì„ : {perplexities[0]:.2f} â†’ {perplexities[-1]:.2f}")

# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
def visualize_attention_patterns(model, tokenizer, text="ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€ í•˜ë£¨ì…ë‹ˆë‹¤"):
    print(f"\nğŸ­ STEP 7: ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„")
    print("-" * 80)
    
    model.eval()
    
    # í…ìŠ¤íŠ¸ í† í°í™”
    tokens = tokenizer.encode(text, add_special_tokens=True)
    max_len = 16
    if len(tokens) < max_len:
        tokens.extend([tokenizer.pad_id] * (max_len - len(tokens)))
    else:
        tokens = tokens[:max_len]
    
    input_tensor = torch.tensor([tokens]).to(device)
    token_words = [tokenizer.id_to_word[id] for id in tokens]
    
    print(f"   ë¶„ì„í•  í…ìŠ¤íŠ¸: '{text}'")
    print(f"   í† í°ë“¤: {token_words}")
    
    # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
    with torch.no_grad():
        outputs, attention_weights_list = model(input_tensor, return_attention=True)
    
    # ìœ íš¨í•œ í† í° ìˆ˜ ê³„ì‚° (íŒ¨ë”© ì œì™¸)
    valid_len = len([t for t in tokens if t != tokenizer.pad_id])
    valid_words = token_words[:valid_len]
    
    print(f"   ìœ íš¨ í† í° ìˆ˜: {valid_len}")
    
    # ê° ë ˆì´ì–´ë³„ ì–´í…ì…˜ ì‹œê°í™”
    n_layers = len(attention_weights_list)
    n_heads = attention_weights_list[0].shape[1]
    
    print(f"   ë ˆì´ì–´ ìˆ˜: {n_layers}, í—¤ë“œ ìˆ˜: {n_heads}")
    
    # ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ëª¨ë“  í—¤ë“œ ì‹œê°í™”
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.flatten()
    
    first_layer_attention = attention_weights_list[0][0]  # ì²« ë²ˆì§¸ ë°°ì¹˜
    
    for head in range(min(8, n_heads)):
        ax = axes[head]
        
        # ìœ íš¨í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        head_attention = safe_numpy(first_layer_attention[head, :valid_len, :valid_len])
        
        # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
        im = ax.imshow(head_attention, cmap='Blues', aspect='auto')
        ax.set_title(f'í—¤ë“œ {head+1}')
        ax.set_xticks(range(valid_len))
        ax.set_yticks(range(valid_len))
        ax.set_xticklabels(valid_words, rotation=45, ha='right', fontsize=8)
        ax.set_yticklabels(valid_words, fontsize=8)
        
        # ì–´í…ì…˜ ê°’ í‘œì‹œ (ë†’ì€ ê°’ë§Œ)
        for i in range(valid_len):
            for j in range(valid_len):
                if head_attention[i, j] > 0.1:  # ì„ê³„ê°’ ì´ìƒë§Œ í‘œì‹œ
                    ax.text(j, i, f'{head_attention[i, j]:.2f}',
                           ha='center', va='center', color='white', fontsize=6)
    
    plt.suptitle(f'ë©€í‹°í—¤ë“œ ì–´í…ì…˜ íŒ¨í„´ (ì²« ë²ˆì§¸ ë ˆì´ì–´)\n"{text}"', fontsize=14)
    plt.tight_layout()
    plt.show()
    
    # ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„
    print(f"\n   ğŸ” ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„:")
    for head in range(min(4, n_heads)):
        head_attention = safe_numpy(first_layer_attention[head, :valid_len, :valid_len])
        print(f"\n   í—¤ë“œ {head+1} ì£¼ìš” íŒ¨í„´:")
        
        for i, query_word in enumerate(valid_words):
            # ìê¸° ìì‹  ì œì™¸í•˜ê³  ê°€ì¥ ë†’ì€ ì–´í…ì…˜ ì°¾ê¸°
            attention_row = head_attention[i].copy()
            attention_row[i] = 0  # ìê¸° ìì‹  ì œì™¸
            
            if attention_row.max() > 0.1:  # ì˜ë¯¸ìˆëŠ” ì–´í…ì…˜ë§Œ
                max_idx = attention_row.argmax()
                max_attention = attention_row[max_idx]
                target_word = valid_words[max_idx]
                print(f"     '{query_word}' â†’ '{target_word}' ({max_attention:.3f})")

# í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ (ì˜¨ë„ ì¡°ì ˆ ê°€ëŠ¥)
def generate_text(model, tokenizer, prompt="ì•ˆë…•í•˜ì„¸ìš”", max_length=20, temperature=1.0, top_k=5):
    print(f"\nğŸ¯ STEP 8: í…ìŠ¤íŠ¸ ìƒì„±")
    print("-" * 80)
    
    model.eval()
    
    print(f"   ì‹œì‘ í”„ë¡¬í”„íŠ¸: '{prompt}'")
    print(f"   ìµœëŒ€ ê¸¸ì´: {max_length}")
    print(f"   ì˜¨ë„: {temperature}")
    print(f"   Top-K: {top_k}")
    
    # í”„ë¡¬í”„íŠ¸ í† í°í™”
    tokens = tokenizer.encode(prompt, add_special_tokens=True, show_process=True)
    generated_tokens = tokens.copy()
    
    print(f"\n   ìƒì„± ê³¼ì •:")
    
    with torch.no_grad():
        for step in range(max_length):
            # í˜„ì¬ ì‹œí€€ìŠ¤ ì¤€ë¹„
            current_sequence = generated_tokens[-model.max_seq_len + 1:] if len(generated_tokens) > model.max_seq_len - 1 else generated_tokens
            
            # íŒ¨ë”©
            padded_sequence = current_sequence + [tokenizer.pad_id] * (model.max_seq_len - 1 - len(current_sequence))
            input_tensor = torch.tensor([padded_sequence]).to(device)
            
            # ì˜ˆì¸¡
            outputs = model(input_tensor)
            next_token_logits = outputs[0, len(current_sequence) - 1]
            
            # ì˜¨ë„ ì ìš©
            if temperature != 1.0:
                next_token_logits = next_token_logits / temperature
            
            # Top-K ìƒ˜í”Œë§
            if top_k > 0:
                top_k_logits, top_k_indices = torch.topk(next_token_logits, k=min(top_k, next_token_logits.size(-1)))
                probs = F.softmax(top_k_logits, dim=-1)
                
                # ë‹¤ìŒ í† í° ìƒ˜í”Œë§
                next_token_idx = torch.multinomial(probs, 1).item()
                next_token = top_k_indices[next_token_idx].item()
                
                # ìƒìœ„ í›„ë³´ë“¤ ì¶œë ¥
                if step < 3:  # ì²˜ìŒ 3ìŠ¤í…ë§Œ ìƒì„¸ ì¶œë ¥
                    print(f"     ìŠ¤í… {step+1} í›„ë³´:")
                    for i, (prob, idx) in enumerate(zip(probs, top_k_indices)):
                        word = tokenizer.id_to_word[idx.item()]
                        print(f"       {i+1}. {word} ({prob.item():.3f})")
                    print(f"     ì„ íƒ: {tokenizer.id_to_word[next_token]}")
            else:
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()
            
            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if next_token == tokenizer.eos_id:
                print(f"     ìŠ¤í… {step+1}: <EOS> í† í° ìƒì„±ìœ¼ë¡œ ì¢…ë£Œ")
                break
            
            generated_tokens.append(next_token)
            
            # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ í…ìŠ¤íŠ¸
            current_text = tokenizer.decode(generated_tokens, remove_special_tokens=True)
            if step < 5 or step % 5 == 4:  # ì²˜ìŒ 5ìŠ¤í…ê³¼ ì´í›„ 5ìŠ¤í…ë§ˆë‹¤
                print(f"     ìŠ¤í… {step+1}: '{current_text}'")
    
    # ìµœì¢… ê²°ê³¼
    final_text = tokenizer.decode(generated_tokens, remove_special_tokens=True)
    print(f"\n   ìµœì¢… ìƒì„± í…ìŠ¤íŠ¸: '{final_text}'")
    print(f"   ìƒì„±ëœ í† í° ìˆ˜: {len(generated_tokens) - len(tokens)}")
    
    return final_text

# ë‹¤ì–‘í•œ ì˜¨ë„ë¡œ ìƒì„± ë¹„êµ
def compare_generation_temperatures(model, tokenizer, prompt="ì¸ê³µì§€ëŠ¥ì€"):
    print(f"\nğŸŒ¡ï¸ STEP 9: ì˜¨ë„ë³„ ìƒì„± ë¹„êµ")
    print("-" * 80)
    
    temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]
    
    print(f"   í”„ë¡¬í”„íŠ¸: '{prompt}'")
    print(f"   ë¹„êµí•  ì˜¨ë„: {temperatures}")
    
    for temp in temperatures:
        print(f"\n   ì˜¨ë„ {temp}:")
        description = "ë§¤ìš° ë³´ìˆ˜ì " if temp < 0.5 else "ë³´ìˆ˜ì " if temp < 1.0 else "ê· í˜•" if temp == 1.0 else "ì°½ì˜ì " if temp < 2.0 else "ë§¤ìš° ì°½ì˜ì "
        print(f"   ({description})")
        
        # ì§§ê²Œ ìƒì„±
        result = generate_text(model, tokenizer, prompt, max_length=8, temperature=temp, top_k=5)
        print(f"   ê²°ê³¼: '{result}'")

# í—¬í¼ í•¨ìˆ˜: í° í…ì„œë¥¼ ìš”ì•½í•´ì„œ ì¶œë ¥
def print_tensor_summary(tensor_array, name, show_full=False):
    if show_full or tensor_array.shape[0] <= 2:
        print(f"-- {name} --")
        print(tensor_array)
    else:
        print(f"-- {name} (showing first and last rows) --")
        # ê° í–‰ì—ì„œë„ ì²˜ìŒ 5ê°œ, ë§ˆì§€ë§‰ 5ê°œë§Œ ì¶œë ¥
        first_row = tensor_array[0]
        last_row = tensor_array[-1]
        
        if len(first_row) > 10:
            first_5 = first_row[:5]
            last_5 = first_row[-5:]
            print(f"First row: [{' '.join(f'{x:.8f}' for x in first_5)} ... {' '.join(f'{x:.8f}' for x in last_5)}]")
        else:
            print(f"First row: {first_row}")
            
        if tensor_array.shape[0] > 2:
            print(f"... ({tensor_array.shape[0]-2} rows omitted) ...")
            
        if len(last_row) > 10:
            first_5 = last_row[:5]
            last_5 = last_row[-5:]
            print(f"Last row:  [{' '.join(f'{x:.8f}' for x in first_5)} ... {' '.join(f'{x:.8f}' for x in last_5)}]")
        else:
            print(f"Last row:  {last_row}")

# ë‹¨ê³„ë³„ ë””ë²„ê¹… ë° ì‹œê°í™” í•¨ìˆ˜
def debug_transformer_step_by_step(model, tokenizer, sample_text="ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€"):
    print(f"\nğŸ” íŠ¸ëœìŠ¤í¬ë¨¸ ë‹¨ê³„ë³„ ë””ë²„ê¹…")
    print("=" * 80)
    print(f"ë¶„ì„í•  í…ìŠ¤íŠ¸: '{sample_text}'")
    
    model.eval()
    
    # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  íŒ¨ë”© ì¶”ê°€
    tokens = tokenizer.encode(sample_text, add_special_tokens=True)
    max_len = 6
    if len(tokens) > max_len:
        tokens = tokens[:max_len]
    while len(tokens) < max_len:
        tokens.append(tokenizer.pad_id)
    
    input_tensor = torch.tensor([tokens]).to(device)
    token_words = [tokenizer.id_to_word[id] for id in tokens]
    
    print(f"\n1ï¸âƒ£ ì…ë ¥ í† í° ì²˜ë¦¬")
    print("-" * 40)
    print(f"Input token ids: {safe_numpy(input_tensor)}")
    print(f"Token mapping:")
    for i, (token_id, word) in enumerate(zip(tokens, token_words)):
        print(f"  Position {i}: {token_id} -> '{word}'")
    
    padding_mask = (input_tensor == tokenizer.pad_id).float()
    print(f"Padding mask (1 where pad): {safe_numpy(padding_mask)}")
    
    with torch.no_grad():
        # í† í° ì„ë² ë”©
        token_embeddings = model.token_embedding(input_tensor)
        print(f"\n2ï¸âƒ£ í† í° ì„ë² ë”© (shape: {token_embeddings.shape})")
        print("-" * 40)
        print_tensor_summary(safe_numpy(token_embeddings[0]), "Token embeddings")
        
        # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        token_embeddings_scaled = token_embeddings * math.sqrt(model.d_model)
        x = model.position_encoding(token_embeddings_scaled)
        print(f"\n3ï¸âƒ£ ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€ (shape: {x.shape})")
        print("-" * 40)
        print_tensor_summary(safe_numpy(x[0]), "After adding positional encoding")
        
        # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ë¶„ì„
        print(f"\n4ï¸âƒ£ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ìƒì„¸ ë¶„ì„")
        print("-" * 40)
        
        first_attention = model.decoder_layers[0].self_attention
        Q = first_attention.W_q(x)
        K = first_attention.W_k(x)
        V = first_attention.W_v(x)
        
        print(f"Q shape: {Q.shape}")
        
        # Q ìƒ˜í”Œ ìš”ì•½ ì¶œë ¥ (ì²˜ìŒ 2ê°œ í† í°ë§Œ)
        q_sample = safe_numpy(Q[0, :2])
        print(f"Q sample (first 2 tokens, showing first/last 5 values):")
        for i, row in enumerate(q_sample):
            if len(row) > 10:
                first_5 = row[:5]
                last_5 = row[-5:]
                print(f"  Token {i}: [{' '.join(f'{x:.8f}' for x in first_5)} ... {' '.join(f'{x:.8f}' for x in last_5)}]")
            else:
                print(f"  Token {i}: {row}")
        
        # ë©€í‹°í—¤ë“œë¡œ ë¶„í• 
        batch_size, seq_len, d_model = Q.shape
        Q = Q.view(batch_size, seq_len, first_attention.n_heads, first_attention.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, first_attention.n_heads, first_attention.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, first_attention.n_heads, first_attention.d_k).transpose(1, 2)
        
        print(f"Q split into heads shape: {Q.shape}")
        
        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° (ì²« ë²ˆì§¸ í—¤ë“œë§Œ)
        scores = torch.matmul(Q[0, 0], K[0, 0].transpose(-1, -2)) / math.sqrt(first_attention.d_k)
        print(f"Raw attention scores (before mask & softmax):")
        print(safe_numpy(scores))
        
        # ë§ˆìŠ¤í¬ ì ìš©
        padding_mask_expanded = (input_tensor == tokenizer.pad_id).unsqueeze(1).unsqueeze(2)
        causal_mask = create_causal_mask(seq_len).to(device)
        combined_mask = padding_mask_expanded & causal_mask.unsqueeze(0).unsqueeze(0)
        
        mask_for_attention = combined_mask[0, 0]
        scores_masked = scores.clone()
        scores_masked = scores_masked.masked_fill(~mask_for_attention, -1e9)
        
        print(f"Scores after applying mask:")
        print(safe_numpy(scores_masked))
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©
        attention_weights = F.softmax(scores_masked, dim=-1)
        print(f"Attention weights (softmaxed):")
        print(safe_numpy(attention_weights))
        
        # Valueì™€ ê³±ì…ˆ
        context = torch.matmul(attention_weights.unsqueeze(0), V[0, 0].unsqueeze(0))
        print(f"Output per head sample:")
        output_per_head = safe_numpy(context[0])
        for i, row in enumerate(output_per_head):
            if len(row) > 10:
                first_5 = row[:5]
                last_5 = row[-5:]
                print(f"  Head output {i}: [{' '.join(f'{x:.8f}' for x in first_5)} ... {' '.join(f'{x:.8f}' for x in last_5)}]")
            else:
                print(f"  Head output {i}: {row}")
        
        # ì „ì²´ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ì‹¤í–‰
        attn_output, _ = first_attention(x, x, x, combined_mask)
        print(f"\n5ï¸âƒ£ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ìµœì¢… ì¶œë ¥ (shape: {attn_output.shape})")
        print("-" * 40)
        print_tensor_summary(safe_numpy(attn_output[0]), "Final MHA output")
        
        # í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
        ff_output = model.decoder_layers[0].feed_forward(attn_output)
        print(f"\n6ï¸âƒ£ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (shape: {ff_output.shape})")
        print("-" * 40)
        print_tensor_summary(safe_numpy(ff_output[0]), "Feed-forward output")
        
        # ì”ì°¨ ì—°ê²° ë° ë ˆì´ì–´ ì •ê·œí™”
        print(f"\n7ï¸âƒ£ ì”ì°¨ ì—°ê²° ë° ë ˆì´ì–´ ì •ê·œí™”")
        print("-" * 40)
        
        x_after_attn = model.decoder_layers[0].norm1(x + model.decoder_layers[0].dropout(attn_output))
        print(f"After attention + residual + norm: Mean={x_after_attn.mean().item():.4f}, Std={x_after_attn.std().item():.4f}")
        
        x_final = model.decoder_layers[0].norm2(x_after_attn + model.decoder_layers[0].dropout(ff_output))
        print(f"After feedforward + residual + norm: Mean={x_final.mean().item():.4f}, Std={x_final.std().item():.4f}")
        
        # ìµœì¢… ì¶œë ¥ í”„ë¡œì ì…˜
        print(f"\n8ï¸âƒ£ ìµœì¢… ì¶œë ¥ ë° ì˜ˆì¸¡")
        print("-" * 40)
        
        # ëª¨ë“  ë ˆì´ì–´ í†µê³¼
        x_all_layers = x
        for layer in model.decoder_layers:
            x_all_layers, _ = layer(x_all_layers, combined_mask)
        
        x_all_layers = model.layer_norm(x_all_layers)
        logits = model.output_projection(x_all_layers)
        
        print(f"Final logits shape: {logits.shape}")
        print(f"Logits range: [{logits.min().item():.3f}, {logits.max().item():.3f}]")
        
        # ê° ìœ„ì¹˜ë³„ ìƒìœ„ ì˜ˆì¸¡ ì¶œë ¥ (ìœ íš¨í•œ í† í°ë§Œ)
        print(f"Top predictions per position:")
        for pos in range(seq_len):
            if tokens[pos] != tokenizer.pad_id:
                pos_logits = logits[0, pos]
                top3_values, top3_indices = torch.topk(pos_logits, 3)
                print(f"  Position {pos} ('{token_words[pos]}'):")
                for i, (val, idx) in enumerate(zip(top3_values, top3_indices)):
                    predicted_word = tokenizer.id_to_word[idx.item()]
                    prob = F.softmax(pos_logits, dim=-1)[idx].item()
                    print(f"    {i+1}. {predicted_word} (logit: {val.item():.3f}, prob: {prob:.3f})")
        
        print(f"\n9ï¸âƒ£ ìš”ì•½")
        print("-" * 40)
        print(f"âœ“ ì…ë ¥: '{sample_text}' â†’ {len([t for t in tokens if t != tokenizer.pad_id])} í† í°")
        print(f"âœ“ ì²˜ë¦¬: ì„ë² ë”© â†’ ìœ„ì¹˜ì¸ì½”ë”© â†’ {len(model.decoder_layers)}ê°œ ë ˆì´ì–´ â†’ ì¶œë ¥({logits.shape})")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    # í† í¬ë‚˜ì´ì € ìƒì„±
    tokenizer = AdvancedTokenizer(sample_texts)
    
    # ë§ˆìŠ¤í‚¹ ì‹œì—°
    sample_input, mask = demonstrate_masking(tokenizer)
    
    # ëª¨ë¸ ìƒì„±
    model = TransformerLanguageModel(
        vocab_size=tokenizer.vocab_size,
        d_model=config['d_model'],
        n_heads=config['n_heads'],
        n_layers=config['n_layers'],
        d_ff=config['d_ff'],
        max_seq_len=config['max_seq_len'],
        dropout=config['dropout']
    ).to(device)
    
    # ë‹¨ê³„ë³„ ë””ë²„ê¹… (í•™ìŠµ ì „ - ì´ˆê¸° ìƒíƒœ)
    print(f"\n" + "="*100)
    print("ğŸ” í•™ìŠµ ì „ ëª¨ë¸ ìƒíƒœ ë¶„ì„")
    print("="*100)
    debug_transformer_step_by_step(model, tokenizer, "ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€")
    
    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    train_inputs, train_targets = prepare_training_data(tokenizer, sample_texts, max_length=config['max_seq_len'] - 1)
    train_inputs = train_inputs.to(device)
    train_targets = train_targets.to(device)
    
    # ëª¨ë¸ í•™ìŠµ
    losses, perplexities = train_model(model, train_inputs, train_targets, tokenizer, config)
    
    # ë‹¨ê³„ë³„ ë””ë²„ê¹… (í•™ìŠµ í›„ - í›ˆë ¨ëœ ìƒíƒœ)
    print(f"\n" + "="*100)
    print("ğŸ” í•™ìŠµ í›„ ëª¨ë¸ ìƒíƒœ ë¶„ì„")
    print("="*100)
    debug_transformer_step_by_step(model, tokenizer, "ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€")
    
    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plot_training_curves(losses, perplexities)
    
    # ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„
    visualize_attention_patterns(model, tokenizer)
    
    # í…ìŠ¤íŠ¸ ìƒì„±
    generate_text(model, tokenizer, "ì•ˆë…•í•˜ì„¸ìš”", max_length=15, temperature=1.0)
    
    # ì˜¨ë„ë³„ ìƒì„± ë¹„êµ
    compare_generation_temperatures(model, tokenizer)
    
    print(f"\nğŸ‰ STEP 10: í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    print("Transformer ì–¸ì–´ ìƒì„± ëª¨ë¸ì˜ í•µì‹¬ ê°œë…ë“¤ì„ ëª¨ë‘ í•™ìŠµí–ˆìŠµë‹ˆë‹¤!")
    print()
    print("ğŸ“š í•™ìŠµí•œ í•µì‹¬ ê°œë…ë“¤:")
    print("   1. í† í¬ë‚˜ì´ì œì´ì…˜: í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜")
    print("   2. ì„ë² ë”©: í† í°ì„ ë²¡í„°ë¡œ í‘œí˜„") 
    print("   3. ìœ„ì¹˜ ì¸ì½”ë”©: ìˆœì„œ ì •ë³´ ì¶”ê°€")
    print("   4. ë§ˆìŠ¤í‚¹: íŒ¨ë”©ê³¼ ë¯¸ë˜ í† í° ì œì–´")
    print("   5. ë©€í‹°í—¤ë“œ ì–´í…ì…˜: ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ê´€ê³„ í•™ìŠµ")
    print("   6. Q, K, V ë©”ì»¤ë‹ˆì¦˜: ì–´í…ì…˜ ê³„ì‚° ê³¼ì •")
    print("   7. ìŠ¤ì¼€ì¼ë“œ ë‹·-í”„ë¡œë•íŠ¸: ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°")
    print("   8. ì†Œí”„íŠ¸ë§¥ìŠ¤: í™•ë¥  ë¶„í¬ ë³€í™˜")
    print("   9. í”¼ë“œí¬ì›Œë“œ: ë¹„ì„ í˜• ë³€í™˜")
    print("   10. ì”ì°¨ ì—°ê²° & ì¸µ ì •ê·œí™”: í•™ìŠµ ì•ˆì •í™”")
    print("   11. ì–¸ì–´ ëª¨ë¸ë§: ë‹¤ìŒ í† í° ì˜ˆì¸¡")
    print("   12. í…ìŠ¤íŠ¸ ìƒì„±: ìê¸°íšŒê·€ì  ë””ì½”ë”©")
    print("   13. ì˜¨ë„ ì¡°ì ˆ: ìƒì„± ë‹¤ì–‘ì„± ì œì–´")
    print("   14. Top-K ìƒ˜í”Œë§: í’ˆì§ˆ ìˆëŠ” ë‹¤ì–‘ì„±")
    print()
    print("ğŸ”¬ í•µì‹¬ ìˆ˜ì‹ë“¤:")
    print("   â€¢ Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V")
    print("   â€¢ MultiHead = Concat(head_1,...,head_h)W^O")  
    print("   â€¢ LayerNorm(x + Sublayer(x))")
    print("   â€¢ PE(pos,2i) = sin(pos/10000^(2i/d_model))")
    print("   â€¢ PE(pos,2i+1) = cos(pos/10000^(2i/d_model))")
    print()
    print("ğŸ¯ ì‹¤ìš©ì  í™œìš©:")
    print("   â€¢ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ (ì–¸ì–´ ëª¨ë¸ë§)")
    print("   â€¢ ì°½ì˜ì  í…ìŠ¤íŠ¸ ìƒì„±")
    print("   â€¢ ëŒ€í™” ì‹œìŠ¤í…œ")
    print("   â€¢ ì½”ë“œ ìƒì„±")
    print("   â€¢ ë²ˆì—­ ì‹œìŠ¤í…œ")
    print()
    print("=" * 80)

if __name__ == "__main__":
    main()
