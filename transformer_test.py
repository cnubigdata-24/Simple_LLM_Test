!apt-get -qq install fonts-nanum
!fc-cache -fv
!rm -rf ~/.cache/matplotlib

import matplotlib.pyplot as plt
plt.rc('font', family='NanumGothic')
import matplotlib as mpl
mpl.rcParams['axes.unicode_minus'] = False

# ===============================================================================
# ğŸ¤– Transformer ê¸°ë°˜ ê°„ë‹¨í•œ ì–¸ì–´ëª¨ë¸ ë§Œë“¤ê¸° & ì´í•´í•˜ê¸°
# ===============================================================================
# ì´ ì˜ˆì œë¥¼ í†µí•´ ë°°ìš¸ ìˆ˜ ìˆëŠ” ê²ƒë“¤:
# 1. í† í¬ë‚˜ì´ì €ì™€ ì¸ì½”ë”©/ì„ë² ë”©ì˜ ì—­í• 
# 2. Multi-Head Attentionì˜ ì‘ë™ ì›ë¦¬
# 3. ì˜¨ë„(Temperature)ê°€ ìƒì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
# 4. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ì‹œê°í™”ë¥¼ í†µí•œ ëª¨ë¸ ì´í•´
# 5. ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì˜ ë‹¤ì–‘í•œ ê´€ì 
# ===============================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
print("=" * 80)

# ===============================================================================
# ğŸ“š 1ë‹¨ê³„: í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ë° í† í¬ë‚˜ì´ì € êµ¬í˜„
# ===============================================================================

# í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ë¡œë“œ
def load_training_data(file_path='training_data.txt'):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            texts = [line.strip() for line in f.readlines() if line.strip()]
        print(f"ğŸ“– {len(texts)}ê°œì˜ í•™ìŠµ ë¬¸ì¥ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return texts
    except FileNotFoundError:
        print(f"âš ï¸ íŒŒì¼ '{file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()

# í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤ - í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ì—­í• 
class SimpleTokenizer:
    def __init__(self, texts):
        print("ğŸ”¤ í† í¬ë‚˜ì´ì € ìƒì„± ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        all_text = " ".join(texts)
        words = re.findall(r'\S+', all_text)
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(words)
        print(f"ğŸ“Š ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(word_counts)}")
        print(f"ğŸ“ˆ ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ë“¤: {dict(word_counts.most_common(5))}")
        
        # íŠ¹ìˆ˜ í† í° + ì–´íœ˜ êµ¬ì„±
        self.vocab = ['<PAD>', '<UNK>', '<START>', '<END>'] + list(word_counts.keys())
        self.word_to_id = {word: i for i, word in enumerate(self.vocab)}
        self.id_to_word = {i: word for i, word in enumerate(self.vocab)}
        self.vocab_size = len(self.vocab)
        
        print(f"âœ… ì „ì²´ ì–´íœ˜ í¬ê¸°: {self.vocab_size}")
        print(f"ğŸ“š ì–´íœ˜ ì˜ˆì‹œ: {self.vocab[:8]}")
    
    # í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ë³€í™˜ (ì¸ì½”ë”© ê³¼ì •)
    def encode(self, text, show_process=False):
        words = re.findall(r'\S+', text)
        ids = [self.word_to_id.get(word, self.word_to_id['<UNK>']) for word in words]
        
        if show_process:
            print(f"ğŸ”¤ ì¸ì½”ë”©: '{text}'")
            print(f"   â†’ ë‹¨ì–´: {words}")
            print(f"   â†’ ID: {ids}")
        
        return ids
    
    # í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë””ì½”ë”© ê³¼ì •)
    def decode(self, ids, show_process=False):
        words = [self.id_to_word[id] for id in ids if id not in [0, 1]]  # PAD, UNK ì œì™¸
        result = " ".join(words)
        
        if show_process:
            print(f"ğŸ”¤ ë””ì½”ë”©: {ids} â†’ '{result}'")
        
        return result

# ===============================================================================
# ğŸ§  2ë‹¨ê³„: Multi-Head Attention êµ¬í˜„
# ===============================================================================

# Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜ - Transformerì˜ í•µì‹¬
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        print(f"ğŸ§  Multi-Head Attention ì´ˆê¸°í™”:")
        print(f"   - ëª¨ë¸ ì°¨ì›: {d_model}")
        print(f"   - í—¤ë“œ ìˆ˜: {n_heads}")
        print(f"   - í—¤ë“œë‹¹ ì°¨ì›: {self.d_k}")
        
        # Query, Key, Value ë³€í™˜ í–‰ë ¬
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, x, return_attention=False):
        batch_size, seq_len, d_model = x.size()
        
        # 1ï¸âƒ£ Query, Key, Value ê³„ì‚°
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # 2ï¸âƒ£ Attention Score ê³„ì‚° (ìœ ì‚¬ë„ ì¸¡ì •)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        # 3ï¸âƒ£ Causal Masking (ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ë„ë¡)
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)
        scores.masked_fill_(mask, -float('inf'))
        
        # 4ï¸âƒ£ Softmaxë¡œ í™•ë¥  ë³€í™˜
        attention_weights = F.softmax(scores, dim=-1)
        
        # 5ï¸âƒ£ Valueì™€ ê°€ì¤‘í•©
        attention_output = torch.matmul(attention_weights, V)
        
        # 6ï¸âƒ£ í—¤ë“œë“¤ì„ ë‹¤ì‹œ í•©ì¹˜ê¸°
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        output = self.W_o(attention_output)
        
        if return_attention:
            return output, attention_weights
        return output

# ===============================================================================
# ğŸ—ï¸ 3ë‹¨ê³„: Transformer ë¸”ë¡ êµ¬í˜„
# ===============================================================================

# Transformer ë¸”ë¡ - Attention + Feed Forward + Residual Connection
class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Feed Forward Network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, return_attention=False):
        # 1ï¸âƒ£ Self-Attention with Residual Connection
        if return_attention:
            attn_output, attn_weights = self.attention(x, return_attention=True)
            x = self.norm1(x + self.dropout(attn_output))
            
            # 2ï¸âƒ£ Feed Forward with Residual Connection
            ff_output = self.feed_forward(x)
            x = self.norm2(x + self.dropout(ff_output))
            
            return x, attn_weights
        else:
            attn_output = self.attention(x)
            x = self.norm1(x + self.dropout(attn_output))
            
            ff_output = self.feed_forward(x)
            x = self.norm2(x + self.dropout(ff_output))
            
            return x

# ===============================================================================
# ğŸ¤– 4ë‹¨ê³„: ì–¸ì–´ëª¨ë¸ êµ¬í˜„ (Decoder-only êµ¬ì¡°)
# ===============================================================================

# ê°„ë‹¨í•œ ì–¸ì–´ëª¨ë¸ - GPT ìŠ¤íƒ€ì¼ì˜ Decoder-only êµ¬ì¡°
class SimpleLM(nn.Module):
    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=2, max_seq_len=50):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        self.n_layers = n_layers
        self.n_heads = n_heads
        
        print(f"ğŸ¤– ì–¸ì–´ëª¨ë¸ ì´ˆê¸°í™”:")
        print(f"   - ì–´íœ˜ í¬ê¸°: {vocab_size}")
        print(f"   - ëª¨ë¸ ì°¨ì›: {d_model}")
        print(f"   - í—¤ë“œ ìˆ˜: {n_heads}")
        print(f"   - ë ˆì´ì–´ ìˆ˜: {n_layers}")
        
        # ì„ë² ë”© ë ˆì´ì–´ë“¤
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Transformer ë¸”ë¡ë“¤
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_model*4) 
            for _ in range(n_layers)
        ])
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_projection = nn.Linear(d_model, vocab_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x, return_attention=False):
        batch_size, seq_len = x.size()
        
        # 1ï¸âƒ£ í† í° ì„ë² ë”© + ìœ„ì¹˜ ì„ë² ë”©
        positions = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).to(x.device)
        embeddings = self.token_embedding(x) + self.position_embedding(positions)
        x = self.dropout(embeddings)
        
        # 2ï¸âƒ£ Transformer ë¸”ë¡ë“¤ í†µê³¼
        attention_weights_list = []
        for i, transformer_block in enumerate(self.transformer_blocks):
            if return_attention:
                x, attn_weights = transformer_block(x, return_attention=True)
                attention_weights_list.append(attn_weights)
            else:
                x = transformer_block(x)
        
        # 3ï¸âƒ£ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì„ ìœ„í•œ ë¡œì§“ ê³„ì‚°
        logits = self.output_projection(x)
        
        if return_attention:
            return logits, attention_weights_list
        return logits

# ===============================================================================
# ğŸ“Š 5ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° í•™ìŠµ í•¨ìˆ˜ë“¤
# ===============================================================================

# í•™ìŠµ ë°ì´í„° ìƒì„± í•¨ìˆ˜
def create_training_data(texts, tokenizer, max_length=20):
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘... (ìµœëŒ€ ê¸¸ì´: {max_length})")
    
    input_ids = []
    target_ids = []
    
    for i, text in enumerate(texts):
        # ì‹œì‘/ë í† í° ì¶”ê°€
        tokens = [tokenizer.word_to_id['<START>']] + tokenizer.encode(text) + [tokenizer.word_to_id['<END>']]
        
        # ê¸¸ì´ ì¡°ì ˆ
        if len(tokens) > max_length:
            tokens = tokens[:max_length]
        
        # íŒ¨ë”©
        while len(tokens) < max_length:
            tokens.append(tokenizer.word_to_id['<PAD>'])
        
        # ì…ë ¥ê³¼ íƒ€ê²Ÿ ì¤€ë¹„ (ë‹¤ìŒ í† í° ì˜ˆì¸¡)
        input_ids.append(tokens[:-1])
        target_ids.append(tokens[1:])
        
        if i < 3:  # ì²˜ìŒ 3ê°œ ì˜ˆì‹œë§Œ ì¶œë ¥
            print(f"   ì˜ˆì‹œ {i+1}: {tokenizer.decode(tokens[1:-1])}")
    
    print(f"   ... ì´ {len(texts)}ê°œ ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ")
    return torch.tensor(input_ids), torch.tensor(target_ids)

# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
def train_model(model, train_inputs, train_targets, epochs=50, lr=0.001):
    print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì—í­: {epochs}, í•™ìŠµë¥ : {lr})")
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # PAD í† í° ë¬´ì‹œ
    
    model.train()
    losses = []
    
    for epoch in range(epochs):
        optimizer.zero_grad()
        
        # ìˆœì „íŒŒ
        logits = model(train_inputs)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = criterion(logits.reshape(-1, model.vocab_size), train_targets.reshape(-1))
        
        # ì—­ì „íŒŒ
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        if epoch % 10 == 0:
            print(f"   Epoch {epoch:3d}, Loss: {loss.item():.4f}")
    
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    return losses

# ===============================================================================
# ğŸ¨ 6ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± ë° ì‹œê°í™” í•¨ìˆ˜ë“¤
# ===============================================================================

# ì˜¨ë„ë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„± - ë‹¤ì–‘ì„± ì¡°ì ˆì˜ í•µì‹¬
def generate_text(model, tokenizer, prompt, max_length=20, temperature=1.0, top_k=5, show_process=False):
    if show_process:
        print(f"ğŸ¨ í…ìŠ¤íŠ¸ ìƒì„±:")
        print(f"   í”„ë¡¬í”„íŠ¸: '{prompt}' | ì˜¨ë„: {temperature} | Top-K: {top_k}")
    
    model.eval()
    
    # í”„ë¡¬í”„íŠ¸ í† í°í™”
    tokens = [tokenizer.word_to_id['<START>']] + tokenizer.encode(prompt)
    
    with torch.no_grad():
        for step in range(max_length - len(tokens)):
            # ì…ë ¥ ì¤€ë¹„ (ìµœê·¼ í† í°ë“¤ë§Œ ì‚¬ìš©)
            input_tensor = torch.tensor([tokens[-19:]]).to(device)
            
            # ì˜ˆì¸¡
            logits = model(input_tensor)
            
            # ì˜¨ë„ ì ìš© - í™•ë¥  ë¶„í¬ì˜ ë¾°ì¡±í•¨ ì¡°ì ˆ
            next_token_logits = logits[0, -1] / temperature
            
            # Top-K ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘ì„± ì¦ê°€
            if top_k > 0:
                top_k_logits, top_k_indices = torch.topk(next_token_logits, k=min(top_k, len(next_token_logits)))
                probs = F.softmax(top_k_logits, dim=-1)
                next_token_idx = torch.multinomial(probs, 1).item()
                next_token = top_k_indices[next_token_idx].item()
            else:
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()
            
            # ì¢…ë£Œ ì¡°ê±´
            if next_token == tokenizer.word_to_id['<END>']:
                break
                
            tokens.append(next_token)
            
            # ì²« 2ìŠ¤í…ë§Œ í™•ë¥  ë¶„í¬ ì¶œë ¥
            if show_process and step < 2:  
                top_probs, top_indices = torch.topk(probs, k=3)
                top_words = [tokenizer.id_to_word[idx.item()] for idx in top_indices]
                print(f"   ìŠ¤í… {step+1}: {list(zip(top_words, [f'{p:.2f}' for p in top_probs]))}")
    
    result = tokenizer.decode(tokens[1:])  # START í† í° ì œì™¸
    if show_process:
        print(f"   â†’ ê²°ê³¼: '{result}'")
    return result

# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™” - ëª¨ë¸ì´ ì–´ë””ì— ì£¼ëª©í•˜ëŠ”ì§€ í™•ì¸
def visualize_attention(model, tokenizer, text, layer_idx=0, head_idx=0):
    print(f"ğŸ‘ï¸ ì–´í…ì…˜ ì‹œê°í™”: '{text}'")
    print(f"   â†’ ë ˆì´ì–´ {layer_idx+1}, í—¤ë“œ {head_idx+1}ì—ì„œ ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì£¼ëª©í•˜ëŠ” ì •ë„")
    
    model.eval()
    
    # í† í°í™”
    tokens = [tokenizer.word_to_id['<START>']] + tokenizer.encode(text)
    input_tensor = torch.tensor([tokens]).to(device)
    
    with torch.no_grad():
        logits, attention_weights_list = model(input_tensor, return_attention=True)
    
    # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
    if layer_idx >= len(attention_weights_list):
        layer_idx = 0
    if head_idx >= model.n_heads:
        head_idx = 0
        
    attn = attention_weights_list[layer_idx][0, head_idx].cpu().numpy()
    
    # í† í° ë¼ë²¨ ì¤€ë¹„
    token_labels = [tokenizer.id_to_word[id] for id in tokens]
    
    # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(10, 8))
    sns.heatmap(attn, 
                xticklabels=token_labels, 
                yticklabels=token_labels,
                cmap='Blues', 
                annot=True, 
                fmt='.2f',
                cbar_kws={'label': 'Attention Weight'})
    
    plt.title(f'ğŸ§  ì–´í…ì…˜ íˆíŠ¸ë§µ (ë ˆì´ì–´ {layer_idx+1}, í—¤ë“œ {head_idx+1})\nê° í–‰ì€ í•´ë‹¹ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì£¼ëª©í•˜ëŠ” ì •ë„', 
              fontsize=12, pad=20)
    plt.xlabel('ì°¸ì¡°ë˜ëŠ” ë‹¨ì–´ (Key)')
    plt.ylabel('ì°¸ì¡°í•˜ëŠ” ë‹¨ì–´ (Query)')
    plt.tight_layout()
    plt.show()
    
    # í•µì‹¬ ì–´í…ì…˜ íŒ¨í„´ë§Œ ì¶œë ¥
    print("ğŸ“Š ì£¼ìš” ì–´í…ì…˜ íŒ¨í„´ (ìƒìœ„ 2ê°œ):")
    for i, query_token in enumerate(token_labels):
        top_attention = np.argsort(attn[i])[-2:][::-1]  # ìƒìœ„ 2ê°œ
        top_tokens = [token_labels[j] for j in top_attention]
        top_weights = [attn[i][j] for j in top_attention]
        print(f"   '{query_token}' â†’ {top_tokens[0]}({top_weights[0]:.2f}), {top_tokens[1]}({top_weights[1]:.2f})")

# ì˜¨ë„ë³„ ìƒì„± ë‹¤ì–‘ì„± ì‹¤í—˜
def experiment_with_temperature(model, tokenizer, prompt="ì¸ê³µì§€ëŠ¥ì€"):
    print("ğŸŒ¡ï¸ ì˜¨ë„ë³„ ìƒì„± ë‹¤ì–‘ì„± ë¹„êµ")
    print("=" * 50)
    
    temperatures = [0.2, 0.5, 1.0, 2.0]
    
    for temp in temperatures:
        print(f"\nğŸŒ¡ï¸ ì˜¨ë„ {temp} ({'ì•ˆì •ì ' if temp < 1 else 'ì°½ì˜ì '})")
        
        # í•œ ë²ˆë§Œ ìƒì„±í•˜ë˜ ê³¼ì •ì„ ë³´ì—¬ì¤Œ
        generated = generate_text(model, tokenizer, prompt, 
                                temperature=temp, max_length=12, show_process=True)
        print()

# ë‹¤ì–‘í•œ ë ˆì´ì–´/í—¤ë“œì˜ ì–´í…ì…˜ íŒ¨í„´ ë¹„êµ
def compare_attention_patterns(model, tokenizer, text):
    print("ğŸ” ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ì´í•´í•˜ê¸°")
    print("=" * 50)
    print(f"ì…ë ¥: '{text}'")
    print("â†’ ì—¬ëŸ¬ í—¤ë“œê°€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ í•™ìŠµ")
    
    model.eval()
    tokens = [tokenizer.word_to_id['<START>']] + tokenizer.encode(text)
    input_tensor = torch.tensor([tokens]).to(device)
    
    with torch.no_grad():
        logits, attention_weights_list = model(input_tensor, return_attention=True)
    
    token_labels = [tokenizer.id_to_word[id] for id in tokens]
    
    # 2x2 ê·¸ë¦¬ë“œë¡œ í—¤ë“œ ë¹„êµ (ì²˜ìŒ 4ê°œ í—¤ë“œë§Œ)
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    axes = axes.flatten()
    
    patterns = [
        (0, 0, "ë ˆì´ì–´1-í—¤ë“œ1"),
        (0, 1, "ë ˆì´ì–´1-í—¤ë“œ2") if model.n_heads > 1 else (0, 0, "ë ˆì´ì–´1-í—¤ë“œ1"),
        (0, 2, "ë ˆì´ì–´1-í—¤ë“œ3") if model.n_heads > 2 else (0, 0, "ë ˆì´ì–´1-í—¤ë“œ1"),
        (0, 3, "ë ˆì´ì–´1-í—¤ë“œ4") if model.n_heads > 3 else (0, 0, "ë ˆì´ì–´1-í—¤ë“œ1")
    ]
    
    for i, (layer_idx, head_idx, title) in enumerate(patterns):
        layer_idx = min(layer_idx, len(attention_weights_list) - 1)
        head_idx = min(head_idx, model.n_heads - 1)
        
        attn = attention_weights_list[layer_idx][0, head_idx].cpu().numpy()
        
        im = axes[i].imshow(attn, cmap='Blues', aspect='auto')
        axes[i].set_title(title, fontsize=11)
        axes[i].set_xticks(range(len(token_labels)))
        axes[i].set_yticks(range(len(token_labels)))
        axes[i].set_xticklabels(token_labels, rotation=45, ha='right', fontsize=9)
        axes[i].set_yticklabels(token_labels, fontsize=9)
        
        # í•µì‹¬ ê°’ë§Œ í‘œì‹œ
        for y in range(len(token_labels)):
            for x in range(len(token_labels)):
                if attn[y, x] > 0.3:  # ë†’ì€ ê°’ë§Œ í‘œì‹œ
                    axes[i].text(x, y, f'{attn[y, x]:.2f}', 
                               ha='center', va='center', 
                               color='white', fontsize=8)
    
    plt.suptitle(f'ğŸ§  ë©€í‹°í—¤ë“œ ì–´í…ì…˜: ê° í—¤ë“œì˜ ì„œë¡œ ë‹¤ë¥¸ ê´€ì \n"{text}"', fontsize=14, y=0.95)
    plt.tight_layout()
    plt.show()
    
    print("\nğŸ’¡ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì˜ ì¥ì :")
    print("   â€¢ ê° í—¤ë“œê°€ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê´€ê³„ë¥¼ í•™ìŠµ (ë¬¸ë²•ì , ì˜ë¯¸ì  ë“±)")
    print("   â€¢ ë” í’ë¶€í•˜ê³  ë‹¤ì–‘í•œ í‘œí˜„ í•™ìŠµ ê°€ëŠ¥")
    print("   â€¢ ë³‘ë ¬ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ")

# ì¸ì½”ë”©/ì„ë² ë”© ê³¼ì • ì‹œê°í™”
def visualize_encoding_process(model, tokenizer, text):
    print("ğŸ”¤ í† í¬ë‚˜ì´ì €ì™€ ì„ë² ë”© ì´í•´í•˜ê¸°")
    print("=" * 50)
    
    # ê°„ë‹¨í•œ ì˜ˆì‹œë“¤ë¡œ í† í¬ë‚˜ì´ì € ì„¤ëª…
    examples = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°",
        "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ",
        "ì¢‹ì€ í•˜ë£¨",
        "ì»´í“¨í„° ê³¼í•™"
    ]
    
    print("1ï¸âƒ£ í† í¬ë‚˜ì´ì € ë™ì‘ ì˜ˆì‹œ:")
    for i, example in enumerate(examples):
        if i >= 5:  # 5ê°œë§Œ ì¶œë ¥
            break
        encoded = tokenizer.encode(example, show_process=True)
        print()
    
    # 2ë‹¨ê³„: ì„ë² ë”© ê³¼ì •
    print("2ï¸âƒ£ ì„ë² ë”© ê³¼ì •:")
    model.eval()
    tokens = tokenizer.encode(text)
    input_tensor = torch.tensor([tokens]).to(device)
    
    # ì„ë² ë”© ì¶”ì¶œ
    token_embeddings = model.token_embedding(input_tensor)
    position_embeddings = model.position_embedding(torch.arange(len(tokens)).unsqueeze(0).to(device))
    final_embeddings = token_embeddings + position_embeddings
    
    print(f"   ì…ë ¥ í…ìŠ¤íŠ¸: '{text}'")
    print(f"   í† í° ìˆ˜: {len(tokens)}")
    print(f"   ì„ë² ë”© ì°¨ì›: {final_embeddings.shape[-1]}")
    print(f"   ìµœì¢… ì„ë² ë”© ëª¨ì–‘: {final_embeddings.shape}")
    
    # ê°„ë‹¨í•œ ì„ë² ë”© ì‹œê°í™”
    embeddings_np = final_embeddings[0].detach().cpu().numpy()
    words = text.split()
    
    plt.figure(figsize=(10, 6))
    plt.imshow(embeddings_np.T, cmap='coolwarm', aspect='auto')
    plt.colorbar(label='ì„ë² ë”© ê°’')
    plt.title(f'ğŸ¯ ì„ë² ë”© ë²¡í„°: "{text}"\nê° ë‹¨ì–´ê°€ {final_embeddings.shape[-1]}ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ë¨', fontsize=12)
    plt.xlabel('ë‹¨ì–´ ìœ„ì¹˜')
    plt.ylabel('ì„ë² ë”© ì°¨ì›')
    plt.xticks(range(len(words)), words, rotation=45)
    plt.tight_layout()
    plt.show()

# ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‰½ê²Œ ì´í•´í•˜ê¸°
def analyze_attention_mechanism(model, tokenizer, text="ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤"):
    print("ğŸ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‰½ê²Œ ì´í•´í•˜ê¸°")
    print("=" * 50)
    print("ğŸ’¡ ì–´í…ì…˜ = 'ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ ì–¼ë§ˆë‚˜ ì°¸ê³ í•˜ëŠ”ê°€?'")
    
    model.eval()
    tokens = [tokenizer.word_to_id['<START>']] + tokenizer.encode(text)
    input_tensor = torch.tensor([tokens]).to(device)
    
    with torch.no_grad():
        # ì„ë² ë”© ì¶”ì¶œ
        embeddings = model.token_embedding(input_tensor) + model.position_embedding(
            torch.arange(len(tokens)).unsqueeze(0).to(device))
        
        # ì²« ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ Q, K, V ì¶”ì¶œ
        first_attention = model.transformer_blocks[0].attention
        Q = first_attention.W_q(embeddings)
        K = first_attention.W_k(embeddings)
        V = first_attention.W_v(embeddings)
        
        print(f"\nğŸ“ ì…ë ¥: '{text}'")
        token_words = [tokenizer.id_to_word[id] for id in tokens]
        print(f"ğŸ”¤ í† í°ë“¤: {token_words}")
        
        print(f"\nğŸ§® ì–´í…ì…˜ ê³„ì‚° ë‹¨ê³„:")
        print(f"   1. Query(Q): 'ëˆ„ê°€ ì§ˆë¬¸í•˜ëŠ”ê°€?' - í¬ê¸°: {Q.shape}")
        print(f"   2. Key(K): 'ëˆ„êµ¬ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆëŠ”ê°€?' - í¬ê¸°: {K.shape}")  
        print(f"   3. Value(V): 'ì‹¤ì œ ì •ë³´ ë‚´ìš©' - í¬ê¸°: {V.shape}")
        
        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(first_attention.d_k)
        
        print(f"\nğŸ¯ í•µì‹¬: Causal Masking")
        print(f"   â†’ ë¯¸ë˜ ë‹¨ì–´ëŠ” ë³¼ ìˆ˜ ì—†ìŒ (ìê¸°íšŒê·€ ìƒì„±ì„ ìœ„í•´)")
        
        # ë§ˆìŠ¤í‚¹ ì ìš©
        mask = torch.triu(torch.ones(len(tokens), len(tokens)), diagonal=1).bool().to(device)
        scores_masked = scores.clone()
        scores_masked.masked_fill_(mask, -float('inf'))
        
        # Softmax ì ìš©
        attention_weights = F.softmax(scores_masked, dim=-1)
        
        print(f"\nğŸ“Š ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì˜ˆì‹œ (ì²« ë²ˆì§¸ í—¤ë“œ):")
        attn_np = attention_weights[0, 0, :, :].cpu().numpy()
        for i, word in enumerate(token_words):
            # ìê¸° ìì‹  ì œì™¸í•˜ê³  ê°€ì¥ ë†’ì€ ì–´í…ì…˜ ì°¾ê¸°
            attn_row = attn_np[i].copy()
            attn_row[i] = 0  # ìê¸° ìì‹  ì œì™¸
            max_idx = np.argmax(attn_row)
            max_weight = attn_row[max_idx]
            if max_weight > 0.1:  # ì˜ë¯¸ìˆëŠ” ì–´í…ì…˜ë§Œ
                print(f"   '{word}' â†’ '{token_words[max_idx]}' ({max_weight:.2f})")
            else:
                print(f"   '{word}' â†’ ì£¼ë¡œ ìê¸° ìì‹ ì— ì§‘ì¤‘")

# ===============================================================================
# ğŸš€ 7ë‹¨ê³„: ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ===============================================================================

print("ğŸš€ Transformer ì–¸ì–´ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
print("=" * 80)

# í•™ìŠµ ë°ì´í„° ë¡œë“œ
sample_texts = load_training_data()

# í† í¬ë‚˜ì´ì € ìƒì„±
tokenizer = SimpleTokenizer(sample_texts)
print()

# ì¸ì½”ë”©/ë””ì½”ë”© ì˜ˆì‹œ
print("ğŸ”¤ ì¸ì½”ë”©/ë””ì½”ë”© ì˜ˆì‹œ:")
sample_text = "ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€ í•˜ë£¨"
encoded = tokenizer.encode(sample_text, show_process=True)
decoded = tokenizer.decode(encoded, show_process=True)
print()

# í•™ìŠµ ë°ì´í„° ì¤€ë¹„
train_inputs, train_targets = create_training_data(sample_texts, tokenizer)
print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° í¬ê¸°: {train_inputs.shape}")
print()

# ëª¨ë¸ ìƒì„±
model = SimpleLM(tokenizer.vocab_size, d_model=64, n_heads=4, n_layers=2)
model = model.to(device)

param_count = sum(p.numel() for p in model.parameters())
print(f"ğŸ”§ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {param_count:,}")
print()

# ëª¨ë¸ í•™ìŠµ
losses = train_model(model, train_inputs.to(device), train_targets.to(device), epochs=100)

# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.plot(losses, 'b-', linewidth=2)
plt.title('ğŸ“ˆ í•™ìŠµ ì†ì‹¤ ê³¡ì„ ', fontsize=14)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)
plt.show()

print("\nâœ… ê¸°ë³¸ í•™ìŠµ ì™„ë£Œ! ì´ì œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
print("=" * 80)

# ===============================================================================
# ğŸ¯ 8ë‹¨ê³„: ë‹¤ì–‘í•œ ì‹¤í—˜ë“¤
# ===============================================================================

# 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
print("\nğŸ¨ ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
print("=" * 50)

test_prompts = ["ì•ˆë…•í•˜ì„¸ìš”", "íŒŒì´ì¬ì„", "ì¸ê³µì§€ëŠ¥ì€"]

for prompt in test_prompts:
    print(f"\nğŸ“ í”„ë¡¬í”„íŠ¸: '{prompt}'")
    generated = generate_text(model, tokenizer, prompt, max_length=10, temperature=1.0)
    print(f"   â†’ {generated}")

# 2. ì˜¨ë„ë³„ ìƒì„± ë‹¤ì–‘ì„± ì‹¤í—˜
print("\n")
experiment_with_temperature(model, tokenizer, "ì»´í“¨í„°ëŠ”")

# 3. í† í¬ë‚˜ì´ì €ì™€ ì„ë² ë”© ì´í•´
print("\n")
visualize_encoding_process(model, tokenizer, "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ")

# 4. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì´í•´
print("\n")
analyze_attention_mechanism(model, tokenizer, "ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤")

# 5. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
print("\n")
visualize_attention(model, tokenizer, "ì¢‹ì€ í•˜ë£¨", layer_idx=0, head_idx=0)

# 6. ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ì´í•´
print("\n")
compare_attention_patterns(model, tokenizer, "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°")

# ===============================================================================
# ğŸ¯ 9ë‹¨ê³„: ê°„ì†Œí™”ëœ ì¶”ê°€ ì‹¤í—˜ë“¤
# ===============================================================================

print("\n" + "="*60)
print("ğŸ”¬ í•µì‹¬ ê°œë… ì •ë¦¬")
print("="*60)

print("\nğŸ¯ í•™ìŠµí•œ í•µì‹¬ ê°œë…:")
print("   1. í† í¬ë‚˜ì´ì €: í…ìŠ¤íŠ¸ â†” ìˆ«ì ë³€í™˜ì˜ ë‹¤ë¦¬ ì—­í• ")
print("   2. ì„ë² ë”©: ë‹¨ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë²¡í„°ë¡œ ë³€í™˜")
print("   3. ì–´í…ì…˜: ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ í•™ìŠµ")
print("   4. ë©€í‹°í—¤ë“œ: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ê´€ê³„ë¥¼ íŒŒì•…")
print("   5. ì˜¨ë„: ìƒì„±ì˜ ì°½ì˜ì„±ê³¼ ì•ˆì •ì„±ì„ ì¡°ì ˆí•˜ëŠ” í•µì‹¬ íŒŒë¼ë¯¸í„°")

print("\nğŸ› ï¸ ì‹¤í—˜í•´ë³¼ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë“¤:")
print("   â€¢ generate_text(model, tokenizer, 'í”„ë¡¬í”„íŠ¸', temperature=1.5)")
print("   â€¢ visualize_attention(model, tokenizer, 'í…ìŠ¤íŠ¸')")
print("   â€¢ experiment_with_temperature(model, tokenizer, 'ì‹œì‘ë§')")

print("\nğŸ’¡ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
print("   1. ë‹¤ë¥¸ ì˜¨ë„ê°’ìœ¼ë¡œ ì°½ì˜ì„± ë³€í™” ê´€ì°°")
print("   2. ê¸´ í…ìŠ¤íŠ¸ë¡œ ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„")  
print("   3. ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
print("   4. ë‹¤ë¥¸ ë ˆì´ì–´/í—¤ë“œì˜ ì–´í…ì…˜ ë¹„êµ")

print("\nâœ¨ ì¶•í•˜í•©ë‹ˆë‹¤! Transformer ì–¸ì–´ëª¨ë¸ì˜ í•µì‹¬ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤!")
print("="*60)
