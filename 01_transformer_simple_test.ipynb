{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# GPT 모델 학습 및 생성 통합 코드\n",
        "# Jupyter Notebook에서 두 개의 셀로 나누어 실행\n",
        "\n",
        "# =============================================================================\n",
        "# 첫 번째 셀: 라이브러리 임포트 및 모델 학습\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "# KLUE/BERT 토크나이저 import\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "    print(\"Transformers 사용 가능: KLUE/BERT 토크나이저 사용\")\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"Transformers 없음: 기본 토크나이저 사용\")\n",
        "\n",
        "# 하드웨어 환경 확인\n",
        "print(\"=\"*60)\n",
        "print(\"하드웨어 환경 확인\")\n",
        "print(\"=\"*60)\n",
        "print(f\"PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"CPU 전용 모드\")\n",
        "    device = 'cpu'\n",
        "print(f\"사용 디바이스: {device}\")\n",
        "print()\n",
        "\n",
        "# 텍스트 파일에서 데이터 로드\n",
        "def load_text_data(filename='헤밍웨이-노인과바다.txt'):\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
        "        print(f\"파일에서 {len(sentences)}개의 문장 로드\")\n",
        "\n",
        "        if len(sentences) == 0:\n",
        "            print(f\"'{filename}' 파일이 비어 있음\")\n",
        "            exit(1)\n",
        "\n",
        "        return sentences\n",
        "    except FileNotFoundError:\n",
        "        print(f\"'{filename}' 파일 없음\")\n",
        "        exit(1)\n",
        "\n",
        "# KLUE/BERT 토크나이저 래퍼 (출력 개선)\n",
        "class KLUETokenizer:\n",
        "    def __init__(self, texts):\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "            self.use_bert = True\n",
        "            print(\"KLUE/BERT 토크나이저 초기화 중...\")\n",
        "        else:\n",
        "            self.use_bert = False\n",
        "            print(\"기본 문자 단위 토크나이저로 대체...\")\n",
        "\n",
        "        if self.use_bert:\n",
        "            vocab_set = set()\n",
        "            printed_count = 0\n",
        "            total_texts = len(texts)\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                # 처음 5개, 마지막 5개만 출력\n",
        "                if i < 5 or i >= total_texts - 5:\n",
        "                    if printed_count == 5:\n",
        "                        print(\"... (중간 과정 생략) ...\")\n",
        "                    if i < 5 or i >= total_texts - 5:\n",
        "                        print(f\"토크나이징 진행: {i+1}/{total_texts}\")\n",
        "                        printed_count += 1\n",
        "\n",
        "                tokens = self.tokenizer.tokenize(text)\n",
        "                vocab_set.update(tokens)\n",
        "\n",
        "            special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "            bert_special = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
        "\n",
        "            self.vocab = special_tokens + bert_special + sorted(vocab_set)\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            self.token_to_idx = {token: i for i, token in enumerate(self.vocab)}\n",
        "            self.idx_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
        "        else:\n",
        "            vocab_set = set()\n",
        "            for text in texts:\n",
        "                vocab_set.update(text)\n",
        "\n",
        "            special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "            self.vocab = special_tokens + sorted(vocab_set)\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            self.token_to_idx = {token: i for i, token in enumerate(self.vocab)}\n",
        "            self.idx_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"토크나이저 정보\")\n",
        "        print(\"=\"*60)\n",
        "        if self.use_bert:\n",
        "            print(f\"토크나이저 타입: KLUE/BERT WordPiece\")\n",
        "            print(f\"어휘 크기: {self.vocab_size:,}\")\n",
        "            print(f\"토큰 예시: {self.vocab[9:19]}\")\n",
        "        else:\n",
        "            print(f\"토크나이저 타입: 문자 단위 (Character-level)\")\n",
        "            print(f\"어휘 크기: {self.vocab_size:,}\")\n",
        "        print(f\"특수 토큰: {['<pad>', '<sos>', '<eos>', '<unk>']}\")\n",
        "        print()\n",
        "\n",
        "    def encode(self, text):\n",
        "        if self.use_bert:\n",
        "            bert_tokens = self.tokenizer.tokenize(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in bert_tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        else:\n",
        "            tokens = list(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = []\n",
        "        for idx in token_ids:\n",
        "            token = self.idx_to_token[idx]\n",
        "            if token not in ['<pad>', '<sos>', '<eos>', '<unk>', '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
        "                tokens.append(token)\n",
        "\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            return self.tokenizer.convert_tokens_to_string(tokens)\n",
        "        else:\n",
        "            return ''.join(tokens)\n",
        "\n",
        "# 위치 인코딩\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"위치 인코딩 정보\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"임베딩 차원: {d_model}\")\n",
        "        print(f\"최대 길이: {max_len}\")\n",
        "        print(f\"PE 값 범위: [{pe.min():.3f}, {pe.max():.3f}]\")\n",
        "        print(\"sin/cos 함수로 생성\")\n",
        "        print()\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len]\n",
        "\n",
        "# 멀티헤드 어텐션 메커니즘\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "\n",
        "    def forward(self, x, mask: Optional[torch.Tensor] = None, verbose: bool = False):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(\"어텐션 메커니즘 시작\")\n",
        "            print(\"=\"*40)\n",
        "\n",
        "        Q = self.w_q(x)\n",
        "        K = self.w_k(x)\n",
        "        V = self.w_v(x)\n",
        "\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.w_o(output)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[어텐션] 최종 출력 형태: {output.shape}\")\n",
        "            print(\"=\"*40)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 피드포워드 신경망\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, verbose: bool = False):\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(\"피드포워드 신경망 시작\")\n",
        "            print(\"=\"*40)\n",
        "            print(f\"[FFN] 입력 형태: {x.shape}\")\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[FFN] 출력 형태: {x.shape}\")\n",
        "            print(\"=\"*40)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Decoder Block\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask: Optional[torch.Tensor] = None, verbose: bool = False):\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"디코더 블록 처리 시작\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        attn_output = self.attention(x, mask, verbose)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        ff_output = self.feed_forward(x, verbose)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[디코더 블록] 최종 출력: {x.shape}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        return x\n",
        "\n",
        "# GPT Style Decoder-Only Transformer\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_heads: int,\n",
        "                 n_layers: int, d_ff: int, max_seq_len: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"모델 구조\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"모델 타입: GPT (Decoder-Only Transformer)\")\n",
        "        print(f\"어휘 크기: {vocab_size:,}\")\n",
        "        print(f\"임베딩 차원: {d_model}\")\n",
        "        print(f\"어텐션 헤드: {n_heads}\")\n",
        "        print(f\"디코더 층: {n_layers}\")\n",
        "        print(f\"FFN 차원: {d_ff}\")\n",
        "        print(f\"최대 시퀀스 길이: {max_seq_len}\")\n",
        "        print(f\"드롭아웃: {dropout}\")\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"총 파라미터: {total_params:,}\")\n",
        "        print()\n",
        "\n",
        "    def create_causal_mask(self, seq_len: int, device: torch.device):\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
        "        return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x, verbose: bool = False):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"모델 순전파 시작\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"[모델] 입력 형태: {x.shape}\")\n",
        "\n",
        "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        causal_mask = self.create_causal_mask(seq_len, x.device)\n",
        "\n",
        "        for i, block in enumerate(self.decoder_blocks):\n",
        "            x = block(x, causal_mask, verbose and i == 0)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[모델] 최종 로짓 형태: {logits.shape}\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# 배치 데이터 생성\n",
        "def create_batches(tokenizer, texts, max_seq_len, batch_size):\n",
        "    all_tokens = []\n",
        "\n",
        "    print(f\"배치 생성 중... (최대 시퀀스 길이: {max_seq_len})\")\n",
        "\n",
        "    total_texts = len(texts)\n",
        "    printed_count = 0\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        # 처음 5개, 마지막 5개만 출력\n",
        "        if i < 5 or i >= total_texts - 5:\n",
        "            if printed_count == 5:\n",
        "                print(\"... (중간 과정 생략) ...\")\n",
        "            print(f\"처리된 텍스트: {i+1}/{total_texts}, 생성된 시퀀스: {len(all_tokens)}\")\n",
        "            printed_count += 1\n",
        "\n",
        "        tokens = tokenizer.encode(text)\n",
        "\n",
        "        # 토큰이 너무 길면 여러 청크로 나눔\n",
        "        if len(tokens) > max_seq_len:\n",
        "            for start in range(0, len(tokens), max_seq_len - 2):\n",
        "                chunk = tokens[start:start + max_seq_len - 2]\n",
        "                if len(chunk) >= 10:\n",
        "                    chunk = [tokenizer.token_to_idx['<sos>']] + chunk[1:-1] + [tokenizer.token_to_idx['<eos>']]\n",
        "\n",
        "                    while len(chunk) < max_seq_len:\n",
        "                        chunk.append(tokenizer.token_to_idx['<pad>'])\n",
        "\n",
        "                    all_tokens.append(chunk)\n",
        "        else:\n",
        "            while len(tokens) < max_seq_len:\n",
        "                tokens.append(tokenizer.token_to_idx['<pad>'])\n",
        "            all_tokens.append(tokens)\n",
        "\n",
        "    if len(all_tokens) == 0:\n",
        "        print(\"ERROR: 처리된 토큰 시퀀스가 없음!\")\n",
        "        return []\n",
        "\n",
        "    print(f\"총 {len(all_tokens)}개의 시퀀스가 생성...\")\n",
        "\n",
        "    data = torch.tensor(all_tokens, dtype=torch.long)\n",
        "\n",
        "    # 배치 생성 (불완전한 배치도 포함)\n",
        "    batches = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        batches.append(batch)\n",
        "\n",
        "    print(f\"총 {len(batches)}개의 배치 생성...\")\n",
        "    return batches\n",
        "\n",
        "# 모델 학습\n",
        "def train_model(model, batches, tokenizer, epochs, device, lr=3e-4):\n",
        "    if len(batches) == 0:\n",
        "        print(\"ERROR: 학습할 배치가 없음!!\")\n",
        "        print(\"- 텍스트 파일의 문장이 너무 적거나\")\n",
        "        print(\"- 배치 크기가 너무 크거나\")\n",
        "        print(\"- 시퀀스 길이가 너무 길 수 있습니다.\")\n",
        "        return []\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.95))\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=epochs//2)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_idx['<pad>'])\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 학습 시작\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"학습 배치 수: {len(batches)}\")\n",
        "    print(f\"초기 학습률: {lr}\")\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(batches):\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(inputs, verbose=(epoch == 0 and batch_idx == 0))\n",
        "\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if num_batches >= min(20, len(batches)):\n",
        "                break\n",
        "\n",
        "        if num_batches > 0:\n",
        "            avg_loss = total_loss / num_batches\n",
        "            losses.append(avg_loss)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"에포크 {epoch+1:2d}/{epochs}, 손실: {avg_loss:.4f}, 학습률: {current_lr:.6f}\")\n",
        "        else:\n",
        "            print(f\"에포크 {epoch+1:2d}/{epochs}, 배치 없음 - 학습 중단!!\")\n",
        "            break\n",
        "\n",
        "    return losses\n",
        "\n",
        "# 모델 저장\n",
        "def save_model(model, tokenizer, filepath):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab_size': tokenizer.vocab_size,\n",
        "        'd_model': model.d_model,\n",
        "        'n_heads': model.n_heads,\n",
        "        'n_layers': model.n_layers,\n",
        "        'd_ff': model.d_ff,\n",
        "        'max_seq_len': model.max_seq_len,\n",
        "        'vocab': tokenizer.vocab,\n",
        "        'token_to_idx': tokenizer.token_to_idx,\n",
        "        'idx_to_token': tokenizer.idx_to_token,\n",
        "        'use_bert': tokenizer.use_bert\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"모델이 '{filepath}'에 저장되었습니다.\")\n",
        "\n",
        "# 학습 실행 함수\n",
        "def run_training():\n",
        "    # 하이퍼파라미터\n",
        "    MAX_SEQ_LEN = 32      # 시퀀스 길이\n",
        "    BATCH_SIZE = 1        # 배치 크기\n",
        "    D_MODEL = 128         # 모델 크기\n",
        "    N_HEADS = 4           # 헤드 수\n",
        "    N_LAYERS = 2          # 레이어 수\n",
        "    D_FF = 256            # FFN 크기\n",
        "    EPOCHS = 5            # 에포크\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 학습 모드\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"시퀀스 길이: {MAX_SEQ_LEN}\")\n",
        "    print(f\"배치 크기: {BATCH_SIZE}\")\n",
        "    print(f\"모델 크기: {D_MODEL}\")\n",
        "    print()\n",
        "\n",
        "    # 데이터 로드 및 토크나이저 생성\n",
        "    texts = load_text_data()\n",
        "    tokenizer = KLUETokenizer(texts)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = GPTModel(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        max_seq_len=MAX_SEQ_LEN\n",
        "    ).to(device)\n",
        "\n",
        "    # 배치 데이터 생성\n",
        "    batches = create_batches(tokenizer, texts, MAX_SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "    if len(batches) == 0:\n",
        "        print(\"ERROR: 배치 생성에 실패!!\")\n",
        "        print(\"1. 텍스트 파일에 충분한 내용이 있는지?\")\n",
        "        print(\"2. 텍스트 파일 인코딩이 UTF-8인지?\")\n",
        "        return False\n",
        "\n",
        "    print(f\"총 배치 수: {len(batches)}\")\n",
        "\n",
        "    # 모델 학습\n",
        "    losses = train_model(model, batches, tokenizer, EPOCHS, device)\n",
        "\n",
        "    # 모델 저장\n",
        "    if losses:\n",
        "        save_model(model, tokenizer, 'gpt_model.pth')\n",
        "        print(\"학습 완료!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"학습 실패 - 모델 저장 실패!!\")\n",
        "        return False\n",
        "\n",
        "# 학습 실행\n",
        "print(\"GPT 모델 학습\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "success = run_training()\n",
        "\n",
        "if success:\n",
        "    print(\"\\n학습 성공...\")\n",
        "else:\n",
        "    print(\"\\n학습 실패!!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvghuD8o0-kH",
        "outputId": "61d28138-04ac-44a3-9e5f-e522e67436cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers 사용 가능: KLUE/BERT 토크나이저 사용\n",
            "============================================================\n",
            "하드웨어 환경 확인\n",
            "============================================================\n",
            "PyTorch 버전: 2.8.0+cu126\n",
            "CUDA 사용 가능: False\n",
            "CPU 전용 모드\n",
            "사용 디바이스: cpu\n",
            "\n",
            "GPT 모델 학습\n",
            "============================================================\n",
            "============================================================\n",
            "모델 학습 모드\n",
            "============================================================\n",
            "시퀀스 길이: 32\n",
            "배치 크기: 1\n",
            "모델 크기: 128\n",
            "\n",
            "파일에서 1개의 문장 로드\n",
            "KLUE/BERT 토크나이저 초기화 중...\n",
            "토크나이징 진행: 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (45087 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "토크나이저 정보\n",
            "============================================================\n",
            "토크나이저 타입: KLUE/BERT WordPiece\n",
            "어휘 크기: 4,310\n",
            "토큰 예시: ['!', '##1', '##19', '##2', '##29', '##54', '##8', '##가', '##가죽', '##가지']\n",
            "특수 토큰: ['<pad>', '<sos>', '<eos>', '<unk>']\n",
            "\n",
            "============================================================\n",
            "위치 인코딩 정보\n",
            "============================================================\n",
            "임베딩 차원: 128\n",
            "최대 길이: 32\n",
            "PE 값 범위: [-1.000, 1.000]\n",
            "sin/cos 함수로 생성\n",
            "\n",
            "============================================================\n",
            "모델 구조\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 4,310\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "FFN 차원: 256\n",
            "최대 시퀀스 길이: 32\n",
            "드롭아웃: 0.1\n",
            "총 파라미터: 1,372,886\n",
            "\n",
            "배치 생성 중... (최대 시퀀스 길이: 32)\n",
            "처리된 텍스트: 1/1, 생성된 시퀀스: 0\n",
            "총 1503개의 시퀀스가 생성...\n",
            "총 1503개의 배치 생성...\n",
            "총 배치 수: 1503\n",
            "============================================================\n",
            "모델 학습 시작\n",
            "============================================================\n",
            "학습 배치 수: 1503\n",
            "초기 학습률: 0.0003\n",
            "\n",
            "============================================================\n",
            "모델 순전파 시작\n",
            "============================================================\n",
            "[모델] 입력 형태: torch.Size([1, 31])\n",
            "\n",
            "==================================================\n",
            "디코더 블록 처리 시작\n",
            "==================================================\n",
            "\n",
            "========================================\n",
            "어텐션 메커니즘 시작\n",
            "========================================\n",
            "[어텐션] 최종 출력 형태: torch.Size([1, 31, 128])\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "피드포워드 신경망 시작\n",
            "========================================\n",
            "[FFN] 입력 형태: torch.Size([1, 31, 128])\n",
            "[FFN] 출력 형태: torch.Size([1, 31, 128])\n",
            "========================================\n",
            "[디코더 블록] 최종 출력: torch.Size([1, 31, 128])\n",
            "==================================================\n",
            "\n",
            "[모델] 최종 로짓 형태: torch.Size([1, 31, 4310])\n",
            "============================================================\n",
            "에포크  1/5, 손실: 8.4870, 학습률: 0.000300\n",
            "에포크  2/5, 손실: 7.9752, 학습률: 0.000300\n",
            "에포크  3/5, 손실: 7.3811, 학습률: 0.000300\n",
            "에포크  4/5, 손실: 6.6975, 학습률: 0.000300\n",
            "에포크  5/5, 손실: 6.1043, 학습률: 0.000300\n",
            "모델이 'gpt_model.pth'에 저장되었습니다.\n",
            "학습 완료!\n",
            "\n",
            "학습 성공...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 로드된 토크나이저 클래스\n",
        "class LoadedTokenizer:\n",
        "    def __init__(self, vocab, token_to_idx, idx_to_token, use_bert):\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.token_to_idx = token_to_idx\n",
        "        self.idx_to_token = idx_to_token\n",
        "        self.use_bert = use_bert\n",
        "        if use_bert and TRANSFORMERS_AVAILABLE:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            bert_tokens = self.tokenizer.tokenize(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in bert_tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        else:\n",
        "            tokens = list(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = []\n",
        "        for idx in token_ids:\n",
        "            token = self.idx_to_token[idx]\n",
        "            if token not in ['<pad>', '<sos>', '<eos>', '<unk>', '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
        "                tokens.append(token)\n",
        "\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            return self.tokenizer.convert_tokens_to_string(tokens)\n",
        "        else:\n",
        "            return ''.join(tokens)\n",
        "\n",
        "# 모델 로드\n",
        "def load_model(filepath, device):\n",
        "    try:\n",
        "        checkpoint = torch.load(filepath, map_location=device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"모델 파일 '{filepath}'을 찾기 실패!!\")\n",
        "        return None, None\n",
        "\n",
        "    # 토크나이저 복원\n",
        "    tokenizer = LoadedTokenizer(\n",
        "        checkpoint['vocab'],\n",
        "        checkpoint['token_to_idx'],\n",
        "        checkpoint['idx_to_token'],\n",
        "        checkpoint['use_bert']\n",
        "    )\n",
        "\n",
        "    # 모델 생성 및 가중치 로드\n",
        "    model = GPTModel(\n",
        "        vocab_size=checkpoint['vocab_size'],\n",
        "        d_model=checkpoint['d_model'],\n",
        "        n_heads=checkpoint['n_heads'],\n",
        "        n_layers=checkpoint['n_layers'],\n",
        "        d_ff=checkpoint['d_ff'],\n",
        "        max_seq_len=checkpoint['max_seq_len']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()  # 평가 모드로 설정\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 로드 완료\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"모델 타입: GPT (Decoder-Only Transformer)\")\n",
        "    print(f\"어휘 크기: {checkpoint['vocab_size']:,}\")\n",
        "    print(f\"임베딩 차원: {checkpoint['d_model']}\")\n",
        "    print(f\"어텐션 헤드: {checkpoint['n_heads']}\")\n",
        "    print(f\"디코더 층: {checkpoint['n_layers']}\")\n",
        "    print(f\"최대 시퀀스 길이: {checkpoint['max_seq_len']}\")\n",
        "    print(f\"토크나이저: {'KLUE/BERT' if checkpoint['use_bert'] else '문자 단위'}\")\n",
        "    print()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# 텍스트 생성 (확률 상위 5개 후보 표시)\n",
        "def generate_text_with_candidates(model, tokenizer, prompt, max_length, device, temperature=1.0, verbose=False):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens = tokenizer.encode(prompt)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"입력: '{prompt}' (Temperature: {temperature})\")\n",
        "            print(f\"토큰화 결과: {tokens}\")\n",
        "\n",
        "        generated_tokens = tokens.copy()\n",
        "        all_candidates = []\n",
        "\n",
        "        for i in range(max_length):\n",
        "            if len(generated_tokens) >= model.max_seq_len:\n",
        "                if verbose:\n",
        "                    print(f\"최대 시퀀스 길이({model.max_seq_len})에 도달!!\")\n",
        "                break\n",
        "\n",
        "            # 시퀀스 길이 제한 적용\n",
        "            input_tokens = generated_tokens[-model.max_seq_len:]\n",
        "            current_input = torch.tensor([input_tokens], device=device)\n",
        "\n",
        "            # 어텐션 메커니즘 상세 출력 (첫 번째 단계에서만)\n",
        "            show_attention = verbose and i == 0\n",
        "            logits = model(current_input, verbose=show_attention)\n",
        "\n",
        "            next_token_logits = logits[0, -1]\n",
        "\n",
        "            if verbose and i == 0:\n",
        "                print(f\"\\n[생성] 다음 토큰 예측을 위한 로짓 형태: {next_token_logits.shape}\")\n",
        "                print(f\"[생성] 로짓 샘플 (처음 10개): {next_token_logits[:10]}\")\n",
        "\n",
        "            if temperature == 0.0:\n",
        "                next_token = torch.argmax(next_token_logits).item()\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "            else:\n",
        "                scaled_logits = next_token_logits / temperature\n",
        "                probs = F.softmax(scaled_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # 상위 5개 후보 저장\n",
        "            top_probs, top_indices = torch.topk(probs, 5)\n",
        "            step_candidates = []\n",
        "            for j, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
        "                if hasattr(tokenizer, 'idx_to_token'):\n",
        "                    token = tokenizer.idx_to_token[idx.item()]\n",
        "                else:\n",
        "                    token = tokenizer.idx_to_char[idx.item()]\n",
        "                step_candidates.append({\n",
        "                    'rank': j+1,\n",
        "                    'token': token,\n",
        "                    'probability': prob.item(),\n",
        "                    'selected': idx.item() == next_token\n",
        "                })\n",
        "            all_candidates.append(step_candidates)\n",
        "\n",
        "            if verbose and i < 5:  # 처음 5단계로 증가\n",
        "                # 현재까지 생성된 텍스트\n",
        "                current_text = tokenizer.decode(generated_tokens)\n",
        "                print(f\"\\n--- 생성 단계 {i+1} ---\")\n",
        "                print(f\"현재 텍스트: '{current_text}'\")\n",
        "                print(f\"다음 토큰 예측 후보:\")\n",
        "\n",
        "                # 확률 분포 시각화 (첫 번째 단계에서만)\n",
        "                if i == 0:\n",
        "                    print(f\"[확률 분포] 전체 어휘에 대한 확률 합: {probs.sum():.6f}\")\n",
        "                    print(f\"[확률 분포] 최대 확률: {probs.max():.6f}, 최소 확률: {probs.min():.6f}\")\n",
        "\n",
        "                # 실제 선택된 토큰이 상위 5개에 있는지 확인\n",
        "                selected_in_top5 = any(candidate['selected'] for candidate in step_candidates)\n",
        "\n",
        "                for candidate in step_candidates:\n",
        "                    selected_mark = \" ★\" if candidate['selected'] else \"\"\n",
        "                    # 각 후보를 추가했을 때의 텍스트 미리보기\n",
        "                    if hasattr(tokenizer, 'token_to_idx'):\n",
        "                        token_idx = tokenizer.token_to_idx.get(candidate['token'], tokenizer.token_to_idx['<unk>'])\n",
        "                    else:\n",
        "                        token_idx = tokenizer.char_to_idx.get(candidate['token'], tokenizer.char_to_idx['<unk>'])\n",
        "                    preview_tokens = generated_tokens + [token_idx]\n",
        "                    preview_text = tokenizer.decode(preview_tokens)\n",
        "                    print(f\"  {candidate['rank']}. '{candidate['token']}' → '{preview_text}' (확률: {candidate['probability']:.4f}){selected_mark}\")\n",
        "\n",
        "                # 선택된 토큰이 상위 5개에 없는 경우 경고 표시\n",
        "                if not selected_in_top5:\n",
        "                    if hasattr(tokenizer, 'idx_to_token'):\n",
        "                        selected_token = tokenizer.idx_to_token[next_token]\n",
        "                    else:\n",
        "                        selected_token = tokenizer.idx_to_char[next_token]\n",
        "                    print(f\"  ⚠️ 실제 선택된 토큰 '{selected_token}'이 상위 5개에 없음\")\n",
        "\n",
        "                # Temperature 효과 설명 (첫 번째 단계에서만)\n",
        "                if i == 0:\n",
        "                    print(f\"[Temperature 효과] {temperature:.1f} - \", end=\"\")\n",
        "                    if temperature < 0.7:\n",
        "                        print(\"보수적 생성 (높은 확률 토큰 선호)\")\n",
        "                    elif temperature > 1.3:\n",
        "                        print(\"창의적 생성 (다양한 토큰 선택)\")\n",
        "                    else:\n",
        "                        print(\"균형잡힌 생성\")\n",
        "\n",
        "            # 생성 종료 조건\n",
        "            eos_token_id = tokenizer.token_to_idx.get('<eos>', -1) if hasattr(tokenizer, 'token_to_idx') else tokenizer.char_to_idx.get('<eos>', -1)\n",
        "            if next_token == eos_token_id:\n",
        "                if verbose:\n",
        "                    print(f\"<eos> 토큰을 만나 생성을 종료함...\")\n",
        "                break\n",
        "\n",
        "            generated_tokens.append(next_token)\n",
        "\n",
        "        generated_text = tokenizer.decode(generated_tokens)\n",
        "\n",
        "        return generated_text, all_candidates\n",
        "\n",
        "# 텍스트 생성\n",
        "def run_generation(input_text=\"고기가\", max_length=20, show_analysis=True):\n",
        "    print(\"=\"*60)\n",
        "    print(\"GPT 모델 텍스트 생성\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 모델 로드\n",
        "    model, tokenizer = load_model('gpt_model.pth', device)\n",
        "\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    print(f\"입력 텍스트: '{input_text}'\")\n",
        "    print(f\"생성 길이: {max_length}\")\n",
        "    print()\n",
        "\n",
        "    # 다양한 Temperature로 생성\n",
        "    temperatures = [0.5, 1.0, 1.5]\n",
        "\n",
        "    for i, temp in enumerate(temperatures):\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Temperature {temp} 결과\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        result, candidates = generate_text_with_candidates(\n",
        "            model, tokenizer, input_text, max_length, device,\n",
        "            temperature=temp, verbose=(i == 0)  # 첫 번째만 상세 출력 (5단계)\n",
        "        )\n",
        "\n",
        "        print(f\"생성 결과: '{result}'\")\n",
        "        print()\n",
        "\n",
        "    print(\"텍스트 생성 완료!\")\n",
        "\n",
        "# 대화형 생성 함수\n",
        "def interactive_generation():\n",
        "    print(\"=\"*60)\n",
        "    print(\"대화형 텍스트 생성\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 모델 로드\n",
        "    model, tokenizer = load_model('gpt_model.pth', device)\n",
        "\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    print(\"모델 로드 성공...\")\n",
        "    print(\"종료하려면 'quit' 또는 'exit'를 입력하세요.\")\n",
        "    print()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"입력 텍스트: \").strip()\n",
        "\n",
        "            if user_input.lower() in ['quit', 'exit', '종료']:\n",
        "                print(\"생성을 종료합니다.\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                print(\"텍스트를 입력해주세요.\")\n",
        "                continue\n",
        "\n",
        "            # Temperature 선택\n",
        "            try:\n",
        "                temp_input = input(\"Temperature (0.5-2.0, 기본값 1.0): \").strip()\n",
        "                temperature = float(temp_input) if temp_input else 1.0\n",
        "                temperature = max(0.1, min(2.0, temperature))  # 범위 제한\n",
        "            except ValueError:\n",
        "                temperature = 1.0\n",
        "\n",
        "            # 길이 선택\n",
        "            try:\n",
        "                length_input = input(\"생성 길이 (기본값 15): \").strip()\n",
        "                max_length = int(length_input) if length_input else 15\n",
        "                max_length = max(1, min(50, max_length))  # 범위 제한\n",
        "            except ValueError:\n",
        "                max_length = 15\n",
        "\n",
        "            print(f\"\\n생성 중... (Temperature: {temperature}, 길이: {max_length})\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            result, candidates = generate_text_with_candidates(\n",
        "                model, tokenizer, user_input, max_length, device,\n",
        "                temperature=temperature, verbose=False\n",
        "            )\n",
        "\n",
        "            print(f\"결과: '{result}'\")\n",
        "            print(\"-\" * 40)\n",
        "            print()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n생성을 중단합니다.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "            print(\"다시 시도해주세요.\")\n",
        "            print()\n",
        "\n",
        "# 메인 실행\n",
        "print(\"GPT 모델 텍스트 생성 스크립트\")\n",
        "print(\"=\"*60)\n",
        "print(\"사용 방법:\")\n",
        "print(\"1. 기본 생성: run_generation('입력텍스트')\")\n",
        "print(\"2. 대화형 생성: interactive_generation()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 기본 실행\n",
        "run_generation(\"고기가\", max_length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ud97g8m4Sjl",
        "outputId": "5a8cb88e-e9ca-4d20-dba5-c0d612417ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT 모델 텍스트 생성 스크립트\n",
            "============================================================\n",
            "사용 방법:\n",
            "1. 기본 생성: run_generation('입력텍스트')\n",
            "2. 대화형 생성: interactive_generation()\n",
            "============================================================\n",
            "============================================================\n",
            "GPT 모델 텍스트 생성\n",
            "============================================================\n",
            "============================================================\n",
            "위치 인코딩 정보\n",
            "============================================================\n",
            "임베딩 차원: 128\n",
            "최대 길이: 32\n",
            "PE 값 범위: [-1.000, 1.000]\n",
            "sin/cos 함수로 생성\n",
            "\n",
            "============================================================\n",
            "모델 구조\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 4,310\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "FFN 차원: 256\n",
            "최대 시퀀스 길이: 32\n",
            "드롭아웃: 0.1\n",
            "총 파라미터: 1,372,886\n",
            "\n",
            "============================================================\n",
            "모델 로드 완료\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 4,310\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "최대 시퀀스 길이: 32\n",
            "토크나이저: KLUE/BERT\n",
            "\n",
            "입력 텍스트: '고기가'\n",
            "생성 길이: 20\n",
            "\n",
            "==================================================\n",
            "Temperature 0.5 결과\n",
            "==================================================\n",
            "입력: '고기가' (Temperature: 0.5)\n",
            "토큰화 결과: [1, 1183, 16, 2]\n",
            "\n",
            "============================================================\n",
            "모델 순전파 시작\n",
            "============================================================\n",
            "[모델] 입력 형태: torch.Size([1, 4])\n",
            "\n",
            "==================================================\n",
            "디코더 블록 처리 시작\n",
            "==================================================\n",
            "\n",
            "========================================\n",
            "어텐션 메커니즘 시작\n",
            "========================================\n",
            "[어텐션] 최종 출력 형태: torch.Size([1, 4, 128])\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "피드포워드 신경망 시작\n",
            "========================================\n",
            "[FFN] 입력 형태: torch.Size([1, 4, 128])\n",
            "[FFN] 출력 형태: torch.Size([1, 4, 128])\n",
            "========================================\n",
            "[디코더 블록] 최종 출력: torch.Size([1, 4, 128])\n",
            "==================================================\n",
            "\n",
            "[모델] 최종 로짓 형태: torch.Size([1, 4, 4310])\n",
            "============================================================\n",
            "\n",
            "[생성] 다음 토큰 예측을 위한 로짓 형태: torch.Size([4310])\n",
            "[생성] 로짓 샘플 (처음 10개): tensor([-2.2002, -1.8484,  1.8910, -2.6435, -2.5215, -1.8787, -1.4920, -0.7856,\n",
            "        -1.1787, -2.1357])\n",
            "\n",
            "--- 생성 단계 1 ---\n",
            "현재 텍스트: '고기가'\n",
            "다음 토큰 예측 후보:\n",
            "[확률 분포] 전체 어휘에 대한 확률 합: 1.000000\n",
            "[확률 분포] 최대 확률: 0.202144, 최소 확률: 0.000000\n",
            "  1. '.' → '고기가.' (확률: 0.2021)\n",
            "  2. '##었' → '고기가었' (확률: 0.1735)\n",
            "  3. '##다' → '고기가다' (확률: 0.1196)\n",
            "  4. '##의' → '고기가의' (확률: 0.0445)\n",
            "  5. '##는' → '고기가는' (확률: 0.0273)\n",
            "  ⚠️ 실제 선택된 토큰 '##어'이 상위 5개에 없음\n",
            "[Temperature 효과] 0.5 - 보수적 생성 (높은 확률 토큰 선호)\n",
            "\n",
            "--- 생성 단계 2 ---\n",
            "현재 텍스트: '고기가어'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '.' → '고기가어.' (확률: 0.2102)\n",
            "  2. '##었' → '고기가어었' (확률: 0.1312)\n",
            "  3. '##다' → '고기가어다' (확률: 0.0955)\n",
            "  4. '##의' → '고기가어의' (확률: 0.0896)\n",
            "  5. '##는' → '고기가어는' (확률: 0.0509)\n",
            "  ⚠️ 실제 선택된 토큰 '생긴'이 상위 5개에 없음\n",
            "\n",
            "--- 생성 단계 3 ---\n",
            "현재 텍스트: '고기가어 생긴'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '.' → '고기가어 생긴.' (확률: 0.6062) ★\n",
            "  2. '##었' → '고기가어 생긴었' (확률: 0.0538)\n",
            "  3. '##의' → '고기가어 생긴의' (확률: 0.0433)\n",
            "  4. '##다' → '고기가어 생긴다' (확률: 0.0247)\n",
            "  5. '<eos>' → '고기가어 생긴' (확률: 0.0243)\n",
            "\n",
            "--- 생성 단계 4 ---\n",
            "현재 텍스트: '고기가어 생긴.'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '##었' → '고기가어 생긴.었' (확률: 0.2327)\n",
            "  2. '.' → '고기가어 생긴..' (확률: 0.1777)\n",
            "  3. '##다' → '고기가어 생긴.다' (확률: 0.0441) ★\n",
            "  4. '##의' → '고기가어 생긴.의' (확률: 0.0431)\n",
            "  5. '##는' → '고기가어 생긴.는' (확률: 0.0381)\n",
            "\n",
            "--- 생성 단계 5 ---\n",
            "현재 텍스트: '고기가어 생긴.다'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '.' → '고기가어 생긴.다.' (확률: 0.8721) ★\n",
            "  2. '##었' → '고기가어 생긴.다었' (확률: 0.0361)\n",
            "  3. '##다' → '고기가어 생긴.다다' (확률: 0.0179)\n",
            "  4. '있' → '고기가어 생긴.다 있' (확률: 0.0065)\n",
            "  5. '##의' → '고기가어 생긴.다의' (확률: 0.0064)\n",
            "<eos> 토큰을 만나 생성을 종료함...\n",
            "생성 결과: '고기가어 생긴.다. 보낸다.었'\n",
            "\n",
            "==================================================\n",
            "Temperature 1.0 결과\n",
            "==================================================\n",
            "생성 결과: '고기가잡이 썩 자르다 볼주 몸집마다 동작 매듭 저녁 전해졌며했 삼키 꺼져 이놈 잡. 조각'\n",
            "\n",
            "==================================================\n",
            "Temperature 1.5 결과\n",
            "==================================================\n",
            "생성 결과: '고기가 암 빠지 말라 딕 봐라입 아래 가져올 갈색 3 견디 실존 아낌없이. 시작위 누릴야 싸워야 무거워'\n",
            "\n",
            "텍스트 생성 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 모델 학습 및 생성 통합 코드\n",
        "# Jupyter Notebook에서 두 개의 셀로 나누어 실행\n",
        "\n",
        "# =============================================================================\n",
        "# 첫 번째 셀: 라이브러리 임포트 및 모델 학습\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "# KLUE/BERT 토크나이저 import\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "    print(\"Transformers 사용 가능: KLUE/BERT 토크나이저 사용\")\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"Transformers 없음: 기본 토크나이저 사용\")\n",
        "\n",
        "# 하드웨어 환경 확인\n",
        "print(\"=\"*60)\n",
        "print(\"하드웨어 환경 확인\")\n",
        "print(\"=\"*60)\n",
        "print(f\"PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"CPU 전용 모드\")\n",
        "    device = 'cpu'\n",
        "print(f\"사용 디바이스: {device}\")\n",
        "print()\n",
        "\n",
        "# 텍스트 파일에서 데이터 로드\n",
        "def load_text_data(filename='헤밍웨이-노인과바다.txt'):\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
        "        print(f\"파일에서 {len(sentences)}개의 문장 로드\")\n",
        "\n",
        "        if len(sentences) == 0:\n",
        "            print(f\"'{filename}' 파일이 비어 있음\")\n",
        "            exit(1)\n",
        "\n",
        "        return sentences\n",
        "    except FileNotFoundError:\n",
        "        print(f\"'{filename}' 파일 없음\")\n",
        "        exit(1)\n",
        "\n",
        "# KLUE/BERT 토크나이저 래퍼 (출력 개선)\n",
        "class KLUETokenizer:\n",
        "    def __init__(self, texts):\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "            self.use_bert = True\n",
        "            print(\"KLUE/BERT 토크나이저 초기화 중...\")\n",
        "        else:\n",
        "            self.use_bert = False\n",
        "            print(\"기본 문자 단위 토크나이저로 대체...\")\n",
        "\n",
        "        if self.use_bert:\n",
        "            vocab_set = set()\n",
        "            printed_count = 0\n",
        "            total_texts = len(texts)\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                # 처음 5개, 마지막 5개만 출력\n",
        "                if i < 5 or i >= total_texts - 5:\n",
        "                    if printed_count == 5:\n",
        "                        print(\"... (중간 과정 생략) ...\")\n",
        "                    if i < 5 or i >= total_texts - 5:\n",
        "                        print(f\"토크나이징 진행: {i+1}/{total_texts}\")\n",
        "                        printed_count += 1\n",
        "\n",
        "                tokens = self.tokenizer.tokenize(text)\n",
        "                vocab_set.update(tokens)\n",
        "\n",
        "            special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "            bert_special = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
        "\n",
        "            self.vocab = special_tokens + bert_special + sorted(vocab_set)\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            self.token_to_idx = {token: i for i, token in enumerate(self.vocab)}\n",
        "            self.idx_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
        "        else:\n",
        "            vocab_set = set()\n",
        "            for text in texts:\n",
        "                vocab_set.update(text)\n",
        "\n",
        "            special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "            self.vocab = special_tokens + sorted(vocab_set)\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            self.token_to_idx = {token: i for i, token in enumerate(self.vocab)}\n",
        "            self.idx_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"토크나이저 정보\")\n",
        "        print(\"=\"*60)\n",
        "        if self.use_bert:\n",
        "            print(f\"토크나이저 타입: KLUE/BERT WordPiece\")\n",
        "            print(f\"어휘 크기: {self.vocab_size:,}\")\n",
        "            print(f\"토큰 예시: {self.vocab[9:19]}\")\n",
        "        else:\n",
        "            print(f\"토크나이저 타입: 문자 단위 (Character-level)\")\n",
        "            print(f\"어휘 크기: {self.vocab_size:,}\")\n",
        "        print(f\"특수 토큰: {['<pad>', '<sos>', '<eos>', '<unk>']}\")\n",
        "        print()\n",
        "\n",
        "    def encode(self, text):\n",
        "        if self.use_bert:\n",
        "            bert_tokens = self.tokenizer.tokenize(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in bert_tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        else:\n",
        "            tokens = list(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = []\n",
        "        for idx in token_ids:\n",
        "            token = self.idx_to_token[idx]\n",
        "            if token not in ['<pad>', '<sos>', '<eos>', '<unk>', '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
        "                tokens.append(token)\n",
        "\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            return self.tokenizer.convert_tokens_to_string(tokens)\n",
        "        else:\n",
        "            return ''.join(tokens)\n",
        "\n",
        "# 위치 인코딩\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"위치 인코딩 정보\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"임베딩 차원: {d_model}\")\n",
        "        print(f\"최대 길이: {max_len}\")\n",
        "        print(f\"PE 값 범위: [{pe.min():.3f}, {pe.max():.3f}]\")\n",
        "        print(\"sin/cos 함수로 생성\")\n",
        "        print()\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len]\n",
        "\n",
        "# 멀티헤드 어텐션 메커니즘\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "\n",
        "    def forward(self, x, mask: Optional[torch.Tensor] = None, verbose: bool = False):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(\"어텐션 메커니즘 시작\")\n",
        "            print(\"=\"*40)\n",
        "\n",
        "        Q = self.w_q(x)\n",
        "        K = self.w_k(x)\n",
        "        V = self.w_v(x)\n",
        "\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.w_o(output)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[어텐션] 최종 출력 형태: {output.shape}\")\n",
        "            print(\"=\"*40)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 피드포워드 신경망\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, verbose: bool = False):\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(\"피드포워드 신경망 시작\")\n",
        "            print(\"=\"*40)\n",
        "            print(f\"[FFN] 입력 형태: {x.shape}\")\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[FFN] 출력 형태: {x.shape}\")\n",
        "            print(\"=\"*40)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Decoder Block\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask: Optional[torch.Tensor] = None, verbose: bool = False):\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"디코더 블록 처리 시작\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        attn_output = self.attention(x, mask, verbose)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        ff_output = self.feed_forward(x, verbose)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[디코더 블록] 최종 출력: {x.shape}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        return x\n",
        "\n",
        "# GPT Style Decoder-Only Transformer\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_heads: int,\n",
        "                 n_layers: int, d_ff: int, max_seq_len: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"모델 구조\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"모델 타입: GPT (Decoder-Only Transformer)\")\n",
        "        print(f\"어휘 크기: {vocab_size:,}\")\n",
        "        print(f\"임베딩 차원: {d_model}\")\n",
        "        print(f\"어텐션 헤드: {n_heads}\")\n",
        "        print(f\"디코더 층: {n_layers}\")\n",
        "        print(f\"FFN 차원: {d_ff}\")\n",
        "        print(f\"최대 시퀀스 길이: {max_seq_len}\")\n",
        "        print(f\"드롭아웃: {dropout}\")\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"총 파라미터: {total_params:,}\")\n",
        "        print()\n",
        "\n",
        "    def create_causal_mask(self, seq_len: int, device: torch.device):\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
        "        return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x, verbose: bool = False):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"모델 순전파 시작\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"[모델] 입력 형태: {x.shape}\")\n",
        "\n",
        "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        causal_mask = self.create_causal_mask(seq_len, x.device)\n",
        "\n",
        "        for i, block in enumerate(self.decoder_blocks):\n",
        "            x = block(x, causal_mask, verbose and i == 0)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[모델] 최종 로짓 형태: {logits.shape}\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# 배치 데이터 생성\n",
        "def create_batches(tokenizer, texts, max_seq_len, batch_size):\n",
        "    all_tokens = []\n",
        "\n",
        "    print(f\"배치 생성 중... (최대 시퀀스 길이: {max_seq_len})\")\n",
        "\n",
        "    total_texts = len(texts)\n",
        "    printed_count = 0\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        # 처음 5개, 마지막 5개만 출력\n",
        "        if i < 5 or i >= total_texts - 5:\n",
        "            if printed_count == 5:\n",
        "                print(\"... (중간 과정 생략) ...\")\n",
        "            print(f\"처리된 텍스트: {i+1}/{total_texts}, 생성된 시퀀스: {len(all_tokens)}\")\n",
        "            printed_count += 1\n",
        "\n",
        "        tokens = tokenizer.encode(text)\n",
        "\n",
        "        # 토큰이 너무 길면 여러 청크로 나눔\n",
        "        if len(tokens) > max_seq_len:\n",
        "            for start in range(0, len(tokens), max_seq_len - 2):\n",
        "                chunk = tokens[start:start + max_seq_len - 2]\n",
        "                if len(chunk) >= 10:\n",
        "                    chunk = [tokenizer.token_to_idx['<sos>']] + chunk[1:-1] + [tokenizer.token_to_idx['<eos>']]\n",
        "\n",
        "                    while len(chunk) < max_seq_len:\n",
        "                        chunk.append(tokenizer.token_to_idx['<pad>'])\n",
        "\n",
        "                    all_tokens.append(chunk)\n",
        "        else:\n",
        "            while len(tokens) < max_seq_len:\n",
        "                tokens.append(tokenizer.token_to_idx['<pad>'])\n",
        "            all_tokens.append(tokens)\n",
        "\n",
        "    if len(all_tokens) == 0:\n",
        "        print(\"ERROR: 처리된 토큰 시퀀스가 없음!\")\n",
        "        return []\n",
        "\n",
        "    print(f\"총 {len(all_tokens)}개의 시퀀스가 생성...\")\n",
        "\n",
        "    data = torch.tensor(all_tokens, dtype=torch.long)\n",
        "\n",
        "    # 배치 생성 (불완전한 배치도 포함)\n",
        "    batches = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        batches.append(batch)\n",
        "\n",
        "    print(f\"총 {len(batches)}개의 배치 생성...\")\n",
        "    return batches\n",
        "\n",
        "# 모델 학습\n",
        "def train_model(model, batches, tokenizer, epochs, device, lr=3e-4):\n",
        "    if len(batches) == 0:\n",
        "        print(\"ERROR: 학습할 배치가 없음!!\")\n",
        "        print(\"- 텍스트 파일의 문장이 너무 적거나\")\n",
        "        print(\"- 배치 크기가 너무 크거나\")\n",
        "        print(\"- 시퀀스 길이가 너무 길 수 있습니다.\")\n",
        "        return []\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.95))\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=epochs//2)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_idx['<pad>'])\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 학습 시작\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"학습 배치 수: {len(batches)}\")\n",
        "    print(f\"초기 학습률: {lr}\")\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(batches):\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(inputs, verbose=(epoch == 0 and batch_idx == 0))\n",
        "\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if num_batches >= min(20, len(batches)):\n",
        "                break\n",
        "\n",
        "        if num_batches > 0:\n",
        "            avg_loss = total_loss / num_batches\n",
        "            losses.append(avg_loss)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"에포크 {epoch+1:2d}/{epochs}, 손실: {avg_loss:.4f}, 학습률: {current_lr:.6f}\")\n",
        "        else:\n",
        "            print(f\"에포크 {epoch+1:2d}/{epochs}, 배치 없음 - 학습 중단!!\")\n",
        "            break\n",
        "\n",
        "    return losses\n",
        "\n",
        "# 모델 저장\n",
        "def save_model(model, tokenizer, filepath):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab_size': tokenizer.vocab_size,\n",
        "        'd_model': model.d_model,\n",
        "        'n_heads': model.n_heads,\n",
        "        'n_layers': model.n_layers,\n",
        "        'd_ff': model.d_ff,\n",
        "        'max_seq_len': model.max_seq_len,\n",
        "        'vocab': tokenizer.vocab,\n",
        "        'token_to_idx': tokenizer.token_to_idx,\n",
        "        'idx_to_token': tokenizer.idx_to_token,\n",
        "        'use_bert': tokenizer.use_bert\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"모델이 '{filepath}'에 저장되었습니다.\")\n",
        "\n",
        "# 학습 실행 함수\n",
        "def run_training():\n",
        "    # 하이퍼파라미터\n",
        "    MAX_SEQ_LEN = 32      # 시퀀스 길이\n",
        "    BATCH_SIZE = 1        # 배치 크기\n",
        "    D_MODEL = 128         # 모델 크기\n",
        "    N_HEADS = 4           # 헤드 수\n",
        "    N_LAYERS = 2          # 레이어 수\n",
        "    D_FF = 256            # FFN 크기\n",
        "    EPOCHS = 5            # 에포크\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 학습 모드\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"시퀀스 길이: {MAX_SEQ_LEN}\")\n",
        "    print(f\"배치 크기: {BATCH_SIZE}\")\n",
        "    print(f\"모델 크기: {D_MODEL}\")\n",
        "    print()\n",
        "\n",
        "    # 데이터 로드 및 토크나이저 생성\n",
        "    texts = load_text_data()\n",
        "    tokenizer = KLUETokenizer(texts)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = GPTModel(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        max_seq_len=MAX_SEQ_LEN\n",
        "    ).to(device)\n",
        "\n",
        "    # 배치 데이터 생성\n",
        "    batches = create_batches(tokenizer, texts, MAX_SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "    if len(batches) == 0:\n",
        "        print(\"ERROR: 배치 생성에 실패!!\")\n",
        "        print(\"1. 텍스트 파일에 충분한 내용이 있는지?\")\n",
        "        print(\"2. 텍스트 파일 인코딩이 UTF-8인지?\")\n",
        "        return False\n",
        "\n",
        "    print(f\"총 배치 수: {len(batches)}\")\n",
        "\n",
        "    # 모델 학습\n",
        "    losses = train_model(model, batches, tokenizer, EPOCHS, device)\n",
        "\n",
        "    # 모델 저장\n",
        "    if losses:\n",
        "        save_model(model, tokenizer, 'gpt_model.pth')\n",
        "        print(\"학습 완료!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"학습 실패 - 모델 저장 실패!!\")\n",
        "        return False\n",
        "\n",
        "# 학습 실행\n",
        "print(\"GPT 모델 학습\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "success = run_training()\n",
        "\n",
        "if success:\n",
        "    print(\"\\n학습 성공...\")\n",
        "else:\n",
        "    print(\"\\n학습 실패!!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 두 번째 셀: 모델 로드 및 텍스트 생성\n",
        "# =============================================================================\n",
        "\n",
        "# 로드된 토크나이저 클래스\n",
        "class LoadedTokenizer:\n",
        "    def __init__(self, vocab, token_to_idx, idx_to_token, use_bert):\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.token_to_idx = token_to_idx\n",
        "        self.idx_to_token = idx_to_token\n",
        "        self.use_bert = use_bert\n",
        "        if use_bert and TRANSFORMERS_AVAILABLE:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            bert_tokens = self.tokenizer.tokenize(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in bert_tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        else:\n",
        "            tokens = list(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = []\n",
        "        for idx in token_ids:\n",
        "            token = self.idx_to_token[idx]\n",
        "            if token not in ['<pad>', '<sos>', '<eos>', '<unk>', '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
        "                tokens.append(token)\n",
        "\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            return self.tokenizer.convert_tokens_to_string(tokens)\n",
        "        else:\n",
        "            return ''.join(tokens)\n",
        "\n",
        "# 모델 로드\n",
        "def load_model(filepath, device):\n",
        "    try:\n",
        "        checkpoint = torch.load(filepath, map_location=device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"모델 파일 '{filepath}'을 찾기 실패!!\")\n",
        "        return None, None\n",
        "\n",
        "    # 토크나이저 복원\n",
        "    tokenizer = LoadedTokenizer(\n",
        "        checkpoint['vocab'],\n",
        "        checkpoint['token_to_idx'],\n",
        "        checkpoint['idx_to_token'],\n",
        "        checkpoint['use_bert']\n",
        "    )\n",
        "\n",
        "    # 모델 생성 및 가중치 로드\n",
        "    model = GPTModel(\n",
        "        vocab_size=checkpoint['vocab_size'],\n",
        "        d_model=checkpoint['d_model'],\n",
        "        n_heads=checkpoint['n_heads'],\n",
        "        n_layers=checkpoint['n_layers'],\n",
        "        d_ff=checkpoint['d_ff'],\n",
        "        max_seq_len=checkpoint['max_seq_len']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()  # 평가 모드로 설정\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 로드 완료\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"모델 타입: GPT (Decoder-Only Transformer)\")\n",
        "    print(f\"어휘 크기: {checkpoint['vocab_size']:,}\")\n",
        "    print(f\"임베딩 차원: {checkpoint['d_model']}\")\n",
        "    print(f\"어텐션 헤드: {checkpoint['n_heads']}\")\n",
        "    print(f\"디코더 층: {checkpoint['n_layers']}\")\n",
        "    print(f\"최대 시퀀스 길이: {checkpoint['max_seq_len']}\")\n",
        "    print(f\"토크나이저: {'KLUE/BERT' if checkpoint['use_bert'] else '문자 단위'}\")\n",
        "    print()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# 텍스트 생성 (확률 상위 5개 후보 표시)\n",
        "def generate_text_with_candidates(model, tokenizer, prompt, max_length, device, temperature=1.0, verbose=False):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens = tokenizer.encode(prompt)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"입력: '{prompt}' (Temperature: {temperature})\")\n",
        "            print(f\"토큰화 결과: {tokens}\")\n",
        "\n",
        "        generated_tokens = tokens.copy()\n",
        "        all_candidates = []\n",
        "\n",
        "        for i in range(max_length):\n",
        "            if len(generated_tokens) >= model.max_seq_len:\n",
        "                if verbose:\n",
        "                    print(f\"최대 시퀀스 길이({model.max_seq_len})에 도달!!\")\n",
        "                break\n",
        "\n",
        "            current_input = torch.tensor([generated_tokens], device=device)\n",
        "            logits = model(current_input)\n",
        "\n",
        "            next_token_logits = logits[0, -1]\n",
        "\n",
        "            if temperature == 0.0:\n",
        "                next_token = torch.argmax(next_token_logits).item()\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "            else:\n",
        "                scaled_logits = next_token_logits / temperature\n",
        "                probs = F.softmax(scaled_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # 상위 5개 후보 저장\n",
        "            top_probs, top_indices = torch.topk(probs, 5)\n",
        "            step_candidates = []\n",
        "            for j, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
        "                token = tokenizer.idx_to_token[idx.item()]\n",
        "                step_candidates.append({\n",
        "                    'rank': j+1,\n",
        "                    'token': token,\n",
        "                    'probability': prob.item(),\n",
        "                    'selected': idx.item() == next_token\n",
        "                })\n",
        "            all_candidates.append(step_candidates)\n",
        "\n",
        "            if verbose and i < 5:  # 처음 5단계로 증가\n",
        "                # 현재까지 생성된 텍스트\n",
        "                current_text = tokenizer.decode(generated_tokens)\n",
        "                print(f\"\\n--- 생성 단계 {i+1} ---\")\n",
        "                print(f\"현재 텍스트: '{current_text}'\")\n",
        "                print(f\"다음 토큰 예측 후보:\")\n",
        "\n",
        "                # 실제 선택된 토큰이 상위 5개에 있는지 확인\n",
        "                selected_in_top5 = any(candidate['selected'] for candidate in step_candidates)\n",
        "\n",
        "                for candidate in step_candidates:\n",
        "                    selected_mark = \" ★\" if candidate['selected'] else \"\"\n",
        "                    # 각 후보를 추가했을 때의 텍스트 미리보기\n",
        "                    preview_tokens = generated_tokens + [tokenizer.token_to_idx.get(candidate['token'], tokenizer.token_to_idx['<unk>'])]\n",
        "                    preview_text = tokenizer.decode(preview_tokens)\n",
        "                    print(f\"  {candidate['rank']}. '{candidate['token']}' → '{preview_text}' (확률: {candidate['probability']:.4f}){selected_mark}\")\n",
        "\n",
        "                # 선택된 토큰이 상위 5개에 없는 경우 경고 표시\n",
        "                if not selected_in_top5:\n",
        "                    selected_token = tokenizer.idx_to_token[next_token]\n",
        "                    print(f\"  ⚠️ 실제 선택된 토큰 '{selected_token}'이 상위 5개에 없음\")\n",
        "\n",
        "            # 생성 종료 조건\n",
        "            if next_token == tokenizer.token_to_idx.get('<eos>', -1):\n",
        "                if verbose:\n",
        "                    print(f\"<eos> 토큰을 만나 생성을 종료함...\")\n",
        "                break\n",
        "\n",
        "            generated_tokens.append(next_token)\n",
        "\n",
        "        generated_text = tokenizer.decode(generated_tokens)\n",
        "\n",
        "        return generated_text, all_candidates\n",
        "\n",
        "# 후보 분석 출력 함수 제거 (더 이상 사용하지 않음)\n",
        "\n",
        "# 텍스트 생성\n",
        "def run_generation(input_text=\"고기가\", max_length=20, show_analysis=True):\n",
        "    print(\"=\"*60)\n",
        "    print(\"GPT 모델 텍스트 생성\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 모델 로드\n",
        "    model, tokenizer = load_model('gpt_model.pth', device)\n",
        "\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    print(f\"입력 텍스트: '{input_text}'\")\n",
        "    print(f\"생성 길이: {max_length}\")\n",
        "    print()\n",
        "\n",
        "    # 다양한 Temperature로 생성\n",
        "    temperatures = [0.5, 1.0, 1.5]\n",
        "\n",
        "    for i, temp in enumerate(temperatures):\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Temperature {temp} 결과\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        result, candidates = generate_text_with_candidates(\n",
        "            model, tokenizer, input_text, max_length, device,\n",
        "            temperature=temp, verbose=(i == 0)  # 첫 번째만 상세 출력 (5단계)\n",
        "        )\n",
        "\n",
        "        print(f\"생성 결과: '{result}'\")\n",
        "        print()\n",
        "\n",
        "    print(\"텍스트 생성 완료!\")\n",
        "\n",
        "# 대화형 생성 함수\n",
        "def interactive_generation():\n",
        "    print(\"=\"*60)\n",
        "    print(\"대화형 텍스트 생성\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 모델 로드\n",
        "    model, tokenizer = load_model('gpt_model.pth', device)\n",
        "\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    print(\"모델 로드 성공...\")\n",
        "    print(\"종료하려면 'quit' 또는 'exit'를 입력하세요.\")\n",
        "    print()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"입력 텍스트: \").strip()\n",
        "\n",
        "            if user_input.lower() in ['quit', 'exit', '종료']:\n",
        "                print(\"생성을 종료합니다.\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                print(\"텍스트를 입력해주세요.\")\n",
        "                continue\n",
        "\n",
        "            # Temperature 선택\n",
        "            try:\n",
        "                temp_input = input(\"Temperature (0.5-2.0, 기본값 1.0): \").strip()\n",
        "                temperature = float(temp_input) if temp_input else 1.0\n",
        "                temperature = max(0.1, min(2.0, temperature))  # 범위 제한\n",
        "            except ValueError:\n",
        "                temperature = 1.0\n",
        "\n",
        "            # 길이 선택\n",
        "            try:\n",
        "                length_input = input(\"생성 길이 (기본값 15): \").strip()\n",
        "                max_length = int(length_input) if length_input else 15\n",
        "                max_length = max(1, min(50, max_length))  # 범위 제한\n",
        "            except ValueError:\n",
        "                max_length = 15\n",
        "\n",
        "            print(f\"\\n생성 중... (Temperature: {temperature}, 길이: {max_length})\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            result, candidates = generate_text_with_candidates(\n",
        "                model, tokenizer, user_input, max_length, device,\n",
        "                temperature=temperature, verbose=False\n",
        "            )\n",
        "\n",
        "            print(f\"결과: '{result}'\")\n",
        "            print(\"-\" * 40)\n",
        "            print()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n생성을 중단합니다.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "            print(\"다시 시도해주세요.\")\n",
        "            print()\n",
        "\n",
        "# 메인 실행\n",
        "print(\"GPT 모델 텍스트 생성 스크립트\")\n",
        "print(\"=\"*60)\n",
        "print(\"사용 방법:\")\n",
        "print(\"1. 기본 생성: run_generation('입력텍스트')\")\n",
        "print(\"2. 대화형 생성: interactive_generation()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 기본 실행\n",
        "run_generation(\"노인은\", max_length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ILRwizJK8fI",
        "outputId": "a38117c2-6d66-4535-f6d4-0134ed5855af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers 사용 가능: KLUE/BERT 토크나이저 사용\n",
            "============================================================\n",
            "하드웨어 환경 확인\n",
            "============================================================\n",
            "PyTorch 버전: 2.8.0+cu126\n",
            "CUDA 사용 가능: False\n",
            "CPU 전용 모드\n",
            "사용 디바이스: cpu\n",
            "\n",
            "GPT 모델 학습\n",
            "============================================================\n",
            "============================================================\n",
            "모델 학습 모드\n",
            "============================================================\n",
            "시퀀스 길이: 32\n",
            "배치 크기: 1\n",
            "모델 크기: 128\n",
            "\n",
            "파일에서 1개의 문장 로드\n",
            "KLUE/BERT 토크나이저 초기화 중...\n",
            "토크나이징 진행: 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (45087 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "토크나이저 정보\n",
            "============================================================\n",
            "토크나이저 타입: KLUE/BERT WordPiece\n",
            "어휘 크기: 4,310\n",
            "토큰 예시: ['!', '##1', '##19', '##2', '##29', '##54', '##8', '##가', '##가죽', '##가지']\n",
            "특수 토큰: ['<pad>', '<sos>', '<eos>', '<unk>']\n",
            "\n",
            "============================================================\n",
            "위치 인코딩 정보\n",
            "============================================================\n",
            "임베딩 차원: 128\n",
            "최대 길이: 32\n",
            "PE 값 범위: [-1.000, 1.000]\n",
            "sin/cos 함수로 생성\n",
            "\n",
            "============================================================\n",
            "모델 구조\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 4,310\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "FFN 차원: 256\n",
            "최대 시퀀스 길이: 32\n",
            "드롭아웃: 0.1\n",
            "총 파라미터: 1,372,886\n",
            "\n",
            "배치 생성 중... (최대 시퀀스 길이: 32)\n",
            "처리된 텍스트: 1/1, 생성된 시퀀스: 0\n",
            "총 1503개의 시퀀스가 생성...\n",
            "총 1503개의 배치 생성...\n",
            "총 배치 수: 1503\n",
            "============================================================\n",
            "모델 학습 시작\n",
            "============================================================\n",
            "학습 배치 수: 1503\n",
            "초기 학습률: 0.0003\n",
            "\n",
            "============================================================\n",
            "모델 순전파 시작\n",
            "============================================================\n",
            "[모델] 입력 형태: torch.Size([1, 31])\n",
            "\n",
            "==================================================\n",
            "디코더 블록 처리 시작\n",
            "==================================================\n",
            "\n",
            "========================================\n",
            "어텐션 메커니즘 시작\n",
            "========================================\n",
            "[어텐션] 최종 출력 형태: torch.Size([1, 31, 128])\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "피드포워드 신경망 시작\n",
            "========================================\n",
            "[FFN] 입력 형태: torch.Size([1, 31, 128])\n",
            "[FFN] 출력 형태: torch.Size([1, 31, 128])\n",
            "========================================\n",
            "[디코더 블록] 최종 출력: torch.Size([1, 31, 128])\n",
            "==================================================\n",
            "\n",
            "[모델] 최종 로짓 형태: torch.Size([1, 31, 4310])\n",
            "============================================================\n",
            "에포크  1/5, 손실: 8.3988, 학습률: 0.000300\n",
            "에포크  2/5, 손실: 7.8603, 학습률: 0.000300\n",
            "에포크  3/5, 손실: 7.2336, 학습률: 0.000300\n",
            "에포크  4/5, 손실: 6.5542, 학습률: 0.000300\n",
            "에포크  5/5, 손실: 5.9937, 학습률: 0.000300\n",
            "모델이 'gpt_model.pth'에 저장되었습니다.\n",
            "학습 완료!\n",
            "\n",
            "학습 성공...\n",
            "GPT 모델 텍스트 생성 스크립트\n",
            "============================================================\n",
            "사용 방법:\n",
            "1. 기본 생성: run_generation('입력텍스트')\n",
            "2. 대화형 생성: interactive_generation()\n",
            "============================================================\n",
            "============================================================\n",
            "GPT 모델 텍스트 생성\n",
            "============================================================\n",
            "============================================================\n",
            "위치 인코딩 정보\n",
            "============================================================\n",
            "임베딩 차원: 128\n",
            "최대 길이: 32\n",
            "PE 값 범위: [-1.000, 1.000]\n",
            "sin/cos 함수로 생성\n",
            "\n",
            "============================================================\n",
            "모델 구조\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 4,310\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "FFN 차원: 256\n",
            "최대 시퀀스 길이: 32\n",
            "드롭아웃: 0.1\n",
            "총 파라미터: 1,372,886\n",
            "\n",
            "============================================================\n",
            "모델 로드 완료\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 4,310\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "최대 시퀀스 길이: 32\n",
            "토크나이저: KLUE/BERT\n",
            "\n",
            "입력 텍스트: '노인은'\n",
            "생성 길이: 20\n",
            "\n",
            "==================================================\n",
            "Temperature 0.5 결과\n",
            "==================================================\n",
            "입력: '노인은' (Temperature: 0.5)\n",
            "토큰화 결과: [1, 1624, 670, 2]\n",
            "\n",
            "--- 생성 단계 1 ---\n",
            "현재 텍스트: '노인은'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '.' → '노인은.' (확률: 0.3472) ★\n",
            "  2. '##이' → '노인은이' (확률: 0.0787)\n",
            "  3. '<eos>' → '노인은' (확률: 0.0774)\n",
            "  4. '##은' → '노인은은' (확률: 0.0561)\n",
            "  5. '##다' → '노인은다' (확률: 0.0377)\n",
            "\n",
            "--- 생성 단계 2 ---\n",
            "현재 텍스트: '노인은.'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '.' → '노인은..' (확률: 0.4634) ★\n",
            "  2. '##이' → '노인은.이' (확률: 0.0622)\n",
            "  3. '<eos>' → '노인은.' (확률: 0.0620)\n",
            "  4. '있' → '노인은. 있' (확률: 0.0257)\n",
            "  5. '노인' → '노인은. 노인' (확률: 0.0213)\n",
            "\n",
            "--- 생성 단계 3 ---\n",
            "현재 텍스트: '노인은..'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '.' → '노인은...' (확률: 0.4662)\n",
            "  2. '<eos>' → '노인은..' (확률: 0.0636)\n",
            "  3. '##이' → '노인은..이' (확률: 0.0584)\n",
            "  4. '있' → '노인은.. 있' (확률: 0.0247)\n",
            "  5. '노인' → '노인은.. 노인' (확률: 0.0213)\n",
            "  ⚠️ 실제 선택된 토큰 '그래서'이 상위 5개에 없음\n",
            "\n",
            "--- 생성 단계 4 ---\n",
            "현재 텍스트: '노인은.. 그래서'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '.' → '노인은.. 그래서.' (확률: 0.4619) ★\n",
            "  2. '##이' → '노인은.. 그래서이' (확률: 0.0820)\n",
            "  3. '<eos>' → '노인은.. 그래서' (확률: 0.0535)\n",
            "  4. '##다' → '노인은.. 그래서다' (확률: 0.0337)\n",
            "  5. '##었' → '노인은.. 그래서었' (확률: 0.0297)\n",
            "\n",
            "--- 생성 단계 5 ---\n",
            "현재 텍스트: '노인은.. 그래서.'\n",
            "다음 토큰 예측 후보:\n",
            "  1. '.' → '노인은.. 그래서..' (확률: 0.4716)\n",
            "  2. '<eos>' → '노인은.. 그래서.' (확률: 0.0700)\n",
            "  3. '##이' → '노인은.. 그래서.이' (확률: 0.0394)\n",
            "  4. '노인' → '노인은.. 그래서. 노인' (확률: 0.0283)\n",
            "  5. '있' → '노인은.. 그래서. 있' (확률: 0.0271)\n",
            "  ⚠️ 실제 선택된 토큰 '##을'이 상위 5개에 없음\n",
            "<eos> 토큰을 만나 생성을 종료함...\n",
            "생성 결과: '노인은.. 그래서.을..락의'\n",
            "\n",
            "==================================================\n",
            "Temperature 1.0 결과\n",
            "==================================================\n",
            "생성 결과: '노인은 아래 증명 파묻 움직이 '효과디 삭스놈 산다 넓이 그래서 기다렸 걸쳐 패배들이 덕택를인 쓰이'\n",
            "\n",
            "==================================================\n",
            "Temperature 1.5 결과\n",
            "==================================================\n",
            "생성 결과: '노인은 모여 육지 두른가 상어 끈질기 드릴까요그러 가장 너머 밀가루 대면 일정 느꼈 요령 때려 무게 어머니 실제 맛있'\n",
            "\n",
            "텍스트 생성 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 모델 학습 및 생성 통합 코드\n",
        "# Jupyter Notebook에서 두 개의 셀로 나누어 실행\n",
        "\n",
        "# =============================================================================\n",
        "# 첫 번째 셀: 라이브러리 임포트 및 모델 학습\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "# KLUE/BERT 토크나이저 import\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "    print(\"Transformers 사용 가능: KLUE/BERT 토크나이저 사용\")\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"Transformers 없음: 기본 토크나이저 사용\")\n",
        "\n",
        "# 하드웨어 환경 확인\n",
        "print(\"=\"*60)\n",
        "print(\"하드웨어 환경 확인\")\n",
        "print(\"=\"*60)\n",
        "print(f\"PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"CPU 전용 모드\")\n",
        "    device = 'cpu'\n",
        "print(f\"사용 디바이스: {device}\")\n",
        "print()\n",
        "\n",
        "# 텍스트 파일에서 데이터 로드\n",
        "def load_text_data(filename='헤밍웨이-노인과바다.txt'):\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
        "        print(f\"파일에서 {len(sentences)}개의 문장 로드\")\n",
        "\n",
        "        if len(sentences) == 0:\n",
        "            print(f\"'{filename}' 파일이 비어 있음\")\n",
        "            exit(1)\n",
        "\n",
        "        return sentences\n",
        "    except FileNotFoundError:\n",
        "        print(f\"'{filename}' 파일 없음\")\n",
        "        exit(1)\n",
        "\n",
        "# KLUE/BERT 토크나이저 래퍼 (출력 개선)\n",
        "class KLUETokenizer:\n",
        "    def __init__(self, texts):\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            # 경고 메시지 억제를 위한 설정\n",
        "            import warnings\n",
        "            warnings.filterwarnings(\"ignore\", message=\"Token indices sequence length is longer than\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "            self.use_bert = True\n",
        "            print(\"KLUE/BERT 토크나이저 초기화 중...\")\n",
        "        else:\n",
        "            self.use_bert = False\n",
        "            print(\"기본 문자 단위 토크나이저로 대체...\")\n",
        "\n",
        "        if self.use_bert:\n",
        "            vocab_set = set()\n",
        "            printed_count = 0\n",
        "            total_texts = len(texts)\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                # 처음 5개, 마지막 5개만 출력\n",
        "                if i < 5 or i >= total_texts - 5:\n",
        "                    if printed_count == 5:\n",
        "                        print(\"... (중간 과정 생략) ...\")\n",
        "                    if i < 5 or i >= total_texts - 5:\n",
        "                        print(f\"토크나이징 진행: {i+1}/{total_texts}\")\n",
        "                        printed_count += 1\n",
        "\n",
        "                # 텍스트 길이 제한 (BERT 최대 길이 고려)\n",
        "                truncated_text = text[:100] if len(text) > 100 else text\n",
        "                tokens = self.tokenizer.tokenize(truncated_text)\n",
        "                vocab_set.update(tokens)\n",
        "\n",
        "            special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "            bert_special = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
        "\n",
        "            self.vocab = special_tokens + bert_special + sorted(vocab_set)\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            self.token_to_idx = {token: i for i, token in enumerate(self.vocab)}\n",
        "            self.idx_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
        "        else:\n",
        "            vocab_set = set()\n",
        "            for text in texts:\n",
        "                vocab_set.update(text)\n",
        "\n",
        "            special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "            self.vocab = special_tokens + sorted(vocab_set)\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            self.token_to_idx = {token: i for i, token in enumerate(self.vocab)}\n",
        "            self.idx_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"토크나이저 정보\")\n",
        "        print(\"=\"*60)\n",
        "        if self.use_bert:\n",
        "            print(f\"토크나이저 타입: KLUE/BERT WordPiece\")\n",
        "            print(f\"어휘 크기: {self.vocab_size:,}\")\n",
        "            print(f\"토큰 예시: {self.vocab[9:19]}\")\n",
        "        else:\n",
        "            print(f\"토크나이저 타입: 문자 단위 (Character-level)\")\n",
        "            print(f\"어휘 크기: {self.vocab_size:,}\")\n",
        "        print(f\"특수 토큰: {['<pad>', '<sos>', '<eos>', '<unk>']}\")\n",
        "        print()\n",
        "\n",
        "    def encode(self, text):\n",
        "        if self.use_bert:\n",
        "            # 텍스트 길이 제한 (BERT 최대 길이 고려)\n",
        "            truncated_text = text[:100] if len(text) > 100 else text\n",
        "            bert_tokens = self.tokenizer.tokenize(truncated_text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in bert_tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        else:\n",
        "            tokens = list(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = []\n",
        "        for idx in token_ids:\n",
        "            if hasattr(self, 'idx_to_token'):\n",
        "                token = self.idx_to_token[idx]\n",
        "            else:\n",
        "                token = self.idx_to_char[idx]\n",
        "            if token not in ['<pad>', '<sos>', '<eos>', '<unk>', '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
        "                tokens.append(token)\n",
        "\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            return self.tokenizer.convert_tokens_to_string(tokens)\n",
        "        else:\n",
        "            return ''.join(tokens)\n",
        "\n",
        "# 위치 인코딩\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"위치 인코딩 정보\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"임베딩 차원: {d_model}\")\n",
        "        print(f\"최대 길이: {max_len}\")\n",
        "        print(f\"PE 값 범위: [{pe.min():.3f}, {pe.max():.3f}]\")\n",
        "        print(\"sin/cos 함수로 생성\")\n",
        "        print()\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len]\n",
        "\n",
        "# 멀티헤드 어텐션 메커니즘\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "\n",
        "    def forward(self, x, mask: Optional[torch.Tensor] = None, verbose: bool = False):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"Attention Mechanism Analysis\")\n",
        "            print(\"=\"*50)\n",
        "            print(f\"Input shape: {x.shape}\")\n",
        "            print(f\"Batch size: {batch_size}, Sequence length: {seq_len}\")\n",
        "            print(f\"Number of heads: {self.n_heads}, Head dimension: {self.d_k}\")\n",
        "\n",
        "        # 1. Query, Key, Value 계산\n",
        "        Q = self.w_q(x)\n",
        "        K = self.w_k(x)\n",
        "        V = self.w_v(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Q, K, V shape: {Q.shape}\")\n",
        "            print(f\"Q sample (first token, first 5 values): {Q[0, 0, :5].detach()}\")\n",
        "\n",
        "        # 2. 멀티헤드를 위한 reshape\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Multi-head Q, K, V shape: {Q.shape}\")\n",
        "            print(f\"First head Q sample: {Q[0, 0, 0, :5].detach()}\")\n",
        "\n",
        "        # 3. Attention scores 계산 (Q * K^T / sqrt(d_k))\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Attention scores shape: {scores.shape}\")\n",
        "            print(f\"Scaling factor: {self.scale:.3f}\")\n",
        "            print(f\"Attention scores sample (first head, 3x3):\")\n",
        "            print(f\"{scores[0, 0, :3, :3].detach()}\")\n",
        "\n",
        "        # 4. 마스크 적용 (decoder에서는 미래 정보 차단)\n",
        "        if mask is not None:\n",
        "            if verbose:\n",
        "                print(f\"Causal mask shape: {mask.shape}\")\n",
        "                print(f\"Causal mask sample (3x3):\")\n",
        "                print(f\"{mask[0, 0, :3, :3]}\")\n",
        "\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Masked attention scores (3x3):\")\n",
        "                print(f\"{scores[0, 0, :3, :3].detach()}\")\n",
        "\n",
        "        # 5. 소프트맥스로 attention weights 계산\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Attention weights shape: {attn_weights.shape}\")\n",
        "            print(f\"Attention weights sample (first head, 3x3):\")\n",
        "            print(f\"{attn_weights[0, 0, :3, :3].detach()}\")\n",
        "            print(f\"Row sums (should be 1.0): {attn_weights[0, 0, :3].sum(dim=-1).detach()}\")\n",
        "\n",
        "        # 6. Attention 적용 (weights * Value)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Weighted value output shape: {output.shape}\")\n",
        "            print(f\"First head output sample: {output[0, 0, 0, :5].detach()}\")\n",
        "\n",
        "        # 7. 헤드 결합\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # 8. 최종 선형 변환\n",
        "        output = self.w_o(output)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Final attention output shape: {output.shape}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 피드포워드 신경망\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, verbose: bool = False):\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(\"Feed Forward Network\")\n",
        "            print(\"=\"*40)\n",
        "            print(f\"Input shape: {x.shape}\")\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Output shape: {x.shape}\")\n",
        "            print(\"=\"*40)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Decoder Block\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask: Optional[torch.Tensor] = None, verbose: bool = False):\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"Decoder Block Processing\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        attn_output = self.attention(x, mask, verbose)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        ff_output = self.feed_forward(x, verbose)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Decoder block final output: {x.shape}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        return x\n",
        "\n",
        "# GPT Style Decoder-Only Transformer\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_heads: int,\n",
        "                 n_layers: int, d_ff: int, max_seq_len: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"모델 구조\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"모델 타입: GPT (Decoder-Only Transformer)\")\n",
        "        print(f\"어휘 크기: {vocab_size:,}\")\n",
        "        print(f\"임베딩 차원: {d_model}\")\n",
        "        print(f\"어텐션 헤드: {n_heads}\")\n",
        "        print(f\"디코더 층: {n_layers}\")\n",
        "        print(f\"FFN 차원: {d_ff}\")\n",
        "        print(f\"최대 시퀀스 길이: {max_seq_len}\")\n",
        "        print(f\"드롭아웃: {dropout}\")\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"총 파라미터: {total_params:,}\")\n",
        "        print()\n",
        "\n",
        "    def create_causal_mask(self, seq_len: int, device: torch.device):\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
        "        return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x, verbose: bool = False):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"Model Forward Pass Analysis\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"Input shape: {x.shape}\")\n",
        "            print(f\"Input token IDs (first sample): {x[0].detach()}\")\n",
        "            print(f\"Batch size: {batch_size}, Sequence length: {seq_len}\")\n",
        "\n",
        "        # 1. Token embedding\n",
        "        x = self.token_embedding(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[1. Token Embedding]\")\n",
        "            print(f\"Embedding shape: {x.shape}\")\n",
        "            print(f\"First token embedding (first 5 values): {x[0, 0, :5].detach()}\")\n",
        "\n",
        "        # Embedding scaling\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Scaling factor: √{self.d_model} = {math.sqrt(self.d_model):.3f}\")\n",
        "            print(f\"Scaled embedding (first 5 values): {x[0, 0, :5].detach()}\")\n",
        "\n",
        "        # 2. Positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[2. Positional Encoding]\")\n",
        "            print(f\"After positional encoding shape: {x.shape}\")\n",
        "            print(f\"After positional encoding (first 5 values): {x[0, 0, :5].detach()}\")\n",
        "\n",
        "        # 3. Causal mask\n",
        "        causal_mask = self.create_causal_mask(seq_len, x.device)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[3. Causal Mask]\")\n",
        "            print(f\"Causal mask shape: {causal_mask.shape}\")\n",
        "            print(f\"Causal mask sample (lower triangular matrix):\")\n",
        "            mask_sample = causal_mask[0, 0, :min(5, seq_len), :min(5, seq_len)]\n",
        "            print(mask_sample)\n",
        "\n",
        "        # 4. Decoder blocks\n",
        "        if verbose:\n",
        "            print(f\"\\n[4. Decoder Blocks Processing]\")\n",
        "            print(f\"Total {len(self.decoder_blocks)} decoder blocks to process\")\n",
        "\n",
        "        for i, block in enumerate(self.decoder_blocks):\n",
        "            if verbose:\n",
        "                print(f\"\\n--- Decoder Block {i+1} ---\")\n",
        "                x_before = x.clone().detach()\n",
        "\n",
        "            x = block(x, causal_mask, verbose and i == 0)\n",
        "\n",
        "            if verbose:\n",
        "                # Residual connection effect\n",
        "                residual_effect = (x.detach() - x_before).abs().mean()\n",
        "                print(f\"Block {i+1} residual effect (change amount): {residual_effect:.4f}\")\n",
        "\n",
        "        # 5. Final layer norm and output projection\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[5. Final Output]\")\n",
        "            print(f\"After layer norm (first 5 values): {x[0, 0, :5].detach()}\")\n",
        "\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Final logits shape: {logits.shape}\")\n",
        "            print(f\"Last token logits range: [{logits[0, -1].min().item():.3f}, {logits[0, -1].max().item():.3f}]\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# 배치 데이터 생성\n",
        "def create_batches(tokenizer, texts, max_seq_len, batch_size):\n",
        "    all_tokens = []\n",
        "\n",
        "    print(f\"배치 생성 중... (최대 시퀀스 길이: {max_seq_len})\")\n",
        "\n",
        "    total_texts = len(texts)\n",
        "    printed_count = 0\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        # 처음 5개, 마지막 5개만 출력\n",
        "        if i < 5 or i >= total_texts - 5:\n",
        "            if printed_count == 5:\n",
        "                print(\"... (중간 과정 생략) ...\")\n",
        "            print(f\"처리된 텍스트: {i+1}/{total_texts}, 생성된 시퀀스: {len(all_tokens)}\")\n",
        "            printed_count += 1\n",
        "\n",
        "        tokens = tokenizer.encode(text)\n",
        "\n",
        "        # 토큰이 너무 길면 여러 청크로 나눔\n",
        "        if len(tokens) > max_seq_len:\n",
        "            for start in range(0, len(tokens), max_seq_len - 2):\n",
        "                chunk = tokens[start:start + max_seq_len - 2]\n",
        "                if len(chunk) >= 10:\n",
        "                    chunk = [tokenizer.token_to_idx['<sos>']] + chunk[1:-1] + [tokenizer.token_to_idx['<eos>']]\n",
        "\n",
        "                    while len(chunk) < max_seq_len:\n",
        "                        chunk.append(tokenizer.token_to_idx['<pad>'])\n",
        "\n",
        "                    all_tokens.append(chunk)\n",
        "        else:\n",
        "            while len(tokens) < max_seq_len:\n",
        "                tokens.append(tokenizer.token_to_idx['<pad>'])\n",
        "            all_tokens.append(tokens)\n",
        "\n",
        "    if len(all_tokens) == 0:\n",
        "        print(\"ERROR: 처리된 토큰 시퀀스가 없음!\")\n",
        "        return []\n",
        "\n",
        "    print(f\"총 {len(all_tokens)}개의 시퀀스가 생성...\")\n",
        "\n",
        "    data = torch.tensor(all_tokens, dtype=torch.long)\n",
        "\n",
        "    # 배치 생성 (불완전한 배치도 포함)\n",
        "    batches = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        batches.append(batch)\n",
        "\n",
        "    print(f\"총 {len(batches)}개의 배치 생성...\")\n",
        "    return batches\n",
        "\n",
        "# 모델 학습\n",
        "def train_model(model, batches, tokenizer, epochs, device, lr=3e-4):\n",
        "    if len(batches) == 0:\n",
        "        print(\"ERROR: 학습할 배치가 없음!!\")\n",
        "        print(\"- 텍스트 파일의 문장이 너무 적거나\")\n",
        "        print(\"- 배치 크기가 너무 크거나\")\n",
        "        print(\"- 시퀀스 길이가 너무 길 수 있습니다.\")\n",
        "        return []\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.95))\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=epochs//2)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_idx['<pad>'])\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 학습 시작\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"학습 배치 수: {len(batches)}\")\n",
        "    print(f\"초기 학습률: {lr}\")\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(batches):\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(inputs, verbose=(epoch == 0 and batch_idx == 0))\n",
        "\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if num_batches >= min(20, len(batches)):\n",
        "                break\n",
        "\n",
        "        if num_batches > 0:\n",
        "            avg_loss = total_loss / num_batches\n",
        "            losses.append(avg_loss)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"에포크 {epoch+1:2d}/{epochs}, 손실: {avg_loss:.4f}, 학습률: {current_lr:.6f}\")\n",
        "        else:\n",
        "            print(f\"에포크 {epoch+1:2d}/{epochs}, 배치 없음 - 학습 중단!!\")\n",
        "            break\n",
        "\n",
        "    return losses\n",
        "\n",
        "# 모델 저장\n",
        "def save_model(model, tokenizer, filepath):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab_size': tokenizer.vocab_size,\n",
        "        'd_model': model.d_model,\n",
        "        'n_heads': model.n_heads,\n",
        "        'n_layers': model.n_layers,\n",
        "        'd_ff': model.d_ff,\n",
        "        'max_seq_len': model.max_seq_len,\n",
        "        'vocab': tokenizer.vocab,\n",
        "        'token_to_idx': tokenizer.token_to_idx,\n",
        "        'idx_to_token': tokenizer.idx_to_token,\n",
        "        'use_bert': tokenizer.use_bert\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"모델이 '{filepath}'에 저장되었습니다.\")\n",
        "\n",
        "# 학습 실행 함수\n",
        "def run_training():\n",
        "    # 하이퍼파라미터\n",
        "    MAX_SEQ_LEN = 32      # 시퀀스 길이\n",
        "    BATCH_SIZE = 1        # 배치 크기\n",
        "    D_MODEL = 128         # 모델 크기\n",
        "    N_HEADS = 4           # 헤드 수\n",
        "    N_LAYERS = 2          # 레이어 수\n",
        "    D_FF = 256            # FFN 크기\n",
        "    EPOCHS = 5            # 에포크\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 학습 모드\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"시퀀스 길이: {MAX_SEQ_LEN}\")\n",
        "    print(f\"배치 크기: {BATCH_SIZE}\")\n",
        "    print(f\"모델 크기: {D_MODEL}\")\n",
        "    print()\n",
        "\n",
        "    # 데이터 로드 및 토크나이저 생성\n",
        "    texts = load_text_data()\n",
        "    tokenizer = KLUETokenizer(texts)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = GPTModel(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        max_seq_len=MAX_SEQ_LEN\n",
        "    ).to(device)\n",
        "\n",
        "    # 배치 데이터 생성\n",
        "    batches = create_batches(tokenizer, texts, MAX_SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "    if len(batches) == 0:\n",
        "        print(\"ERROR: 배치 생성에 실패!!\")\n",
        "        print(\"1. 텍스트 파일에 충분한 내용이 있는지?\")\n",
        "        print(\"2. 텍스트 파일 인코딩이 UTF-8인지?\")\n",
        "        return False\n",
        "\n",
        "    print(f\"총 배치 수: {len(batches)}\")\n",
        "\n",
        "    # 모델 학습\n",
        "    losses = train_model(model, batches, tokenizer, EPOCHS, device)\n",
        "\n",
        "    # 모델 저장\n",
        "    if losses:\n",
        "        save_model(model, tokenizer, 'gpt_model.pth')\n",
        "        print(\"학습 완료!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"학습 실패 - 모델 저장 실패!!\")\n",
        "        return False\n",
        "\n",
        "# 학습 실행\n",
        "print(\"GPT 모델 학습\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "success = run_training()\n",
        "\n",
        "if success:\n",
        "    print(\"\\n학습 성공...\")\n",
        "else:\n",
        "    print(\"\\n학습 실패!!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 두 번째 셀: 모델 로드 및 텍스트 생성\n",
        "# =============================================================================\n",
        "\n",
        "# 로드된 토크나이저 클래스\n",
        "class LoadedTokenizer:\n",
        "    def __init__(self, vocab, token_to_idx, idx_to_token, use_bert):\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.token_to_idx = token_to_idx\n",
        "        self.idx_to_token = idx_to_token\n",
        "        self.use_bert = use_bert\n",
        "        if use_bert and TRANSFORMERS_AVAILABLE:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            # 텍스트 길이 제한\n",
        "            truncated_text = text[:100] if len(text) > 100 else text\n",
        "            bert_tokens = self.tokenizer.tokenize(truncated_text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in bert_tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        else:\n",
        "            tokens = list(text)\n",
        "            token_ids = [self.token_to_idx['<sos>']]\n",
        "            for token in tokens:\n",
        "                token_ids.append(self.token_to_idx.get(token, self.token_to_idx['<unk>']))\n",
        "            token_ids.append(self.token_to_idx['<eos>'])\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = []\n",
        "        for idx in token_ids:\n",
        "            token = self.idx_to_token[idx]\n",
        "            if token not in ['<pad>', '<sos>', '<eos>', '<unk>', '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
        "                tokens.append(token)\n",
        "\n",
        "        if self.use_bert and hasattr(self, 'tokenizer'):\n",
        "            return self.tokenizer.convert_tokens_to_string(tokens)\n",
        "        else:\n",
        "            return ''.join(tokens)\n",
        "\n",
        "# 모델 로드\n",
        "def load_model(filepath, device):\n",
        "    try:\n",
        "        checkpoint = torch.load(filepath, map_location=device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"모델 파일 '{filepath}'을 찾기 실패!!\")\n",
        "        return None, None\n",
        "\n",
        "    # 토크나이저 복원\n",
        "    tokenizer = LoadedTokenizer(\n",
        "        checkpoint['vocab'],\n",
        "        checkpoint['token_to_idx'],\n",
        "        checkpoint['idx_to_token'],\n",
        "        checkpoint['use_bert']\n",
        "    )\n",
        "\n",
        "    # 모델 생성 및 가중치 로드\n",
        "    model = GPTModel(\n",
        "        vocab_size=checkpoint['vocab_size'],\n",
        "        d_model=checkpoint['d_model'],\n",
        "        n_heads=checkpoint['n_heads'],\n",
        "        n_layers=checkpoint['n_layers'],\n",
        "        d_ff=checkpoint['d_ff'],\n",
        "        max_seq_len=checkpoint['max_seq_len']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()  # 평가 모드로 설정\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"모델 로드 완료\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"모델 타입: GPT (Decoder-Only Transformer)\")\n",
        "    print(f\"어휘 크기: {checkpoint['vocab_size']:,}\")\n",
        "    print(f\"임베딩 차원: {checkpoint['d_model']}\")\n",
        "    print(f\"어텐션 헤드: {checkpoint['n_heads']}\")\n",
        "    print(f\"디코더 층: {checkpoint['n_layers']}\")\n",
        "    print(f\"최대 시퀀스 길이: {checkpoint['max_seq_len']}\")\n",
        "    print(f\"토크나이저: {'KLUE/BERT' if checkpoint['use_bert'] else '문자 단위'}\")\n",
        "    print()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# 텍스트 생성 (확률 상위 5개 후보 표시)\n",
        "def generate_text_with_candidates(model, tokenizer, prompt, max_length, device, temperature=1.0, verbose=False):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens = tokenizer.encode(prompt)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Input text: '{prompt}' (Temperature: {temperature})\")\n",
        "            print(f\"Tokenized result: {tokens}\")\n",
        "            # 토큰을 실제 텍스트로 변환해서 보여주기\n",
        "            if len(tokens) > 0:\n",
        "                decoded_tokens = []\n",
        "                for i, token_id in enumerate(tokens):\n",
        "                    token = tokenizer.idx_to_token[token_id]\n",
        "                    decoded_tokens.append(f\"{token_id}:{token}\")\n",
        "                print(f\"Token details: {decoded_tokens}\")\n",
        "\n",
        "        generated_tokens = tokens.copy()\n",
        "        all_candidates = []\n",
        "\n",
        "        for i in range(max_length):\n",
        "            if len(generated_tokens) >= model.max_seq_len:\n",
        "                if verbose:\n",
        "                    print(f\"Maximum sequence length ({model.max_seq_len}) reached!\")\n",
        "                break\n",
        "\n",
        "            # 시퀀스 길이 제한 적용\n",
        "            input_tokens = generated_tokens[-model.max_seq_len:]\n",
        "            current_input = torch.tensor([input_tokens], device=device)\n",
        "\n",
        "            # 어텐션 메커니즘 상세 출력 (첫 번째 단계에서만)\n",
        "            show_attention = verbose and i == 0\n",
        "            logits = model(current_input, verbose=show_attention)\n",
        "\n",
        "            next_token_logits = logits[0, -1]\n",
        "\n",
        "            if verbose and i == 0:\n",
        "                print(f\"\\nNext token prediction logits shape: {next_token_logits.shape}\")\n",
        "                print(f\"Logits 샘플 (first 10): {next_token_logits[:10].detach()}\")\n",
        "\n",
        "            if temperature == 0.0:\n",
        "                next_token = torch.argmax(next_token_logits).item()\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "            else:\n",
        "                scaled_logits = next_token_logits / temperature\n",
        "                probs = F.softmax(scaled_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # 상위 5개 후보 저장\n",
        "            top_probs, top_indices = torch.topk(probs, 5)\n",
        "            step_candidates = []\n",
        "            for j, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
        "                token = tokenizer.idx_to_token[idx.item()]\n",
        "                step_candidates.append({\n",
        "                    'rank': j+1,\n",
        "                    'token': token,\n",
        "                    'probability': prob.item(),\n",
        "                    'selected': idx.item() == next_token\n",
        "                })\n",
        "            all_candidates.append(step_candidates)\n",
        "\n",
        "            if verbose and i < 5:  # 처음 5단계\n",
        "                # 현재까지 생성된 텍스트\n",
        "                current_text = tokenizer.decode(generated_tokens)\n",
        "                print(f\"\\n--- Generation Step {i+1} ---\")\n",
        "                print(f\"Current text: '{current_text}'\")\n",
        "                print(f\"Next token candidates:\")\n",
        "\n",
        "                # 확률 분포 정보 (첫 번째 단계에서만)\n",
        "                if i == 0:\n",
        "                    print(f\"Probability distribution - Sum: {probs.sum():.6f}, Max: {probs.max():.6f}, Min: {probs.min():.6f}\")\n",
        "\n",
        "                # 실제 선택된 토큰이 상위 5개에 있는지 확인\n",
        "                selected_in_top5 = any(candidate['selected'] for candidate in step_candidates)\n",
        "\n",
        "                for candidate in step_candidates:\n",
        "                    selected_mark = \" ★\" if candidate['selected'] else \"\"\n",
        "                    # 각 후보를 추가했을 때의 텍스트 미리보기\n",
        "                    token_idx = tokenizer.token_to_idx.get(candidate['token'], tokenizer.token_to_idx['<unk>'])\n",
        "                    preview_tokens = generated_tokens + [token_idx]\n",
        "                    preview_text = tokenizer.decode(preview_tokens)\n",
        "                    print(f\"  {candidate['rank']}. '{candidate['token']}' → '{preview_text}' (prob: {candidate['probability']:.4f}){selected_mark}\")\n",
        "\n",
        "                # 선택된 토큰이 상위 5개에 없는 경우 경고 표시\n",
        "                if not selected_in_top5:\n",
        "                    selected_token = tokenizer.idx_to_token[next_token]\n",
        "                    print(f\"  ⚠️ Actually selected token '{selected_token}' not in top 5\")\n",
        "\n",
        "                # Temperature 효과 설명 (첫 번째 단계에서만)\n",
        "                if i == 0:\n",
        "                    print(f\"Temperature effect ({temperature:.1f}): \", end=\"\")\n",
        "                    if temperature < 0.7:\n",
        "                        print(\"Conservative generation (prefers high probability tokens)\")\n",
        "                    elif temperature > 1.3:\n",
        "                        print(\"Creative generation (explores diverse tokens)\")\n",
        "                    else:\n",
        "                        print(\"Balanced generation\")\n",
        "\n",
        "            # 생성 종료 조건\n",
        "            eos_token_id = tokenizer.token_to_idx.get('<eos>', -1)\n",
        "            if next_token == eos_token_id:\n",
        "                if verbose:\n",
        "                    print(f\"Encountered <eos> token, stopping generation...\")\n",
        "                break\n",
        "\n",
        "            generated_tokens.append(next_token)\n",
        "\n",
        "        generated_text = tokenizer.decode(generated_tokens)\n",
        "\n",
        "        return generated_text, all_candidates\n",
        "\n",
        "# 텍스트 생성\n",
        "def run_generation(input_text=\"고기가\", max_length=20, show_analysis=True):\n",
        "    print(\"=\"*60)\n",
        "    print(\"GPT 모델 텍스트 생성\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 모델 로드\n",
        "    model, tokenizer = load_model('gpt_model.pth', device)\n",
        "\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    print(f\"입력 텍스트: '{input_text}'\")\n",
        "    print(f\"생성 길이: {max_length}\")\n",
        "    print()\n",
        "\n",
        "    # 다양한 Temperature로 생성\n",
        "    temperatures = [0.5, 1.0, 1.5]\n",
        "\n",
        "    for i, temp in enumerate(temperatures):\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Temperature {temp} 결과\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        result, candidates = generate_text_with_candidates(\n",
        "            model, tokenizer, input_text, max_length, device,\n",
        "            temperature=temp, verbose=(i == 0)  # 첫 번째만 상세 출력 (5단계)\n",
        "        )\n",
        "\n",
        "        print(f\"생성 결과: '{result}'\")\n",
        "        print()\n",
        "\n",
        "    print(\"텍스트 생성 완료!\")\n",
        "\n",
        "# 대화형 생성 함수\n",
        "def interactive_generation():\n",
        "    print(\"=\"*60)\n",
        "    print(\"대화형 텍스트 생성\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 모델 로드\n",
        "    model, tokenizer = load_model('gpt_model.pth', device)\n",
        "\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    print(\"모델 로드 성공...\")\n",
        "    print(\"종료하려면 'quit' 또는 'exit'를 입력하세요.\")\n",
        "    print()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"입력 텍스트: \").strip()\n",
        "\n",
        "            if user_input.lower() in ['quit', 'exit', '종료']:\n",
        "                print(\"생성을 종료합니다.\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                print(\"텍스트를 입력해주세요.\")\n",
        "                continue\n",
        "\n",
        "            # Temperature 선택\n",
        "            try:\n",
        "                temp_input = input(\"Temperature (0.5-2.0, 기본값 1.0): \").strip()\n",
        "                temperature = float(temp_input) if temp_input else 1.0\n",
        "                temperature = max(0.1, min(2.0, temperature))  # 범위 제한\n",
        "            except ValueError:\n",
        "                temperature = 1.0\n",
        "\n",
        "            # 길이 선택\n",
        "            try:\n",
        "                length_input = input(\"생성 길이 (기본값 15): \").strip()\n",
        "                max_length = int(length_input) if length_input else 15\n",
        "                max_length = max(1, min(50, max_length))  # 범위 제한\n",
        "            except ValueError:\n",
        "                max_length = 15\n",
        "\n",
        "            print(f\"\\n생성 중... (Temperature: {temperature}, 길이: {max_length})\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            result, candidates = generate_text_with_candidates(\n",
        "                model, tokenizer, user_input, max_length, device,\n",
        "                temperature=temperature, verbose=False\n",
        "            )\n",
        "\n",
        "            print(f\"결과: '{result}'\")\n",
        "            print(\"-\" * 40)\n",
        "            print()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n생성을 중단합니다.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "            print(\"다시 시도해주세요.\")\n",
        "            print()\n",
        "\n",
        "# 메인 실행\n",
        "print(\"GPT 모델 텍스트 생성 스크립트\")\n",
        "print(\"=\"*60)\n",
        "print(\"사용 방법:\")\n",
        "print(\"1. 기본 생성: run_generation('입력텍스트')\")\n",
        "print(\"2. 대화형 생성: interactive_generation()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 기본 실행\n",
        "run_generation(\"고기가\", max_length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSCyrPw3MPHB",
        "outputId": "4050f0e4-ecfb-4fc6-94e6-41dc8e9bef4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers 사용 가능: KLUE/BERT 토크나이저 사용\n",
            "============================================================\n",
            "하드웨어 환경 확인\n",
            "============================================================\n",
            "PyTorch 버전: 2.8.0+cu126\n",
            "CUDA 사용 가능: False\n",
            "CPU 전용 모드\n",
            "사용 디바이스: cpu\n",
            "\n",
            "GPT 모델 학습\n",
            "============================================================\n",
            "============================================================\n",
            "모델 학습 모드\n",
            "============================================================\n",
            "시퀀스 길이: 32\n",
            "배치 크기: 1\n",
            "모델 크기: 128\n",
            "\n",
            "파일에서 1개의 문장 로드\n",
            "KLUE/BERT 토크나이저 초기화 중...\n",
            "토크나이징 진행: 1/1\n",
            "============================================================\n",
            "토크나이저 정보\n",
            "============================================================\n",
            "토크나이저 타입: KLUE/BERT WordPiece\n",
            "어휘 크기: 54\n",
            "토큰 예시: ['##고', '##곤', '##는', '##는데', '##다', '##를', '##만', '##배', '##세', '##송']\n",
            "특수 토큰: ['<pad>', '<sos>', '<eos>', '<unk>']\n",
            "\n",
            "============================================================\n",
            "위치 인코딩 정보\n",
            "============================================================\n",
            "임베딩 차원: 128\n",
            "최대 길이: 32\n",
            "PE 값 범위: [-1.000, 1.000]\n",
            "sin/cos 함수로 생성\n",
            "\n",
            "============================================================\n",
            "모델 구조\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 54\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "FFN 차원: 256\n",
            "최대 시퀀스 길이: 32\n",
            "드롭아웃: 0.1\n",
            "총 파라미터: 279,094\n",
            "\n",
            "배치 생성 중... (최대 시퀀스 길이: 32)\n",
            "처리된 텍스트: 1/1, 생성된 시퀀스: 0\n",
            "총 2개의 시퀀스가 생성...\n",
            "총 2개의 배치 생성...\n",
            "총 배치 수: 2\n",
            "============================================================\n",
            "모델 학습 시작\n",
            "============================================================\n",
            "학습 배치 수: 2\n",
            "초기 학습률: 0.0003\n",
            "\n",
            "============================================================\n",
            "Model Forward Pass Analysis\n",
            "============================================================\n",
            "Input shape: torch.Size([1, 31])\n",
            "Input token IDs (first sample): tensor([ 1, 34, 11, 41, 40, 20, 45, 16, 14, 48, 33, 26, 14, 49, 11, 37, 24, 19,\n",
            "        13, 30, 38, 23, 53, 16, 14, 48, 35, 10, 51,  2,  0])\n",
            "Batch size: 1, Sequence length: 31\n",
            "\n",
            "[1. Token Embedding]\n",
            "Embedding shape: torch.Size([1, 31, 128])\n",
            "First token embedding (first 5 values): tensor([-1.0551,  0.4572,  0.2682, -0.2009, -0.1676])\n",
            "Scaling factor: √128 = 11.314\n",
            "Scaled embedding (first 5 values): tensor([-11.9375,   5.1728,   3.0338,  -2.2727,  -1.8959])\n",
            "\n",
            "[2. Positional Encoding]\n",
            "After positional encoding shape: torch.Size([1, 31, 128])\n",
            "After positional encoding (first 5 values): tensor([-13.2638,   6.8587,   0.0000,  -0.0000,  -2.1065])\n",
            "\n",
            "[3. Causal Mask]\n",
            "Causal mask shape: torch.Size([1, 1, 31, 31])\n",
            "Causal mask sample (lower triangular matrix):\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1.]])\n",
            "\n",
            "[4. Decoder Blocks Processing]\n",
            "Total 2 decoder blocks to process\n",
            "\n",
            "--- Decoder Block 1 ---\n",
            "\n",
            "==================================================\n",
            "Decoder Block Processing\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "Attention Mechanism Analysis\n",
            "==================================================\n",
            "Input shape: torch.Size([1, 31, 128])\n",
            "Batch size: 1, Sequence length: 31\n",
            "Number of heads: 4, Head dimension: 32\n",
            "Q, K, V shape: torch.Size([1, 31, 128])\n",
            "Q sample (first token, first 5 values): tensor([ 0.0968, -9.4587,  7.8425, -8.8123,  2.7385])\n",
            "Multi-head Q, K, V shape: torch.Size([1, 4, 31, 32])\n",
            "First head Q sample: tensor([ 0.0968, -9.4587,  7.8425, -8.8123,  2.7385])\n",
            "Attention scores shape: torch.Size([1, 4, 31, 31])\n",
            "Scaling factor: 5.657\n",
            "Attention scores sample (first head, 3x3):\n",
            "tensor([[ 64.7791,  84.5115,   5.3933],\n",
            "        [-64.7484, -45.2811,  33.2427],\n",
            "        [104.5961, -27.5202, -56.3168]])\n",
            "Causal mask shape: torch.Size([1, 1, 31, 31])\n",
            "Causal mask sample (3x3):\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "Masked attention scores (3x3):\n",
            "tensor([[ 6.4779e+01, -1.0000e+09, -1.0000e+09],\n",
            "        [-6.4748e+01, -4.5281e+01, -1.0000e+09],\n",
            "        [ 1.0460e+02, -2.7520e+01, -5.6317e+01]])\n",
            "Attention weights shape: torch.Size([1, 4, 31, 31])\n",
            "Attention weights sample (first head, 3x3):\n",
            "tensor([[1.1111e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.9014e-09, 1.1111e+00, 0.0000e+00],\n",
            "        [1.1111e+00, 0.0000e+00, 0.0000e+00]])\n",
            "Row sums (should be 1.0): tensor([1.1111, 1.1111, 1.1111])\n",
            "Weighted value output shape: torch.Size([1, 4, 31, 32])\n",
            "First head output sample: tensor([ 3.8720,  9.3235, -4.9892, 10.8690, -4.2805])\n",
            "Final attention output shape: torch.Size([1, 31, 128])\n",
            "==================================================\n",
            "\n",
            "========================================\n",
            "Feed Forward Network\n",
            "========================================\n",
            "Input shape: torch.Size([1, 31, 128])\n",
            "Output shape: torch.Size([1, 31, 128])\n",
            "========================================\n",
            "Decoder block final output: torch.Size([1, 31, 128])\n",
            "==================================================\n",
            "Block 1 residual effect (change amount): 8.5939\n",
            "\n",
            "--- Decoder Block 2 ---\n",
            "Block 2 residual effect (change amount): 0.2253\n",
            "\n",
            "[5. Final Output]\n",
            "After layer norm (first 5 values): tensor([-1.2940,  0.3074,  0.6300, -0.4968, -0.0569])\n",
            "Final logits shape: torch.Size([1, 31, 54])\n",
            "Last token logits range: [-1.284, 0.820]\n",
            "============================================================\n",
            "에포크  1/5, 손실: 3.9861, 학습률: 0.000300\n",
            "에포크  2/5, 손실: 3.8875, 학습률: 0.000300\n",
            "에포크  3/5, 손실: 3.7739, 학습률: 0.000300\n",
            "에포크  4/5, 손실: 3.6740, 학습률: 0.000300\n",
            "에포크  5/5, 손실: 3.5015, 학습률: 0.000300\n",
            "모델이 'gpt_model.pth'에 저장되었습니다.\n",
            "학습 완료!\n",
            "\n",
            "학습 성공...\n",
            "GPT 모델 텍스트 생성 스크립트\n",
            "============================================================\n",
            "사용 방법:\n",
            "1. 기본 생성: run_generation('입력텍스트')\n",
            "2. 대화형 생성: interactive_generation()\n",
            "============================================================\n",
            "============================================================\n",
            "GPT 모델 텍스트 생성\n",
            "============================================================\n",
            "============================================================\n",
            "위치 인코딩 정보\n",
            "============================================================\n",
            "임베딩 차원: 128\n",
            "최대 길이: 32\n",
            "PE 값 범위: [-1.000, 1.000]\n",
            "sin/cos 함수로 생성\n",
            "\n",
            "============================================================\n",
            "모델 구조\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 54\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "FFN 차원: 256\n",
            "최대 시퀀스 길이: 32\n",
            "드롭아웃: 0.1\n",
            "총 파라미터: 279,094\n",
            "\n",
            "============================================================\n",
            "모델 로드 완료\n",
            "============================================================\n",
            "모델 타입: GPT (Decoder-Only Transformer)\n",
            "어휘 크기: 54\n",
            "임베딩 차원: 128\n",
            "어텐션 헤드: 4\n",
            "디코더 층: 2\n",
            "최대 시퀀스 길이: 32\n",
            "토크나이저: KLUE/BERT\n",
            "\n",
            "입력 텍스트: '고기가'\n",
            "생성 길이: 20\n",
            "\n",
            "==================================================\n",
            "Temperature 0.5 결과\n",
            "==================================================\n",
            "Input text: '고기가' (Temperature: 0.5)\n",
            "Tokenized result: [1, 33, 3, 2]\n",
            "Token details: ['1:<sos>', '33:고기', '3:<unk>', '2:<eos>']\n",
            "\n",
            "============================================================\n",
            "Model Forward Pass Analysis\n",
            "============================================================\n",
            "Input shape: torch.Size([1, 4])\n",
            "Input token IDs (first sample): tensor([ 1, 33,  3,  2])\n",
            "Batch size: 1, Sequence length: 4\n",
            "\n",
            "[1. Token Embedding]\n",
            "Embedding shape: torch.Size([1, 4, 128])\n",
            "First token embedding (first 5 values): tensor([-1.0546,  0.4583,  0.2696, -0.1998, -0.1693])\n",
            "Scaling factor: √128 = 11.314\n",
            "Scaled embedding (first 5 values): tensor([-11.9313,   5.1852,   3.0500,  -2.2603,  -1.9156])\n",
            "\n",
            "[2. Positional Encoding]\n",
            "After positional encoding shape: torch.Size([1, 4, 128])\n",
            "After positional encoding (first 5 values): tensor([-11.9313,   6.1852,   3.0500,  -1.2603,  -1.9156])\n",
            "\n",
            "[3. Causal Mask]\n",
            "Causal mask shape: torch.Size([1, 1, 4, 4])\n",
            "Causal mask sample (lower triangular matrix):\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.]])\n",
            "\n",
            "[4. Decoder Blocks Processing]\n",
            "Total 2 decoder blocks to process\n",
            "\n",
            "--- Decoder Block 1 ---\n",
            "\n",
            "==================================================\n",
            "Decoder Block Processing\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "Attention Mechanism Analysis\n",
            "==================================================\n",
            "Input shape: torch.Size([1, 4, 128])\n",
            "Batch size: 1, Sequence length: 4\n",
            "Number of heads: 4, Head dimension: 32\n",
            "Q, K, V shape: torch.Size([1, 4, 128])\n",
            "Q sample (first token, first 5 values): tensor([ 3.1731, -4.0424,  6.3118, -5.1064,  6.0341])\n",
            "Multi-head Q, K, V shape: torch.Size([1, 4, 4, 32])\n",
            "First head Q sample: tensor([ 3.1731, -4.0424,  6.3118, -5.1064,  6.0341])\n",
            "Attention scores shape: torch.Size([1, 4, 4, 4])\n",
            "Scaling factor: 5.657\n",
            "Attention scores sample (first head, 3x3):\n",
            "tensor([[ 83.5872, -14.6439, -26.6122],\n",
            "        [-52.1283,  58.0523,   2.1470],\n",
            "        [ 13.5159,  12.3452, -18.0863]])\n",
            "Causal mask shape: torch.Size([1, 1, 4, 4])\n",
            "Causal mask sample (3x3):\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "Masked attention scores (3x3):\n",
            "tensor([[ 8.3587e+01, -1.0000e+09, -1.0000e+09],\n",
            "        [-5.2128e+01,  5.8052e+01, -1.0000e+09],\n",
            "        [ 1.3516e+01,  1.2345e+01, -1.8086e+01]])\n",
            "Attention weights shape: torch.Size([1, 4, 4, 4])\n",
            "Attention weights sample (first head, 3x3):\n",
            "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
            "        [7.6328e-01, 2.3672e-01, 1.4388e-14]])\n",
            "Row sums (should be 1.0): tensor([1.0000, 1.0000, 1.0000])\n",
            "Weighted value output shape: torch.Size([1, 4, 4, 32])\n",
            "First head output sample: tensor([ 0.8499,  6.8858, -2.0459,  8.8603, -4.3327])\n",
            "Final attention output shape: torch.Size([1, 4, 128])\n",
            "==================================================\n",
            "\n",
            "========================================\n",
            "Feed Forward Network\n",
            "========================================\n",
            "Input shape: torch.Size([1, 4, 128])\n",
            "Output shape: torch.Size([1, 4, 128])\n",
            "========================================\n",
            "Decoder block final output: torch.Size([1, 4, 128])\n",
            "==================================================\n",
            "Block 1 residual effect (change amount): 8.2883\n",
            "\n",
            "--- Decoder Block 2 ---\n",
            "Block 2 residual effect (change amount): 0.2736\n",
            "\n",
            "[5. Final Output]\n",
            "After layer norm (first 5 values): tensor([-1.8706,  0.4314,  0.8311, -0.2859,  0.0265])\n",
            "Final logits shape: torch.Size([1, 4, 54])\n",
            "Last token logits range: [-1.628, 1.197]\n",
            "============================================================\n",
            "\n",
            "Next token prediction logits shape: torch.Size([54])\n",
            "Logits sample (first 10): tensor([-0.4062, -0.9457,  1.1971,  0.8774, -0.3995, -0.1064, -0.4276,  0.2786,\n",
            "        -0.6448,  0.9428])\n",
            "\n",
            "--- Generation Step 1 ---\n",
            "Current text: '고기'\n",
            "Next token candidates:\n",
            "Probability distribution - Sum: 1.000000, Max: 0.106723, Min: 0.000375\n",
            "  1. '<eos>' → '고기' (prob: 0.1067)\n",
            "  2. '낚' → '고기 낚' (prob: 0.0701)\n",
            "  3. '##고' → '고기고' (prob: 0.0642)\n",
            "  4. '<unk>' → '고기' (prob: 0.0563)\n",
            "  5. '한' → '고기 한' (prob: 0.0528)\n",
            "  ⚠️ Actually selected token '했' not in top 5\n",
            "Temperature effect (0.5): Conservative generation (prefers high probability tokens)\n",
            "\n",
            "--- Generation Step 2 ---\n",
            "Current text: '고기 했'\n",
            "Next token candidates:\n",
            "  1. '##에서' → '고기 했에서' (prob: 0.1251)\n",
            "  2. '<eos>' → '고기 했' (prob: 0.0892)\n",
            "  3. '[MASK]' → '고기 했' (prob: 0.0504)\n",
            "  4. '만' → '고기 했 만' (prob: 0.0503)\n",
            "  5. '##지' → '고기 했지' (prob: 0.0385)\n",
            "  ⚠️ Actually selected token '##를' not in top 5\n",
            "\n",
            "--- Generation Step 3 ---\n",
            "Current text: '고기 했를'\n",
            "Next token candidates:\n",
            "  1. '타고' → '고기 했를 타고' (prob: 0.2960)\n",
            "  2. '하' → '고기 했를 하' (prob: 0.0861) ★\n",
            "  3. '##배' → '고기 했를배' (prob: 0.0553)\n",
            "  4. '##는' → '고기 했를는' (prob: 0.0537)\n",
            "  5. '했' → '고기 했를 했' (prob: 0.0415)\n",
            "\n",
            "--- Generation Step 4 ---\n",
            "Current text: '고기 했를 하'\n",
            "Next token candidates:\n",
            "  1. '.' → '고기 했를 하.' (prob: 0.1201)\n",
            "  2. '##는' → '고기 했를 하는' (prob: 0.1003)\n",
            "  3. '처음' → '고기 했를 하 처음' (prob: 0.0825) ★\n",
            "  4. '마리' → '고기 했를 하 마리' (prob: 0.0637)\n",
            "  5. '허' → '고기 했를 하 허' (prob: 0.0528)\n",
            "\n",
            "--- Generation Step 5 ---\n",
            "Current text: '고기 했를 하 처음'\n",
            "Next token candidates:\n",
            "  1. '##곤' → '고기 했를 하 처음곤' (prob: 0.1527) ★\n",
            "  2. '##를' → '고기 했를 하 처음를' (prob: 0.0990)\n",
            "  3. '[CLS]' → '고기 했를 하 처음' (prob: 0.0774)\n",
            "  4. '만' → '고기 했를 하 처음 만' (prob: 0.0674)\n",
            "  5. '하' → '고기 했를 하 처음 하' (prob: 0.0624)\n",
            "생성 결과: '고기 했를 하 처음곤 허은만는 40 40곤 조각일 허은 허 노인 40'\n",
            "\n",
            "==================================================\n",
            "Temperature 1.0 결과\n",
            "==================================================\n",
            "생성 결과: '고기만 84에서 낚.송째 못하 고기에서만만 못하 만는데 40일 낚'\n",
            "\n",
            "==================================================\n",
            "Temperature 1.5 결과\n",
            "==================================================\n",
            "생성 결과: '고기고째 하는일월 혼자잡이,배 만 고기 타고 만이 그 낚송'\n",
            "\n",
            "텍스트 생성 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iete3RprMguw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}